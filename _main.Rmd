--- 
title: "UCSD Psyc 201ab / CSS 205 / Psyc 193"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is the class website and lecture notes for psyc 201ab and CSS 205
link-citations: yes
github-repo: UCSD-PSYC201/website
---

# Welcome {-}

This is the website for Psych 201a, 201b; they are presumed to be taken as a series with 201a in the fall, and 201b in the winter.  201a is also offered as an advanced undergraduate course (Psych 193), and will be crosslisted as the core stats class for the Computational Social Science MS program (CSS 205).    

201a covers statistical graphics, probability, classical statistical methods, with an emphasis on the general linear model, and their implementation in R.     

201b covers more advanced models and methods, including generalized linear models, multilevel/hierarchical regression, and computational approaches such as numerical optimization, Monte Carlo, and resampling.  

**201b is not offered in the 2021-2022 academic year**


<!--chapter:end:index.Rmd-->

# Syllabus {#course-syllabus  .unnumbered}

## Instructors

| Instructor | Office hours |
| ----- | ------ |
| **Ed Vul** | Thursdays 1pm in 5137 McGill |
| **Wenhao (James) Qi** |  Thursdays 10am in 3509 Mandler |

## Class meetings

Class meetings will be in 3545 Mandler Hall.  
Lectures are Tuesday / Thursday 2-4.  
Labs are Wednesdays 5-7.  
Attendance is not required, but is generally helpful.

## Grading

**PSYC 201 / CSS 205**: Homework (25%), Midterm (25%), Final (25%), Project (25%).

**PSYC 193**: Homework (75%), Project (25%).  (no midterm or final)

Details:
  
*Homework*:  These are entirely in R, on our [online system](hw/).  It makes sense to seek help from other students when you get stuck, but make sure you do everything yourself, so that you will be able to do it on your own in exams, and more importantly, real life. 

*Exams*: Midterm and Final (both take home). Working together on exams is prohibited.

*Group project*: You will analyze some data, write a final report, and make a presentation. [Details here](#course-projects).

## Class Resources

- *Class Campuswire*:  Sign up [**here**](https://campuswire.com/p/G8F0D1572), code is **2647**.  All class communication will be here.

- *Homework assignments*:  [**login**](http://vulstats.ucsd.edu/hw/)  (**this may be buggy, please do not hesitate to tell us if something is wrong.** )
Login is your ucsd username, password is full student ID, including letter.

- *Readings*:  Generally we will draw on a number of sources, and our own notes, as needed for whatever we are covering.    
   
- *Software*:  We are using [R](https://www.r-project.org/) and the [Rstudio](https://www.rstudio.com/) IDE.
   - Our own installation instructions and list of starter resources: [Getting started with R](#R-start)    


<!--chapter:end:course-syllabus.Rmd-->

# 201a Schedule  {#course-201a  .unnumbered}

## Week 0: Introduction {-}

In which we will cover the goals of this class, and where the class materials fit into the broader landscape of quantitative / computational / data skills.

#### Readings   

**Basic introduction to R / Rstudio**: [R4DS](https://r4ds.had.co.nz/) Sections: [1](http://r4ds.had.co.nz/introduction.html), [4](http://r4ds.had.co.nz/workflow-basics.html), [6](http://r4ds.had.co.nz/workflow-scripts.html), [8](http://r4ds.had.co.nz/workflow-projects.html)  

#### Homework

[Pre-survey](https://forms.gle/2LEtUqjDGUfGhjHg7)
[HW00: Swirl](http://vulstats.ucsd.edu/hw/) (due 9/30)

#### Thursday

Overview: [slides](slides/L00.pdf)

## Week 1: Data  {-}

In which we cover data organization, cleaning, and basic summaries, while getting acquainted with R syntax.

#### Readings

**Working with data**: R4DS Sections: [5](http://r4ds.had.co.nz/transform.html), [7](http://r4ds.had.co.nz/exploratory-data-analysis.html), [11](http://r4ds.had.co.nz/data-import.html), [12](http://r4ds.had.co.nz/tidy-data.html)   
<!--- **Descriptive stats**: [[notes](notes/descriptive.html)]   
**Cleaning data example (code)**: [[notes](notes/data-cleaning.html)]  -->

#### Homework

[HW01: Data cleaning](http://vulstats.ucsd.edu/hw/) (due 10/11)

#### Tuesday   

Data overview: [slides](slides/L01.pdf) [live code](code/live/L01.R)

#### Wednesday 9/29

R basics (continued) [[files](labs/201a-2021/01/lab1.tar.gz)]

#### Thursday   

Data summaries, data frames, and dplyr: [slides](slides/L02.pdf) [live code](code/live/L02.R)

## Week 2: Visualization {-}

In which we cover how to make scientifically useful graphs.

#### Readings
[[notes](notes/visualizations.html)]   
R4DS: [2](http://r4ds.had.co.nz/explore-intro.html), [3](http://r4ds.had.co.nz/data-visualisation.html)    
socviz: [make a plot](https://socviz.co/makeplot.html) (the rest of this book may also be useful, but we don't have time for a thorough treatment.)

#### Homework

#### Tuesday   

visualization, ggplot #1: [slides](slides/L03.pdf) , [code](code/live/L03.R)

#### Wednesday 10/6

Data visualization [[code](labs/201a-2021/02/lab2.Rmd)]

#### Thursday   



## Week 3: Theoretical foundations {-}

In which we cover probability theory, and the logic of classical statistical methods.

#### Readings

**Probability notes:**   
[terms](notes/prob-terms.html)   
[basics](notes/prob-foundations.html)   
[conditional](notes/prob-conditional.html) [monte-carlo](notes/prob-monte-carlo.html)   
[random-variables](notes/prob-rv.html)   
[distribution functions](notes/prob-rv-functions.html)  
[expectation](notes/prob-expectation.html)   
[central limit theorem, normal](notes/prob-clt-normal.html) 

**NHST notes:**   
[via simulation](notes/nhst-simulation.html)    
[sampling distribution](notes/nhst-sampling-distribution.html)   
[basic NHST via normal](notes/nhst-basics-normal.html)     
[power, effects](notes/nhst-theory-normal.html)     
[bonus: via binomial](notes/theory-binomial.html) 

#### Homework

[NHST probability](http://vulstats.ucsd.edu/hw/) Due: 2020-10-28

#### Tuesday   
[slides: probability](slides/L04.probability.pdf)   

#### Wednesday

Probability [[code](labs/201a-2020/03/lab3.Rmd)] [[answers](labs/201a-2020/03/lab3-ans.Rmd)]

#### Thursday
[slides: NHST](slides/L05.classical-stats.pdf)   

## Week 4: Linear model: Regression {-}


#### Notes
[t-distribution](notes/nhst-t-distribution.html)   
[t-tests](notes/nhst-t-tests.html) (*note: some of this will be covered in week 6 and week 10*)    
[bivariate data](notes/bivariate.html)    
[anscombe's quartet](notes/bivariate-anscombe.html)    
[covariance](notes/bivariate-covariance.html)   
[correlation](notes/bivariate-correlation.html)   
[ordinary least-squares regression](notes/bivariate-ols.html)   
[y~x,x~y,pca](notes/bivariate-lines.html)   
[coefficient of determination](notes/bivariate-determination.html)    
[significance testing of these measures](notes/bivariate-significance.html)
[prediction](notes/bivariate-prediction.html)   
[regression diagnostics](notes/bivariate-ols-diagnostics.html)   

#### Homework

[Regression](http://vulstats.ucsd.edu/hw/) Due: 2020-11-04

#### Tuesday   
[slides](slides/L-regression-1.pdf)

#### Wednesday

Regression [[code](labs/201a-2020/04/lab4.Rmd)] [[answers](labs/201a-2020/04/lab4-ans.Rmd)]

#### Thursday   
[slides](slides/L-regression-2.pdf)

## Week 5: Linear model: Multiple regression {-}

#### Notes  
[multiple regression](notes/multiple-regression.html)  

#### Tuesday
[slides](slides/L-mr-a.pdf)   
[code](code/live/L-mr.R)

#### Wednesday

Multiple regression [[code](labs/201a-2020/05/lab5.Rmd)] [[answers](labs/201a-2020/05/lab5-ans.Rmd)]

#### Thursday
[slides](slides/L-mr-b.pdf)   

## Week 6: Linear model: Categorical predictors {-}

#### Readings / Notes
[Notes on t-tests](notes/nhst-t-tests.html)   
[[howell ch.13 on ANOVA](pdf/howell.ch-13.pdf)]
[[howell ch.16 on ANCOVA](pdf/howell.ch-16.pdf)]   

#### Homework

[Multiple regression](http://vulstats.ucsd.edu/hw/) Due: 2020-11-20

#### Tuesday
[slides](slides/L-lm-cat-a.pdf)   


#### Thursday
[slides](slides/L-lm-cat-b.pdf)   


## Week 7: Linear model: ANCOVA, diagnostics {-}

#### Homework

[ANOVA & ANCOVA](http://vulstats.ucsd.edu/hw/) Due: 2020-11-30

#### Tuesday
[slides](slides/L-lm-ancova.pdf)   

#### Wednesday
ANOVA [[code](labs/201a-2020/06/Lab6.Rmd)] [[answers](labs/201a-2020/06/Lab6-ans.Rmd)]

#### Thursday
[slides](slides/L-lm-diagnostics.pdf)   


## Week 8: Linear model: Linearizing transforms {-}

#### Homework

[Linearizing transforms](http://vulstats.ucsd.edu/hw/) Due: 2020-12-03

#### Tuesday
[slides](slides/L-lm-transforms.pdf)   

## Week 9: Covarying errors (repeated measures / random effects) {-}

#### Readings:  
I don't like either of these.... I am still on the hunt for a pithy conceptual overview of repeated measures designs and analyses:   
This is simpler:  [Howell, ch. 14](pdf/Howell.ch-14.repeated-measures.pdf)     
This is mathier:  [Kutner ch. 27](pdf/Kutner.ch-27.repeated-measures.pdf)


#### Tuesday
[slides](slides/L-repeated-measures.pdf)   

#### Wednesday

Repeated measures [[code](labs/201a-2020/07/lab7.Rmd)] [[answers](labs/201a-2020/07/lab7-ans.Rmd)]

#### Readings
[[Howell (basic)](pdf/Howell.ch-14.repeated-measures.pdf)]    
[[Kutner (mathy)](pdf/Kutner.ch-27.repeated-measures.pdf)] 

## Week 10: Review and preview {-}

#### Tuesday
- Mixed effects model brief    
- Project Q&A, Review Q&A
[slides](slides/L-lmer.pdf)

#### Wednesday
- R Review (for final)   

#### Thursday
- Review


<!--chapter:end:course-201a.Rmd-->

# Projects {#course-projects  .unnumbered}

The goal is to analyze a large, rich dataset to answer an interesting behavioral/social/neural question, with the final product being a potentially publishable paper.

This project is divided into two phases to be implemented in 201a and 201b.

In 201a your goal is to identify a conjunction of an interesting question and a data source that might answer it.  You will need to understand the data, clean it, make graphs of the data that might answer the question, and do simple analyses to get your bearings.  

In 201b you will do the more complete analyses, likely using more advanced methods that we will cover in 201b, and turn the initial report from 201a into something that could be submitted for publication.

## Examples of this sort of thing

- [skill learning in online games](http://journals.sagepub.com/doi/abs/10.1177/0956797613511466)
- [sequential dependence in yelp reviews](https://mindmodeling.org/cogsci2016/papers/0254/)
- [stereotype threat in chess play](https://psyarxiv.com/bpy3t/)
- [income mobility over time](http://science.sciencemag.org/content/early/2017/04/21/science.aal4617.full)
- [scaling laws in cities](http://www.pnas.org/content/104/17/7301.long)
- [crowd within in real estimation](https://www.nature.com/articles/s41562-017-0247-6)
- [personality in blog posts](http://www.sciencedirect.com/science/article/pii/S0092656610000541)
- [neurosynth brain mapping example](http://www.pnas.org/content/113/7/1907.full)
- [hashtag adoption](https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12675)
- [cultural tastes via baby names](http://www.pnas.org/content/106/20/8146.short)

You will notice that particularly successful examples usually have a combination of a few things:

1. a coherent research question, with a good justification for why the naturalistic data maps onto theoretical constructs of interest;
2. a novel dataset, which might mean data that had not preciously been available, or a dataset that was created by cleverly combining/co-registering previously separate datasets;
3. and (sometimes or) a fairly sophisticated analysis that adequately grapples with the complicated structure of the data.

The full project will span both 201a and 201b (previously I had it only in 201b, and that was not enough time)

## 201a

### 201a Timeline

Figure out groups as soon as possible, so we can assist if folks are group-less.

2020-10-20: Groups due  
2020-10-27: Project plan due  
2020-11-10: Preliminary data summaries due  
2020-12-17: (before final) Write-ups due  
2020-12-17: (during final) Project presentations  
2020-12-18: Group-evaluation due

### Groups

You will be in groups of ~3-5 (5 if group includes an undergrad).  Undergrads should not be in a group together (should join a group of grad students).  Hopefully you can self-assemble into groups, but I will help if need be.

**Due: 2020-10-20** Create a Slack channel titled "project-[groupname]" (can be either public or private).  Invite Ed and James into the channel.  Upload a CSV file about your group makeup in the channel: 1 row per group member, and columns: last_name, first_name, email, group_name.

### 201a Project plan

**Due: 2020-10-27**

This is just a message including the following:

- *Description*: (500 words max) describe the research question(s), the data source(s), and how they are related.
- the data

### 201a Preliminary data summaries

**Due: 2020-11-10**

This is an R script (ideally an R markdown file), and output showing coarse summaries of the data (histograms, scatterplots, etc).  This should include various validity checks to figure out if any of the data are corrupted, if co-registration of different data sources was done accurately, etc.  Send the R script and output in the Slack channel.

The only goal here is to force you to look over the data to make sure you find problems early, rather than at the last minute.

### 201a Write-ups

**Due: 2020-12-17, before class**

This should be less than 2000 words, and should include:  

- A description of the research questions: why is this an interesting/important question?  Add a bit of lit review to help others understand what is already known, and what missing information your approach aims to provide.  
- A description of the data, where it came from, any peculiarities about the data structure or collection method, etc.  
- Graphs that try to answer the key research questions, and some explanation about why the graphs answer (or fail to answer) these research questions.  
- Some basic statistics that attempt to quantify the answers to these research questions and the uncertainty associated with these answers (only methods covered in 201a are expected, but by all means do something fancier if you are comfortable)  
- Some discussion about which questions are adequately answered by the methods you used, and which will require a more elaborate analysis.  (e.g., we fit a regression to all the data, but we know there is important substructure that this analysis is ignoring)  

### 201a Presentation

**Due: 2020-12-17, during final time**

Tell us about your results.

This should be like a 10 minute conference talk:

- provide motivation for the question you are asking, and explain why it is worth asking   
- describe (briefly) what was done on this question previously, and why your project fills a gap in knowledge   
- describe the data, any peculiarities therein, and explain why it is a useful data source for answering the question.  what are the linking assumptions?   
- present your results (mostly graphs), and explain what we learn from these graphs.   
- describe the caveats, to set the agenda for what you might try to address in 201b.

The goals here are:

- to teach the audience (us instructors, and the rest of the class) about this research domain/question  
- keep the audience entertained     
- practice working through the logic tying questions to data   
- practice making nice data visualizations in R to highlight the relevant results     
- practice giving talks.   

**Do not** go into agonizing details about the trials and tribulations involved with your R code, your reanalysis, etc.  Just give us your results.

### 201a Group-evaluation

**Due: 2020-12-18**

After you completed your presentation, each student should independently DM Ed on Slack: your group name, and your estimate as to what percentage of the total work each member of your group did.  Basically, I want to know if someone single-handedly carried your group, or if someone was a free-rider and let the rest of the group do all the work.  I'm hoping that there will not be such an uneven distribution of effort, and that the mere fact that these evaluations will be sent will motivate your group not to leave you hanging.

## 201b

In 201b you will start where you left off in 201a: fill in the missing analyses, flesh out the motivation/introduction, add a proper discussion, and hopefully, wind up with a paper that might be submitted for publication with minimal further effort.

## Data sources

The lists below are just pointers, you should do your own googling, and hopefully you will find something new and interesting.   
In general, you will get the most mileage in doing something new by intelligently combining several sources that have not already been combined for you (e.g., cross-linking crime statistics for a city with city demographics, or fluctuations in attitudes over time/geography, with voting patterns at that time/place, etc).

General social science survey: http://www3.norc.org/Gss+website/

data.gov: http://www.data.gov/

census datasets: https://www.census.gov/data/developers/data-sets.html

FBI crime statistics: http://www.fbi.gov/stats-services/crimestats

Perhaps useful forum: http://opendata.stackexchange.com/

List of random data sets: http://rs.io/100-interesting-data-sets-for-statistics/

FAA datasets: https://www.faa.gov/data_research/aviation_data_statistics/data_downloads/

FDIC (retail banking) datasets: https://www2.fdic.gov/idasp/warp_download_all.asp

Fuel economy information from the EPA: http://www.fueleconomy.gov/feg/download.shtml

NIH funding information (may be difficult to pull data into a usable format): http://report.nih.gov/nihdatabook/index.aspx

Personality test data: http://personality-testing.info/_rawdata/

Drug use data: http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/34933?q=&paging.rows=25&sortBy=10

CDC health and nutrition: http://www.cdc.gov/nchs/nhanes/nhanes_questionnaires.htm

American Sign Language corpus: http://www.bu.edu/av/asllrp/dai-asllvd.html

OpenPsychometrics: https://openpsychometrics.org/tests/OSRI/

Large, but likely tricky data set to analyze:   
https://gigaom.com/2014/05/29/more-than-250-million-global-events-are-now-in-the-cloud-for-anyone-to-analyze/

Datasets available upon request:   
Dundee eye-tracking: http://www.dundee.ac.uk/psychology/staff/profile/alan-kennedy

List of somewhat small data sets, more suited to small class examples rather than posing new questions:   
http://www.calvin.edu/~stob/data/

More lists here:   
http://opendata.stackexchange.com/questions/266/a-database-of-open-databases   
http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples

https://github.com/rasbt/pattern_classification/blob/master/resources/dataset_collections.md
https://www.kaggle.com/datasets?sortBy=votes&group=all

https://cseweb.ucsd.edu/~jmcauley/datasets.html

https://aminer.org/citation


Other lists I found while googling for "public social science data sources"   
http://socsciresearch.com/r6.html   
http://ciser.cornell.edu/ASPs/datasource.asp?CATEGORY=2   
http://personality-testing.info/_rawdata/
http://veekaybee.github.io/2018/07/23/small-datasets/
http://blog.yhat.com/posts/7-funny-datasets.html
https://vincentarelbundock.github.io/Rdatasets/datasets.html
http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets
https://www.kaggle.com/datasets

<!--chapter:end:course-projects.Rmd-->

# (PART) Notes

# Getting started with R {#R-start  .unnumbered}

## Installing R {#R-install  .unnumbered}

(1) Download and install R for your system:  [https://cran.rstudio.com/](https://cran.rstudio.com/)

(2) Download and install RStudio for your system: [https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/)

### Packages

We will use a number of packages that extend the functionality of basic R, and make some operations easier/more intuitive.  You can start by installing the tidyverse package using the code below.

```{r eval=FALSE}
install.packages('tidyverse')
```


## Introduction to R  {#R-intro  .unnumbered}

Writing analyses in R is writing code.  If you are new to this notion, you might benefit from this [excellent article on what code is](http://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/), from this discussion of the [two cultures of computer users](http://www.pgbovine.net/two-cultures-of-computing.htm), and from this harsh, but accurate description of [what it takes to *really* learn to  code](http://blog.sailsoftware.co/2015/09/09/what-it-really-takes-to-learn-how-to-code.html)


### Getting started

There's a large set of introductory tutorials to R online, easily accessible via google.

I recommend working through some interactive tutorials to start yourself off:   

- [Try R](tryr.codeschool.com) from codeschool  
- [swirl](http://swirlstats.com/) offers interactive R lessons run within R   
- [datacamp](https://www.datacamp.com/) offers interactive tutorials, but I'm a bit confused what is free, and what requires subscription   
- Here is a handy [tutorial R script](http://learnxinyminutes.com/docs/r/)    
- OpenIntro stats also [R labs](https://www.openintro.org/stat/labs.php?stat_lab_software=R)    
- Rstudio offers a list of such [lessons](https://www.rstudio.com/resources/training/online-learning/) 

As you become familiar with the basics, you may want some quick reference sources.

- Take a look at [Rstudio cheat sheets](https://www.rstudio.com/resources/cheatsheets/), in particular data visualization and data wrangling    
- [This one](https://www.ualberta.ca/~ahamann/teaching/renr690/R_Cheat_Data.pdf) is also useful    
- Take a look at Wickham's list of [basic R functions worth learning](http://adv-r.had.co.nz/Vocabulary.html)   
- [Cookbook for R](http://www.cookbook-r.com/) offers solutions and code snippets for common problems.   

You will need to find help.

- [Google](google.com) "R [what you want to do]"    
- [CrossValidated](http://stats.stackexchange.com/) is a great resource: I often find solutions to my problems there.   

You may also want to consult more advanced lessons to supplement labs/notes:    

- This UBC course offers great notes on [modern and practical R (including ggplot)](http://stat545-ubc.github.io/)    
- Hadley Wickham's advanced R book and [online notes](http://adv-r.had.co.nz/) are very good, but advanced, as described    


To write code well, you will need to know something about how a computer works.

- General [command line](https://en.wikipedia.org/wiki/Command-line_interface) tutorials are good for understanding how CLIs work.  e.g., [learn enough command line to be dangerous](https://www.learnenough.com/command-line-tutorial)   
- Understand your system's directory and file structure and how to navigate it from a console.  In R: getwd(), setwd(), list.files().   

Once you can actually write some code, it is worth learning to make it good.  

- Good code is readable by humans and [self documenting](https://en.wikipedia.org/wiki/Self-documenting)    
- This can be achieved by adopting a consistent and sensible style of code.  A few suggestions: [Google R style guide](https://google-styleguide.googlecode.com/svn/trunk/Rguide.xml), and [Wickham's style guide](http://adv-r.had.co.nz/Style.html).  
- Avoid [magic numbers](https://en.wikipedia.org/wiki/Magic_number_(programming)#Unnamed_numerical_constants).  They make your code hard to read and brittle to change.    
- Use unique and meaningful names for scripts, functions, variables, data.frames, columns, etc.    
- Learn to type well, and pay attention to text details.  In most programming languages, letter order, letter case, visually similar symbols, etc. have to be correct for a computer to understand what you are saying. Human readers are forgiving with typos, computers are not.    
- Learn to use your [IDE](https://en.wikipedia.org/wiki/Integrated_development_environment) (in our case, Rstudio).  [Tab completion](https://support.rstudio.com/hc/en-us/articles/200404846-Working-in-the-Console) is amazing. [Keyboard shortcuts](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts) are very handy.    

## Better data analysis code.

The overarching flow of data analysis is something like:      
data -> `pre-processing code` -> clean data -> `analysis code` -> results -> `presentation code` -> figures, tables, numbers

It is helpful to [factor](https://en.wikipedia.org/wiki/Code_refactoring) your code this way, as it allows you to muck around with various parts without disrupting the others.

A few suggestions for how to write good code for data analysis.   

- Make sure analysis code is state independent (it should re-run correctly after `rm(list=ls())`), and self-sufficient (it should not require any human intervention, mouse-clicks, etc).  All of this ensures that re-running your analysis is not a pain, and is reproducible.    
- Don't arbitrarily create data subsets stored in assorted variables -- that's a great way to make a mess of your code and confuse yourself.  Subset data as needed, while keeping the data frame complete.          
- Build complicated commands piece by piece in the console, then assemble the final compact command in your script.  Especially when using `dplyr` pipes (`%>%`), or nesting functions.     
- When in doubt about whether the code is intuitive, pass named, rather than positional, arguments to functions.    
- Take explicit control of your data types and structures -- don't just assume that when you read in a csv file, all variables, factors, etc. have the correct data type, names, etc.  

<!--chapter:end:notes/R-start.Rmd-->

# Visualizations  {#visualization}

These notes are largely there to provide quick examples of ggplot2 code that generates graphs for particular data.  I assume that the ggplot2 introduction from R4DS got you oriented, but if not, we have a few more notes on the basics of [ggplot](#ggplot).

## General rules for scientific data visualization.

The order of these rules indicates their priority (as I see it): rules further down are superceded by higher up rules if they are in conflict.

1. Everything should be labeled, and interpretable without consulting a figure caption or having to solve a puzzle.

1. Graphs should facilitate *relevant* quantitative interpretation and comparisons.

1. Graphs should represent variability and uncertainty to permit inferential statistics by eye

1. Graphs should follow conventions for the kind of information/data being presented.

1. Graphs should not waste ink and should otherwise look pretty.

## Picking a plot (what's convention)

When you make a plot, you are trying to show the relationship between one or more response/outcome variables, and some explanatory variables (in experimental settings, these are often called dependent and independent variables, respectively).  These variables can be classified as either categorical or numerical.  I find it helpful to think of conventional plot options in terms of a pseudo formula: what kind of response variable is being explained by what types of explanatory variables.  e.g., categorical ~ numerical would mean you are showing how a categorical response variable changes with some numerical explanatory variable.  Most of this document is structured based on such formulas.

## Categorical ~ 0

How do we show the distribution of some categorical variable (that's what I mean by "~0": no explanatory variables)?  The three main plot types to consider here are: histogram, pie chart, and stacked area plot.

Histograms make comparisons of frequency across categories easiest (you just have to compare how tall two bars are).  Pie charts make this somewhat difficult, because we are pretty bad at visually comparing similar angles; however, pie charts do effectively convey the absolute proportion of a category (which is hard to assess in a histogram).  Stacked area plots are a sort of compromise of both: comparisons across categories are harder than in a histogram, but easier than a pie chart; absolute proportion is easier than in a histogram, but harder than a pie chart.


For our plots of a categorical variable with no expanatory variables, we will use the SPSP demographics data.

````{r, warning=F, message=F}
library(tidyverse)
spsp <- read_csv('http://vulstats.ucsd.edu/data/spsp.demographics.cleaned.csv')
spsp %>% group_by(ethnicity) %>% summarize(n=n()) %>% knitr::kable()
````

### Histogram

The classic distribution plot is a histogram: one bar per category, usually arranged along the x-axis, with the y-axis showing frequencies.

````{r, warning=F, message=F, fig.width=3, fig.height=4}
( plot1 <- ggplot(spsp, aes(x=ethnicity, fill=ethnicity)) + 
  geom_bar()+
  ## the lines above form the basis of the plot, 
  ## the lines below just make it look nicer
  scale_fill_brewer(palette = "Set1")+
  theme_bw()+
  theme(legend.position = "none",
        axis.text.x = element_text(angle=90, hjust=1)) )
````

### Pie chart

Pie charts show the proportion of each category as wedges on a circle.  (Pie charts are generally [frowned upon](http://www.businessinsider.com/pie-charts-are-the-worst-2013-6), so you better have a really good reason to use them.)  Also, because they are out of favor, making them in ggplot is a bit weird: you have to make a stacked area plot, then convert the coordinates to polar.

````{r, warning=F, message=F, fig.width=1.5, fig.height=1.5}
( plot2 <- 
  ggplot(spsp, aes(x=factor(1),fill=ethnicity))+
  geom_bar(aes(y = (..count..)/sum(..count..)), width=1)+
  coord_polar(theta="y")+
  ## the lines above form the basis of the plot, 
  ## the lines below just make it look nicer
  scale_fill_brewer(palette = "Set1")+
  theme_bw()+
  theme(legend.position = "none",
        axis.title = element_blank(),
        axis.text = element_blank(),
        panel.grid = element_blank()) )
````


### Stacked area

Think of a stacked area plot as a histogram, with all the bars stacked on top of each other (hopefully, with different colors!).  This plot connects to methods of plotting a categorical variable as a function of some other variables.

````{r, warning=F, message=F, fig.width=4, fig.height=2}
( plot3 <- ggplot(spsp, aes(x=factor(1),fill=ethnicity))+
  geom_bar(width=0.75, stat='count')+
  ## the lines above form the basis of the plot, 
  ## the lines below just make it look nicer
  scale_fill_brewer(palette = "Set1")+
  theme_bw()+
  guides(fill=guide_legend(title="SPSP\nethnicity"))+
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank()) )
````

## numerical ~ 0

To look at the distribution of a numeric variable, we will generally need to do something to smooth or bin over the number line*.  Histograms have explicit bins, and show the frequency of numbers in each bin.  Density plots smooth over the numberline not in explicit bins, but with a continuous kernel.  Histograms do a better job of coneying how noisy your data are, but are quite sensitive to bin placement: they will give you a different impression depending on bin width and bin shifts (e.g., bin 1-2, 2-3, ... vs 0.5-1.5, 1.5-2.5, ....).  Because density plots do not have discrete bins, they are smoother, so there is no notion of shifting a bin, and they tend to be less sensitive to bandwidth; however, they tend to make your data look clean even when they are not.

Let's use some data from a 10 mile race to plot the distribution of a numeric variable.

```{r, warning=F, message=F}
load(url("http://vulstats.ucsd.edu/data/cal1020.cleaned.Rdata"))
glimpse(cal1020)
```

### Histogram & density

```{r, warning=F, message=F, fig.width=6, fig.height=2.5}
plot1 <- ggplot(cal1020, aes(x=time.sec/60))+
  geom_histogram(binwidth = 1)+
    ggtitle('histogram')+
  scale_x_continuous('Minutes', breaks=seq(0,240, by=30))+
  theme_minimal()

plot2 <- ggplot(cal1020, aes(x=time.sec/60))+
  geom_density(fill='gray', alpha=0.5)+
    ggtitle('density')+
  scale_x_continuous('Minutes', breaks=seq(0,240, by=30))+
  theme_minimal()
gridExtra::grid.arrange(plot1, plot2, nrow=1)
```

## numerical ~ categorical

This is the most common data visualization category I see in psychology: how does some numerical variable change as a function of condition (category).  Usually it is portrayed as a bar plot (hopefully with error bars).  But we can do better.

First, let's make all the variations, and put them in one figure (below we just make each plot, and then show them all together.)

### Bar plot with error bars

We adopt the common approach of using the mean +/- 1 standard error of the mean as the error bars here.

```{r, warning=F, message=F}
plot1 <- ggplot(cal1020, aes(x=sex, fill=sex, y=speed.mph))+
  stat_summary(fun.y = mean, 
               geom="bar")+
  stat_summary(fun.data = mean_se,
               geom="errorbar", 
               width=0.5)+
  scale_y_continuous('Avg speed (mph; +/- s.e.m.)', breaks = seq(0, 15, by=1))+
  ggtitle('Barplot')+
  theme_minimal()+
  coord_cartesian(ylim=c(5,7))+
  theme(legend.position = 'none')
```

### Jittered data points.

Data points are jittered, because otherwise they would fall on top of each other, and would be impossible to discern.

```{r, warning=F, message=F}
plot2 <- ggplot(cal1020, aes(x=sex, color=sex, y=speed.mph))+
  geom_jitter(size=0.1, alpha=0.5)+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  ggtitle('Jittered')+
  theme_minimal()+
  theme(legend.position = 'none')
```

### Viola/Violin plot

Like a smoothed density histogram, except it's symmetric, so it looks nicer when many are arrayed side by side.

```{r, warning=F, message=F}
plot3 <- ggplot(cal1020, aes(x=sex, fill=sex, y=speed.mph))+
  geom_violin()+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  ggtitle('Violin')+
  theme_minimal()+
  theme(legend.position = 'none')
```

### Box and whiskers plot

This is a visualization of a bunch of summary statistics of the distribution.  By default, these summary statistics are: the median (middle line), the 25th and 75th percentile (edges of the box), 25th percentile - 1.5(IQR), and 75th percentile + 1.5(IQR) (the whiskers); and it shows the "outliers" (data points that are beyond those IQR intervals.

```{r, warning=F, message=F}
plot4 <- ggplot(cal1020, aes(x=sex, fill=sex, color=sex, y=speed.mph))+
  geom_boxplot(alpha=0.5, outlier.alpha = 0.1)+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  ggtitle('Boxplot')+
  theme_minimal()+
  theme(legend.position = 'none')
```

###  Overlayed densities

(here we flip the coordinates so we can picture them along side the other graphs)

```{r, warning=F, message=F}
plot5 <- ggplot(cal1020, aes(x=speed.mph, fill=sex, color=sex))+
  geom_density(alpha=0.5)+
  coord_flip()+
  scale_x_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  ggtitle('Densities')+
  theme_minimal()+
  theme(legend.position = 'none')
```

###  Empirical cumulative distribution

(here we flip the coordinates so we can picture them along side the other graphs)

```{r, warning=F, message=F}
plot6 <- ggplot(cal1020, aes(x=speed.mph, fill=sex, color=sex))+
  stat_ecdf(geom='line', size=1, alpha=0.75)+
  coord_flip()+
  scale_x_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  scale_y_continuous('CDF', breaks=c(0, 0.5, 1.0))+
  ggtitle('ECDF')+
  theme_minimal()+
  theme(legend.position = 'none')
```

### Comparisons

```{r, warning=F, message=F, fig.width=8, fig.height=3}
gridExtra::grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, nrow=1)
```

The mean + standard error bar plot makes the implicit statistical comparison (t-test) easy to do by eye, but it obscures what the actual data look like (both the underlying variability, as well as it's messiness).  The jittered plot is very faithful to the underlying data, but it's tricky to figure out how the distributions compare.  The violin plot hides some of the data messiness, but does make comparisons easier.  The boxplot is useful for showing the five-number summary, but not much else.  Overlayed density plots are more intuitive than violin plots, but they only work well for a small number of categories.  Empirical CDFs are also only good for a few categories, and their strength is in showing differences in the tails of the distributions.

### Recommendations 

My suggestion is to use a point+range to indicate the mean and standard error (thus facilitating comparisons), and to overlay that on some representaiton of the underlying data.  I recommend using a jittered display of individual data points if you have relatively little data, and a violin plot if you have a lot of data (such that the jitter is very hard to make informative).

Below, to show the few-data-points case, I sub-sample the data.

```{r, warning=F, message=F, fig.width=5, fig.height=3}
# recommendation for few data points
plot1 <- cal1020 %>% 
  sample_n(50) %>%
  ggplot(aes(x=sex, fill=sex, color=sex, y=speed.mph))+
  geom_jitter(width=0.25, size = 0.75, alpha=0.7)+
  stat_summary(fun.data = mean_se,
               geom="pointrange", 
               fatten = 2,
               size=1)+
  scale_y_continuous('Speed (mph +/1 sem)', breaks = seq(0, 15, by=1))+
  theme_minimal()+
  theme(legend.position = 'none')

## Here we will write a function to generate a 95% confidence interval
mean_ci <- function(x){
  m = mean(x)
  se = sd(x)/sqrt(length(x))
  ql = qnorm(1-0.025)
  c('y'=m, 'ymin'=m-ql*se, 'ymax'=m+ql*se)
}
plot2 <- cal1020 %>% 
  ggplot(aes(x=sex, fill=sex, color=sex, y=speed.mph))+
  geom_violin(alpha=0.3)+
  stat_summary(fun.data = mean_ci,
               geom="pointrange",
               fatten = 2,
               size=1)+
  scale_y_continuous('Speed (mph +/- 2 sem)', breaks = seq(0, 15, by=1))+
  theme_minimal()+
  theme(legend.position = 'none')

gridExtra::grid.arrange(plot1, plot2, nrow=1)
```


## numerical ~ numerical (2 x numerical ~ 0)

To show the joint distribution of two numerical variables, or one numerical variable as a function of another, the obvious choice is a scatterplot.  However, a scatterplot, just like a jittered display of data points, is difficult to make usable when you have a lot of data (you need to tinker with point size and point transparency so that the display isn't just an undifferentiated blob).  A 2D histogram, mapping frequency onto color, is a better choice if you have a lot of data.

### Scatter and heatmap

```{r, warning=F, message=F, fig.width=6, fig.height=3.5}
plot1 <- cal1020 %>%
  ggplot(aes(x=age, y=speed.mph, alpha=0.5)) + 
  geom_point(size=0.5, position=position_jitter(0.25))+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  ggtitle('scatterplot')+
  theme_minimal()+
  theme(legend.position = 'none')

plot2 <- cal1020 %>%
  ggplot(aes(x=age, y=speed.mph)) + 
  geom_bin2d()+
  scale_fill_continuous(low="#DDDDDD", high="#000000")+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  ggtitle('heatmap histogram')+
  theme_minimal()
  
gridExtra::grid.arrange(plot1, plot2, nrow=1)
```

### Conditional means

The other popular way to show the relationship between a numerical response (y) and a numerical explanatory variable (x) is to show the mean (plus error bars) of y for a given range of x.  Such conditional means are typically connected with a line, and sometimes even the error bars are connected in a ribbon.  This is the most direct way of showing the "conditional" means, but is generally impractical unless you have a lot of data.  The most common way to show conditional means is to show the fitted y~x line; this takes us further away from the data (because it assumes that the conditional means follow a line), and usually gives us a false impression of how linear the relationship is.  The other common alternative is to do "LOESS" smoothing (locally weighted regression); for each point we are plotting, the LOESS method fits a function to all the data, but weights the data based on how far away it is from the point we are interested; the consequence is a smooth, wiggly line that generally looks nice, but often misleads folks into thinking that they have evidence for non-linearities in their data.

Generally, I would recommend showing the raw data distribution (as in the subsection above), plus a representation of whatever model you fit to those data (if you fit a line, show a line; if you fit a parabola, show the parabola).

```{r, warning=F, message=F, fig.width=8, fig.height=3}
## make bins and calculate mean, se
cal1020.binned <- cal1020 %>% 
  group_by(age.bin = floor(age/5)*5+2.5) %>%
  summarize(mean = mean(speed.mph), sem=sd(speed.mph)/sqrt(n()))
  
plot1 <- cal1020.binned %>% 
  ggplot(aes(x=age.bin, y=mean, alpha=0.5)) + 
  geom_point()+
  geom_line()+
  geom_pointrange(aes(y=mean, ymin=mean-sem, ymax=mean+sem))+
  scale_y_continuous('Mean speed (mph +/- sem)', breaks = seq(0, 15, by=1))+
  ggtitle('y|x')+
  theme_minimal()+
  theme(legend.position = 'none')

plot2 <- cal1020.binned %>% 
  ggplot(aes(x=age.bin, y=mean, alpha=0.5)) + 
  geom_line()+
  geom_point()+
  geom_ribbon(aes(ymin=mean-sem, ymax=mean+sem))+
  scale_y_continuous('Mean speed (mph +/- sem)', breaks = seq(0, 15, by=1))+
  ggtitle('y|x Ribbon')+
  theme_minimal()+
  theme(legend.position = 'none')

plot3 <- cal1020 %>%
  ggplot(aes(x=age, y=speed.mph, alpha=0.5)) + 
  geom_smooth(method='lm', color='red', fill='red', alpha=0.5)+
  scale_y_continuous('Fitted speed (mph + 95% CI)', breaks = seq(0, 15, by=1))+
  ggtitle('Line')+
  theme_minimal()+
  theme(legend.position = 'none')

plot4 <- cal1020 %>%
  ggplot(aes(x=age, y=speed.mph, alpha=0.5)) + 
  geom_smooth(method='loess', color='blue', fill='blue', alpha=0.5)+
  scale_y_continuous('Fitted speed (mph + 95% CI)', breaks = seq(0, 15, by=1))+
  ggtitle('LOESS')+
  theme_minimal()+
  theme(legend.position = 'none')

gridExtra::grid.arrange(plot1, plot2, plot3, plot4, nrow=1)
```

### numerical ~ numerical + categorical

If you add a categorical variable to those numerical~numerical plots, the obvious choice would be to color the points/fitted lines.  However, if you have a lot of data, the scatterplots become unusable, so sometimes it is worth splitting the different categories into facets, as shown below.

```{r, warning=F, message=F, fig.width=8, fig.height=4}

plot1 <- cal1020 %>%
  ggplot(aes(x=age, y=speed.mph, fill=sex, color=sex, group=sex)) + 
  geom_point(size=0.5, alpha=0.5, position=position_jitter(0.25))+
  geom_smooth(method='lm',alpha=0.5)+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  theme_minimal()+
  theme(legend.position = 'none')

plot2 <- cal1020 %>%
  ggplot(aes(x=age, y=speed.mph, fill=sex, color=sex, group=sex)) + 
  facet_grid(~sex)+
  geom_point(size=0.5, alpha=0.5, position=position_jitter(0.25))+
  geom_smooth(method='lm',alpha=0.5)+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  theme_minimal()+
  theme(legend.position = 'none')

gridExtra::grid.arrange(plot1, plot2, nrow=1, widths = c(2, 3))
```


## categorical ~ numerical

It's not very common to see a category distribution as a function of a numerical variable in experimental data, because such visualizations look like noise unless without a *lot* of data.  However, the two obvious options are variations of a stacked area plot.  I recommend the "filled" variety, unless the distribution of the explanatory variable is *really* important to show.

```{r, warning=F, message=F, fig.width=7, fig.height=3}
plot1 <- ggplot(cal1020, aes(x=round(speed.mph*2)/2, fill=sex))+
  geom_bar(position='stack')+
  scale_x_continuous('Avg speed (mph)', breaks = seq(0, 15, by=1))+
  theme_minimal()+
  theme(legend.position = 'none')
plot2 <- ggplot(cal1020, aes(x=round(speed.mph*2)/2, fill=sex))+
  geom_bar(position='fill')+
  scale_x_continuous('Avg speed (mph)', breaks = seq(0, 15, by=1))+
  ylab('proportion')+
  theme_minimal()
gridExtra::grid.arrange(plot1, plot2, nrow=1, widths = c(3,4))
```

## 2 x categorical and categorical ~ categorical

### Heatmap

To show the joint distribution of two categorical variables, the best option is a heatmap.

```{r, warning=F, message=F}
plot1 <- spsp %>% 
  mutate(stage = factor(stage, levels = c("Undergrad", "Grad", "Early Career", "Regular Member", "Retired"))) %>%
  ggplot(aes(x=stage, y=ethnicity))+
      geom_bin2d() +
  scale_fill_continuous(low="#DDDDDD", high="#000000")+
  theme_minimal()+
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle=90, hjust = 1, vjust=1))
```


### categorical ~ categorical

To show how distribution of one categorical variable changes as a function of another, we should use filled stacked area plots.  Note, that since these plots are "filled" along y, we get no sense of the distribution of the x variable, but in such cases we generally care much less about the distribution of x, then about the conditional distribution of y|x.

```{r, warning=F, message=F}
plot1 <- spsp %>% 
  mutate(stage = factor(stage, levels = c("Undergrad", "Grad", "Early Career", "Regular Member", "Retired"))) %>%
  ggplot(aes(x=stage, fill=ethnicity))+
      geom_bar(position='fill') +
  scale_fill_brewer(palette = "Set1")+
  theme_minimal()+
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle=90, hjust = 1, vjust=1))
```

## Extra plot notes.

### numerical ~ 2 x categorical

When plotting one numerical variable as a function of two categorical variables, the most common method is to use color to differentiate adjacent categories on the x-axis.

```{r, warning=F, message=F}

plot1 <- cal1020 %>% 
  mutate(first.1 = substr(name.first, 1, 1)) %>%
  ggplot(aes(x=sex, fill=first.1, color=first.1, y=speed.mph))+
  geom_violin(alpha=0.3,
               position="dodge")+
  # geom_jitter(width=0.25, size = 0.75, alpha=0.7,
  #              position=position_dodge())+
  stat_summary(fun.data = mean_se,
               geom="pointrange", 
               fatten = 2,
               position=position_dodge(width=0.9),
               size=1)+
  scale_y_continuous('Speed (mph)', breaks = seq(0, 15, by=1))+
  theme_minimal()+
  theme(legend.position = 'none')

```





#### Frequency vs Proportion splots

When showing how a categorical variable changes as a function of another variable, we use stacked area plots of one sort or another.  We have a choice about whether to 'stack' or 'fill' the counts.  Meaning, do we show the proportion of category y, without showing the distribution of category x (if we fill), or do we do a worse job conveying the proportions of y, but give a better sense of the distribution of x (if we stack).  Generally, the correct answer is to fill.

```{r, warning=F, message=F}
plot1 <- ggplot(spsp, aes(x=ethnicity, fill=ethnicity)) + 
  geom_bar()+
  ## the lines above form the basis of the plot, 
  ## the lines below just make it look nicer
  scale_fill_brewer(palette = "Set1")+
  theme_bw()+
  theme(legend.position = "none",
        axis.text.x = element_text(angle=90, hjust=1))
plot2  <- ggplot(spsp, aes(x=ethnicity, fill=ethnicity)) + 
      geom_bar(aes(y = (..count..)/sum(..count..)))+
  ## the lines above form the basis of the plot, 
  ## the lines below just make it look nicer
  ylab('proportion')+
  scale_fill_brewer(palette = "Set1")+
  theme_bw()+
  theme(legend.position = "none",
        axis.text.x = element_text(angle=90, hjust=1))
```


### bin width and bandwidths

The plots below will show how histograms and density plots change as a function of bin/band width.

```{r, warning=F, message=F}
plots = list()
i = 1
for(W in c(1, 5, 15)){
  plots[[i]] <- ggplot(cal1020, aes(x=time.sec/60))+
  geom_histogram(binwidth = W)+
  scale_x_continuous('Minutes', breaks=seq(0,240, by=30))+
  theme_minimal()
  i <- i+1
}

for(W in c(1, 5, 15)){
  plots[[i]] <- ggplot(cal1020, aes(x=time.sec/60))+
  geom_density(bw = W, fill='gray', alpha=0.5)+
  scale_x_continuous('Minutes', breaks=seq(0,240, by=30))+
  theme_minimal()
  i <- i+1
}

#do.call(gridExtra::grid.arrange, c(plots, ncol=3))
```

<!--chapter:end:notes/visualizations.Rmd-->

## ggplot {#visualization}

We will teach you to use [ggplot](http://ggplot2.org/) for data visualization.

It has a number of advantages once you are familiar with it.  However, it has a somewhat steep learning curve.

Making plots entirely by writing code will seem weird at first -- you will want to interact with the plot by clicking on it in various ways, and you can't.  However, the advantage of writing code to make your plots is that once the data changes (you get more, or you decide to filter it one way or another, or adopt one transformation or another), you will not need to recreate the plots manually.  This will seem like a negligible advantage to you now, but trust me, this will save you time in the end.

Generally, many of the things we cover are described a bit differently in this [ggplot tutorial](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/) and on the [ggplot cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/08/ggplot2-cheatsheet.pdf).  Lots more resources may be found by googling.

### Installation.

If you do not already have ggplot installed, install it with 

````{r eval=FALSE}
install.packages('ggplot2')
````

You will need to load the ggplot library (and also grid, and gridExtra):

````{r}
library(ggplot2)
library(grid)
library(gridExtra)
````

### Basic overview.

**ggplot works on *data frames***: you give it a data frame and tell it how the various columns of the data frame should be mapped on to display properties.

**We specify the *aesthetic* mapping** using the `aes()` to say how various data.frame columns should be used to generate a display.  For instance, the x coordinate is one aesthetic property, the color is another property, as is size, or fill, or style of point.

**We then specify what kinds of geometric entities to add to the plot**, which will follow the aesthetic mapping we described.

Thus, to produce a scatter plot of two vectors x and y, instead of running a scatterplot(x,y) command of some sort, we would need to put them into a data frame, specify that ggplot should use that data frame, should map the x column on to x, and the y column on to y, and then add a "point" geometric entity.

````{r}
x = rnorm(100)
y = 0.5*x + rnorm(100)*0.25

df.xy = data.frame(x=x, y=y)

ggplot(data = df.xy, mapping = aes(x=x, y=y)) + geom_point()
````

The `ggplot(data = df.xy, mapping = aes(x=x, y=y)) + geom_point()` line is doing a few things:  
`ggplot(data = df.xy, ` tells ggplot to use df.xy as the data frame (the `data=` part can often be dropped)  
`mapping = aes(x=x, y=y))` specifies that the x coordinate should be obtained from the x column of the data frame, and the y coordinate from the y column.  
`+ geom_point()`  says that we should add points that follow that aesthetic mapping.  

If we change the geometric entity from `geom_point`, we will produce a different graph.  For instance, `geom_line()` will connect the points with lines:

````{r}
ggplot(data = df.xy, mapping = aes(x=x, y=y)) + geom_line()
````

And we can add multiple geometric entities all following the same aesthetic mapping: 

````{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point() + 
  geom_line()
````

Note that here we did two things to make the code eaiser to read.  (1) we dropped the `data = ` and `mapping = ` part, as the order is sufficient to indicate which input to `ggplot()` is the data frame, and which is the mapping), and (2) we made the command span multiple lines by entering line breaks after the +s.  While this is not strictly necessary, it makes the code easier to work with.

### Changing labels.

We can add a title, and change the x and y label text.

````{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point() + 
  geom_line() +
  ggtitle("Variable y as a function of variable x")+
  xlab("This is the x variable") +
  ylab("This is the y variable")
````

### Themes and elements and properties.

We can change the way in which various aspects of the display look, which ggplot refers to as the [theme](http://docs.ggplot2.org/current/theme.html) of the plot.

There are many properties to the theme that are *hierarchically organized*.  For instance the `title` property applies to all title text in the plot, including the plot title (`plot.title`), the axis titles (`axis.title`), the legend title (`legend.title`); thus if we were to set the title property to have green font, that would apply to all titles (unless we issued specific instructions to override this for more specific titles).

Changing such theme properties is achieved with the `theme()` command, in which we specify which property of the plot we want to alter, and we set it by invoking a particular element type, initiated with whatever we want to change.  Titles are text, so we change them by setting `title=element_text(...)` where the stuff in `...` includes the  [element_text](http://docs.ggplot2.org/current/element_text.html) properties and the values we set them to.  Such a command will override the properties that we set, but preserve the ones we did not set.

````{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point() + 
  geom_line() +
  ggtitle("Variable y as a function of variable x")+
  xlab("This is the x variable") +
  ylab("This is the y variable") +
  theme(title = element_text(color="green"))
````

Despite the fact that this is hideous, a few things are worth noting: (a) by setting the color of the `title` property, we influenced the color of the plot title, the x axis title, and the y axis title, and (b) we only changed the *color* of all of those, and they retained their differences in font size.  The values we set at a particular level of the plot hierarchy propagate down to all children, but this only influences the property that we changed.

The [theme properties](http://docs.ggplot2.org/current/theme.html) can be partitioned into properties of the axes (`axis.*`, like `axis.text` for the labels for axis tick marks, `axis.title` for axis title, or more specific ones like `axis.title.x`), properties of the legend (`legend.*`), properties of the graph area (`panel.*`), properties of the whole image (`plot.*`), and properties of the "strip" that gives identifying information in multi-panel plots (`strip.*`).  

A fairly decent publication-grade plot theme can be created by simply invoking the black and white theme (`theme_bw()`):

````{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point() + 
  geom_line() +
  ggtitle("Variable y as a function of variable x")+
  xlab("This is the x variable") +
  ylab("This is the y variable") + 
  theme_bw()
````

Generally, I like somewhat larger axis text and titles, and no grid lines, so I might favor something like:  

````{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point() + 
  geom_line() +
  ggtitle("Variable y as a function of variable x")+
  xlab("This is the x variable") +
  ylab("This is the y variable") + 
  theme_bw() + 
  theme(panel.grid = element_blank(), 
        axis.text = element_text(size=12),
        title = element_text(size=16),
        axis.title = element_text(size=14))
````

We can also save a particular theme to reuse later:

```{r}
eds_theme <- theme_bw() + 
  theme(panel.grid = element_blank(), 
        axis.text = element_text(size=12),
        title = element_text(size=16),
        axis.title = element_text(size=14))

ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point() + 
  geom_line() +
  ggtitle("Variable y as a function of variable x")+
  xlab("This is the x variable") +
  ylab("This is the y variable") + 
  eds_theme
````

### Changing display properties of plotted elements.

We can also change properties like size and color of the plot elements.  There are two ways to do so: we might make those properties vary with some aspect of the data by including them in `aes()`, or we can make them be fixed by including them outside of `aes()`.

For instance, if we want the size and color of the points to vary with the y value, we might write:

```{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point(aes(color=y, size=y)) + 
  eds_theme
````

But if we wanted to simply set a particular size and color for those points, we would note it outside of the aesthetic mapping in `aes()`:

```{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point(color="red", size=6) + 
  eds_theme
````

Other notable properties of `geom_point()` include their [shape](http://www.cookbook-r.com/Graphs/Shapes_and_line_types/), their border color and fill color (for shapes 21-25), and their transparency (alpha).  Besides color, width ("size"), and transparency ("alpha"), `geom_line()` has a ['linetype'](http://www.cookbook-r.com/Graphs/Shapes_and_line_types/) property.

So we can make quite a crazy plot by manipulating them all (and this is without even mapping them to data values in aes!)

```{r}
ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point(color="navy", size=10, alpha=0.5, shape=22, fill="steelblue") + 
  geom_line(color="green", size=2, linetype="dotdash", alpha=0.5) +
  eds_theme
````

### Other geometric elements.

Besides `geom_line` and `geom_point`, there are many other geometric elements we might plot (enumerated on the main [reference page](http://docs.ggplot2.org/current/)).  We will cover these as we need to as we go over specific visualization types.

### Assembling multiple plots in one figure

Often we want to make a few plots and attach them together.  For instance, we might want to see the histograms of x, y, and their scatterplot.  This can be accomplished with the `gridExtra` package, which you can install with `install.packages('gridExtra')`.

The strategy is to define the various plots, and save them to variables, then put them together with `grid.arrange()`.

```{r}
library(gridExtra)

hx <- ggplot(df.xy, aes(x=x)) + 
  geom_histogram(fill='green') + 
  theme_bw()

hy <- ggplot(df.xy, aes(x=y)) + 
  geom_histogram(fill='purple') + 
  theme_bw()

xy <- ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point(color="red", size=6) + 
  theme_bw()

grid.arrange(hx,hy,xy,ncol=3)
```

We might rather want the x and y histograms to align with the x and y axes of the scatter plot, so we might want to arrange this as a 2x2 grid, while flipping the coordinates of the y histogram (note that we need a blank panel for the upper right).

```{r}
hx <- ggplot(df.xy, aes(x=x)) + 
  geom_histogram(fill='green') + 
  theme_bw()

hy <- ggplot(df.xy, aes(x=y)) + 
  geom_histogram(fill='purple') + 
  coord_flip() +                              # this flips the histogram sideways.
  theme_bw()

xy <- ggplot(df.xy, aes(x=x, y=y)) + 
  geom_point(color="red", size=3) + 
  theme_bw()

grid.arrange(hx,
             grid.rect(gp=gpar(col="white")),  # this is just a white blank.
             xy,
             hy, 
             ncol=2)
```

There is still lots left to fix here, and I will walk through the process of making this figure presentable [here](vis.tinkering.html).  In the meantime, lets move on.

<!--chapter:end:notes/vis.ggplot.Rmd-->

