[["index.html", "UCSD Psyc 201ab / CSS 205 / Psyc 193 Welcome", " UCSD Psyc 201ab / CSS 205 / Psyc 193 Welcome This is the website for Psych 201a, 201b; they are presumed to be taken as a series with 201a in the fall, and 201b in the winter. 201a is also offered as an advanced undergraduate course (Psych 193), and will be crosslisted as the core stats class for the Computational Social Science MS program (CSS 205). 201a covers statistical graphics, probability, classical statistical methods, with an emphasis on the general linear model, and their implementation in R. 201b covers more advanced models and methods, including generalized linear models, multilevel/hierarchical regression, and computational approaches such as numerical optimization, Monte Carlo, and resampling. 201b is not offered in the 2021-2022 academic year "],["course-syllabus.html", "Syllabus Instructors Class meetings Grading Class Resources", " Syllabus Instructors Instructor Office hours Ed Vul Thursdays 1-2pm in 5137 McGill Wenhao (James) Qi Thursdays 10-11am in 3509 Mandler Class meetings Class meetings will be in 3545 Mandler Hall. Lectures are Tuesday / Thursday 2-4. Labs are Wednesdays 5-7. Attendance is not required, but is generally helpful. Grading PSYC 201 / CSS 205: Homework (25%), Midterm (25%), Final (25%), Project (25%). PSYC 193: Homework (75%), Project (25%). (no midterm or final) Details: Homework: These are entirely in R, on our online system. It makes sense to seek help from other students when you get stuck, but make sure you do everything yourself, so that you will be able to do it on your own in exams, and more importantly, real life. Exams: Midterm and Final (both take home). Working together on exams is prohibited. Group project: You will analyze some data, write a final report, and make a presentation. Details here. Class Resources Class Campuswire: Sign up here, code is 2647. All class communication will be here. Homework assignments: login (this may be buggy, please do not hesitate to tell us if something is wrong. ) Login is your ucsd username, password is full student ID, including letter. Readings: Generally we will draw on a number of sources, and our own notes, as needed for whatever we are covering. Software: We are using R and the Rstudio IDE. Our own installation instructions and list of starter resources: Getting started with R "],["course-201a.html", "201a Schedule Week 0: Introduction Week 1: Data Week 2: Visualization Week 3: Theoretical foundations Week 4: Linear model: Regression Week 5: Linear model: Multiple regression Week 6: Linear model: Categorical predictors Week 7: Linear model: ANCOVA, diagnostics Week 8: Linear model: Linearizing transforms Week 9: Covarying errors (repeated measures / random effects) Week 10: Review and preview", " 201a Schedule Week 0: Introduction In which we will cover the goals of this class, and where the class materials fit into the broader landscape of quantitative / computational / data skills. Readings Basic introduction to R / Rstudio: R4DS Sections: 1, 4, 6, 8 Homework Pre-survey HW00: Swirl (due 9/30) Thursday Overview: slides Week 1: Data In which we cover data organization, cleaning, and basic summaries, while getting acquainted with R syntax. Readings Working with data: R4DS Sections: 5, 7, 11, 12 Homework HW01: Data cleaning (due 10/11) Tuesday Data overview: slides live code Wednesday 9/29 R basics (continued) [files] Thursday Data summaries, data frames, and dplyr: slides live code Week 2: Visualization In which we cover how to make scientifically useful graphs. Readings [notes] R4DS: 2, 3 socviz: make a plot (the rest of this book may also be useful, but we don’t have time for a thorough treatment.) Homework HW02: Data visualization (due 10/15) Tuesday visualization, ggplot #1: slides , code Wednesday 10/6 Data visualization [code] [answers] Thursday Week 3: Theoretical foundations In which we cover probability theory, and the logic of classical statistical methods. Readings Probability notes: terms basics conditional monte-carlo random-variables distribution functions expectation central limit theorem, normal NHST notes: via simulation sampling distribution basic NHST via normal power, effects bonus: via binomial Homework HW03: NHST probability (due 10/22) Tuesday slides: probability Wednesday Probability [code] [answers] Thursday slides: NHST Week 4: Linear model: Regression Notes t-distribution t-tests (note: some of this will be covered in week 6 and week 10) bivariate data anscombe’s quartet covariance correlation ordinary least-squares regression yx,xy,pca coefficient of determination significance testing of these measures prediction regression diagnostics Homework Regression Due: 2020-11-04 Tuesday slides Wednesday Regression [code] [answers] Thursday slides Week 5: Linear model: Multiple regression Notes multiple regression Tuesday slides code Wednesday Multiple regression [code] [answers] Thursday slides Week 6: Linear model: Categorical predictors Readings / Notes Notes on t-tests [howell ch.13 on ANOVA] [howell ch.16 on ANCOVA] Homework Multiple regression Due: 2020-11-20 Tuesday slides Thursday slides Week 7: Linear model: ANCOVA, diagnostics Homework ANOVA &amp; ANCOVA Due: 2020-11-30 Tuesday slides Wednesday ANOVA [code] [answers] Thursday slides Week 8: Linear model: Linearizing transforms Homework Linearizing transforms Due: 2020-12-03 Tuesday slides Week 9: Covarying errors (repeated measures / random effects) Readings: I don’t like either of these…. I am still on the hunt for a pithy conceptual overview of repeated measures designs and analyses: This is simpler: Howell, ch. 14 This is mathier: Kutner ch. 27 Tuesday slides Wednesday Repeated measures [code] [answers] Readings [Howell (basic)] [Kutner (mathy)] Week 10: Review and preview Tuesday Mixed effects model brief Project Q&amp;A, Review Q&amp;A slides Wednesday R Review (for final) Thursday Review "],["course-projects.html", "Projects Examples of this sort of thing 201a 201b Data sources", " Projects The goal is to analyze a large, rich dataset to answer an interesting behavioral/social/neural question, with the final product being a potentially publishable paper. This project is divided into two phases to be implemented in 201a and 201b. In 201a your goal is to identify a conjunction of an interesting question and a data source that might answer it. You will need to understand the data, clean it, make graphs of the data that might answer the question, and do simple analyses to get your bearings. In 201b you will do the more complete analyses, likely using more advanced methods that we will cover in 201b, and turn the initial report from 201a into something that could be submitted for publication. Examples of this sort of thing skill learning in online games sequential dependence in yelp reviews stereotype threat in chess play income mobility over time scaling laws in cities crowd within in real estimation personality in blog posts neurosynth brain mapping example hashtag adoption cultural tastes via baby names You will notice that particularly successful examples usually have a combination of a few things: a coherent research question, with a good justification for why the naturalistic data maps onto theoretical constructs of interest; a novel dataset, which might mean data that had not preciously been available, or a dataset that was created by cleverly combining/co-registering previously separate datasets; and (sometimes or) a fairly sophisticated analysis that adequately grapples with the complicated structure of the data. The full project will span both 201a and 201b (previously I had it only in 201b, and that was not enough time) 201a 201a Timeline Figure out groups as soon as possible, so we can assist if folks are group-less. 2020-10-20: Groups due 2020-10-27: Project plan due 2020-11-10: Preliminary data summaries due 2020-12-17: (before final) Write-ups due 2020-12-17: (during final) Project presentations 2020-12-18: Group-evaluation due Groups You will be in groups of ~3-5 (5 if group includes an undergrad). Undergrads should not be in a group together (should join a group of grad students). Hopefully you can self-assemble into groups, but I will help if need be. Due: 2020-10-20 Create a Slack channel titled “project-[groupname]” (can be either public or private). Invite Ed and James into the channel. Upload a CSV file about your group makeup in the channel: 1 row per group member, and columns: last_name, first_name, email, group_name. 201a Project plan Due: 2020-10-27 This is just a message including the following: Description: (500 words max) describe the research question(s), the data source(s), and how they are related. the data 201a Preliminary data summaries Due: 2020-11-10 This is an R script (ideally an R markdown file), and output showing coarse summaries of the data (histograms, scatterplots, etc). This should include various validity checks to figure out if any of the data are corrupted, if co-registration of different data sources was done accurately, etc. Send the R script and output in the Slack channel. The only goal here is to force you to look over the data to make sure you find problems early, rather than at the last minute. 201a Write-ups Due: 2020-12-17, before class This should be less than 2000 words, and should include: A description of the research questions: why is this an interesting/important question? Add a bit of lit review to help others understand what is already known, and what missing information your approach aims to provide. A description of the data, where it came from, any peculiarities about the data structure or collection method, etc. Graphs that try to answer the key research questions, and some explanation about why the graphs answer (or fail to answer) these research questions. Some basic statistics that attempt to quantify the answers to these research questions and the uncertainty associated with these answers (only methods covered in 201a are expected, but by all means do something fancier if you are comfortable) Some discussion about which questions are adequately answered by the methods you used, and which will require a more elaborate analysis. (e.g., we fit a regression to all the data, but we know there is important substructure that this analysis is ignoring) 201a Presentation Due: 2020-12-17, during final time Tell us about your results. This should be like a 10 minute conference talk: provide motivation for the question you are asking, and explain why it is worth asking describe (briefly) what was done on this question previously, and why your project fills a gap in knowledge describe the data, any peculiarities therein, and explain why it is a useful data source for answering the question. what are the linking assumptions? present your results (mostly graphs), and explain what we learn from these graphs. describe the caveats, to set the agenda for what you might try to address in 201b. The goals here are: to teach the audience (us instructors, and the rest of the class) about this research domain/question keep the audience entertained practice working through the logic tying questions to data practice making nice data visualizations in R to highlight the relevant results practice giving talks. Do not go into agonizing details about the trials and tribulations involved with your R code, your reanalysis, etc. Just give us your results. 201a Group-evaluation Due: 2020-12-18 After you completed your presentation, each student should independently DM Ed on Slack: your group name, and your estimate as to what percentage of the total work each member of your group did. Basically, I want to know if someone single-handedly carried your group, or if someone was a free-rider and let the rest of the group do all the work. I’m hoping that there will not be such an uneven distribution of effort, and that the mere fact that these evaluations will be sent will motivate your group not to leave you hanging. 201b In 201b you will start where you left off in 201a: fill in the missing analyses, flesh out the motivation/introduction, add a proper discussion, and hopefully, wind up with a paper that might be submitted for publication with minimal further effort. Data sources The lists below are just pointers, you should do your own googling, and hopefully you will find something new and interesting. In general, you will get the most mileage in doing something new by intelligently combining several sources that have not already been combined for you (e.g., cross-linking crime statistics for a city with city demographics, or fluctuations in attitudes over time/geography, with voting patterns at that time/place, etc). General social science survey: http://www3.norc.org/Gss+website/ data.gov: http://www.data.gov/ census datasets: https://www.census.gov/data/developers/data-sets.html FBI crime statistics: http://www.fbi.gov/stats-services/crimestats Perhaps useful forum: http://opendata.stackexchange.com/ List of random data sets: http://rs.io/100-interesting-data-sets-for-statistics/ FAA datasets: https://www.faa.gov/data_research/aviation_data_statistics/data_downloads/ FDIC (retail banking) datasets: https://www2.fdic.gov/idasp/warp_download_all.asp Fuel economy information from the EPA: http://www.fueleconomy.gov/feg/download.shtml NIH funding information (may be difficult to pull data into a usable format): http://report.nih.gov/nihdatabook/index.aspx Personality test data: http://personality-testing.info/_rawdata/ Drug use data: http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/34933?q=&amp;paging.rows=25&amp;sortBy=10 CDC health and nutrition: http://www.cdc.gov/nchs/nhanes/nhanes_questionnaires.htm American Sign Language corpus: http://www.bu.edu/av/asllrp/dai-asllvd.html OpenPsychometrics: https://openpsychometrics.org/tests/OSRI/ Large, but likely tricky data set to analyze: https://gigaom.com/2014/05/29/more-than-250-million-global-events-are-now-in-the-cloud-for-anyone-to-analyze/ Datasets available upon request: Dundee eye-tracking: http://www.dundee.ac.uk/psychology/staff/profile/alan-kennedy List of somewhat small data sets, more suited to small class examples rather than posing new questions: http://www.calvin.edu/~stob/data/ More lists here: http://opendata.stackexchange.com/questions/266/a-database-of-open-databases http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples https://github.com/rasbt/pattern_classification/blob/master/resources/dataset_collections.md https://www.kaggle.com/datasets?sortBy=votes&amp;group=all https://cseweb.ucsd.edu/~jmcauley/datasets.html https://aminer.org/citation Other lists I found while googling for “public social science data sources” http://socsciresearch.com/r6.html http://ciser.cornell.edu/ASPs/datasource.asp?CATEGORY=2 http://personality-testing.info/_rawdata/ http://veekaybee.github.io/2018/07/23/small-datasets/ http://blog.yhat.com/posts/7-funny-datasets.html https://vincentarelbundock.github.io/Rdatasets/datasets.html http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets https://www.kaggle.com/datasets "],["r-homework.html", "R homework Submitting R Assignments", " R homework You will have homework roughly every week. Assignments will be submitted through the homework interface. On this page you will find a list of the current and past R assignments - this will tell you (a) whether it is complete, incomplete, or past due, and (b) when it is due. Typically (but not always), assignments will be due half an hour before the lab: 5:00pm on Wednesday. This will not always be the case, so be sure to check the statistics server or assignment pages. You may only submit assignments in the period that they are assigned - typically they open immediately after the relevant lab. You may find assignments posted on this website before they are open - I put these here to give you a sense of what you need to know, but I reserve the right to make changes until it is posted on the statistics server. So just because it can be found here does not mean that you can submit your homework yet. Grading The R assignments are criterion-based: you need to get a certain percentage of the questions correct before you get credit (this percentage varies and is noted on the statistics server). If you fail to reach this cutoff, you are allowed to resubmit as many times as you would like until the deadline. Once you meet the criterion, you get a 100% for that assignment. We also want to encourage generalizability of your scripts - after all, the reason to use R scripts is so that you can repeat the analysis easily, including if you get more data or your assumptions change. In each assignment you will be asked to load in some data - make sure that you use the variables and data you loaded rather than hard-coding any numbers. When your assignment is graded, you will be notified of what your answer is versus the correct answer for that data. However, your assignment will also be tested against another dataset, for which you will only be told if your code extends correctly or not. You need your code to work for both datasets in order to get an answer right. Collaboration Working with other people will make learning R easier and more fun (unless you pick a bad group…) - thus collaboration is encouraged on these assignments. However, we want you to learn how to use R, not just your friends. Thus the script you submit should be your own - you are not allowed to write a script as a group and each submit that. Instead, for maximal learning, we suggest you first attempt the assignment on your own. If you can’t get it by yourself, then get together with a group to discuss it. But don’t write code as a group - instead, take notes and write your own code afterward. In addition, there is lots of R code on the internet for many different tasks. You are allowed to use this code, provided you (a) credit the original authors, and (b) can explain how it works - yes, I will know if you use outside code, even if you don’t cite it, and yes I will quiz you on it. As a final warning: submitting a script that is not your own or contains outside code without citations is a violation of the honor code. Don’t violate the honor code. Submitting R Assignments R assignments are submitted through the homework interface: vulstats.ucsd.edu/hw/ Writing R scripts The first step in submitting an assignment is writing it. In order for the server to grade your homework, you need to write it so that: 1) The code is error-free 2) Any data or files you load are saved in the same folder as your script (don’t use the setwd() function) 3) You don’t use any outside packages - only ones that have been discussed in class 4) You have answered every question - all ans variables you are asked to fill in are set The easiest way to check whether your code has errors is to: 1) Restart RStudio 2) Clear all extraneous variables (use the broom icon in your workspace) 3) Source the script If you don’t get any errors after doing this, and you have answered all of the questions (all of the answer variables are filled), then you should go ahead and submit your assignment. Submitting your assignment To submit R assignments, sign in to the statistics server, and go to the bottom of the page. Under the R assignments section you will find a list of past and present assignments. Each assignment is color coded: if it is red, you still need to do it; if it is green, you have finished it; if it is black, it is past due. You can also get information about each assignment, including the assignment name, the percent correct you need for that assignment, when it is due, how many times you have attempted it, and when it was completed (if it is done). Find the assignment you want to submit, then click on the upload button (as below) Upload Next, you will be able to upload your script. To do so, click on the ‘Choose File’ button as shown below. Choose file This will open a file browser. Find your R script for the current assignment, select it, then press ‘Open.’ Choose file When you get back, the Filename field should display the name of your R script. Click ‘Submit’ to upload and submit. Submit You may see the following screen if your script takes a long time to run. Don’t close your browser or go back. If your script takes too long, you will get an error - if so, check your script for things that might be taking up too much time (e.g. running ten-millions simulations instead of ten-thousand). Running Assuming all is successful, you will see a screen similar to the one below. However, you may also get an error if something goes wrong. If you are having trouble figuring out what the error is, check here. Success The top of this final page will tell you whether you have passed the homework or not. You can get more details about the individual answers in the table below. Each row corresponds to a single answer variable, and each cell is color-coded. If the ‘Answer’ column is green, your script gets the right answer for the data you were given, and the extension test data. ‘Your answer’ and ‘Correct answer’ come from the data you were given - this lets you know how close you got. Both cells will be green if you got the right answer here. The ‘Works on test?’ column lets you know whether your script extends to the test data - it will be green if so. If you get the correct answer on the given data but the wrong answer on the extension test data, you should check for any numbers you may have hard coded. Additional resources. The R homepage: http://www.r-project.org/ R-Tutor: a useful website with basic introductions to R commands. Reading the relevant pages from here prior to lab will be very helpful to familiarize yourself with what will be covered: http://www.r-tutor.com/ A cheat-sheet with a number of common R commands: http://cran.r-project.org/doc/contrib/Short-refcard.pdf Quick-R: a good site if you already know some statistics and programming but don’t yet know R: http://www.statmethods.net/index.html The R Inferno: a more advanced guide for troubleshooting. Once you have a good grasp of R, this can help you figure out why something isn’t working as expected: http://www.burns-stat.com/pages/Tutor/R_inferno.pdf Google “r tutorial” and you will see lots of additional support. "],["part-notes.html", "(PART) Notes", " (PART) Notes "],["R-start.html", "Getting started with R Installing R Introduction to R Better data analysis code. Using R-markdown", " Getting started with R Installing R Download and install R for your system: https://cran.rstudio.com/ Download and install RStudio for your system: https://www.rstudio.com/products/rstudio/download/ Packages We will use a number of packages that extend the functionality of basic R, and make some operations easier/more intuitive. You can start by installing the tidyverse package using the code below. install.packages(&#39;tidyverse&#39;) Introduction to R Writing analyses in R is writing code. If you are new to this notion, you might benefit from this excellent article on what code is, from this discussion of the two cultures of computer users, and from this harsh, but accurate description of what it takes to really learn to code Getting started There’s a large set of introductory tutorials to R online, easily accessible via google. I recommend working through some interactive tutorials to start yourself off: Try R from codeschool swirl offers interactive R lessons run within R datacamp offers interactive tutorials, but I’m a bit confused what is free, and what requires subscription Here is a handy tutorial R script OpenIntro stats also R labs Rstudio offers a list of such lessons As you become familiar with the basics, you may want some quick reference sources. Take a look at Rstudio cheat sheets, in particular data visualization and data wrangling This one is also useful Take a look at Wickham’s list of basic R functions worth learning Cookbook for R offers solutions and code snippets for common problems. You will need to find help. Google “R [what you want to do]” CrossValidated is a great resource: I often find solutions to my problems there. You may also want to consult more advanced lessons to supplement labs/notes: This UBC course offers great notes on modern and practical R (including ggplot) Hadley Wickham’s advanced R book and online notes are very good, but advanced, as described To write code well, you will need to know something about how a computer works. General command line tutorials are good for understanding how CLIs work. e.g., learn enough command line to be dangerous Understand your system’s directory and file structure and how to navigate it from a console. In R: getwd(), setwd(), list.files(). Once you can actually write some code, it is worth learning to make it good. Good code is readable by humans and self documenting This can be achieved by adopting a consistent and sensible style of code. A few suggestions: Google R style guide, and Wickham’s style guide. Avoid magic numbers. They make your code hard to read and brittle to change. Use unique and meaningful names for scripts, functions, variables, data.frames, columns, etc. Learn to type well, and pay attention to text details. In most programming languages, letter order, letter case, visually similar symbols, etc. have to be correct for a computer to understand what you are saying. Human readers are forgiving with typos, computers are not. Learn to use your IDE (in our case, Rstudio). Tab completion is amazing. Keyboard shortcuts are very handy. Better data analysis code. The overarching flow of data analysis is something like: data -&gt; pre-processing code -&gt; clean data -&gt; analysis code -&gt; results -&gt; presentation code -&gt; figures, tables, numbers It is helpful to factor your code this way, as it allows you to muck around with various parts without disrupting the others. A few suggestions for how to write good code for data analysis. Make sure analysis code is state independent (it should re-run correctly after rm(list=ls())), and self-sufficient (it should not require any human intervention, mouse-clicks, etc). All of this ensures that re-running your analysis is not a pain, and is reproducible. Don’t arbitrarily create data subsets stored in assorted variables – that’s a great way to make a mess of your code and confuse yourself. Subset data as needed, while keeping the data frame complete. Build complicated commands piece by piece in the console, then assemble the final compact command in your script. Especially when using dplyr pipes (%&gt;%), or nesting functions. When in doubt about whether the code is intuitive, pass named, rather than positional, arguments to functions. Take explicit control of your data types and structures – don’t just assume that when you read in a csv file, all variables, factors, etc. have the correct data type, names, etc. Using R-markdown We have been using R-markdown throughout this class–the website, lecture notes and lab notes were all written in R-markdown (as are these instructions). R-markdown allows you to embed code and figures in easy to read text documents. This makes it much easier to present data to collaborators and yourself (and us who will be grading your assignments!). You should have R-markdown and knitr installed, but in case you dont: To install R-markdown, execute: install.packages(&quot;rmarkdown&quot;) You will also want to install the knitr package. This will allow you to turn Rmd files into pretty html files. install.packages(&quot;knitr&quot;) In broad strokes, you will want to: Create an Rmd file by going to “File,” selecting “New File” and the “R Markdown.” Alternatively, modify this file. Write text as would in a Word or Text document. If you want to add formatting, check the cheat sheet or the tutorial/basics Insert chunks of code by selecting “Command + Shift + I” on a Mac or “Control + Alt + I” on a PC . This will create a code block demarcated by three backticks, starting with ```{r} and ending with ```, rendered in grey in Rstudio. When you run the Rmd script, R-markdown will execute any code you’ve written in this space. See here for more details. Click “Knit HTML” on the top-bar to turn your Rmd file into a pretty html file (it will be created in the same folder). NOTE: please use echo=TRUE in your global options so that the code prints with the output when you render your html. Here are some useful links * A brief tutorial * R markdown cheat sheet * More throrough reference "],["data-cleaning.html", "Data cleaning – worked example Read file with readr First glimpse at the data. What we want to do rename columns convert time columns into seconds. Extract sex from division, and other clues Get corral from bib number fix the age column (which is a string for some reason) Get time out of the mangled 5.25 and 10 mile columns. Get rid of some unnecessary columns Getting new columns from simple transformations Fixing states. Save the cleaned data.", " Data cleaning – worked example We will practice reading and cleaning data using the published race data for the California “10 20” running event (10 miles, 20 bands). These data were scraped from the official race website, then fixed up a bit and saved as a tab delimited text file. This text file can be found here. We will use the various tidyverse packages to carry out this cleaning. library(tidyverse) Read file with readr First, let’s read the data in using readr::read_tsv, creating a data-frame saved as cal1020. “tsv” generally refers to “tab separated values,” which is what we have. cal1020 = readr::read_tsv(&#39;http://vulstats.ucsd.edu/data/cal10202.txt&#39;) The readr functions (such as read_csv, read_tsv, etc.) are very similar to the base R functions (read.csv), but they have more sensible defaults, and also save the data as a “tibble,” which works just like a data frame, but has a few added bells and whistles. First glimpse at the data. What’s in this data frame? We can check with str(), or a slightly neater display using the dplyr::glimpse() function. glimpse(cal1020) ## Rows: 3,252 ## Columns: 22 ## $ time.gun &lt;time&gt; 00:48:00, 00:48:05, 00:49:30, 00:51:03, 00:51:24, 00:53:26, 00:53:43, 00:54:09, 00:54:49, 00:58:49, 00:55… ## $ time.chip &lt;time&gt; 00:48:00, 00:48:05, 00:49:30, 00:51:02, 00:51:23, 00:53:26, 00:53:42, 00:54:01, 00:54:49, 00:55:18, 00:55… ## $ bib &lt;dbl&gt; 1205, 9, 13, 15, 1303, 1213, 3, 1055, 12, 1351, 1054, 1216, 1352, 1218, 6, 1220, 1304, 18, 1353, 1305, 101… ## $ name.first &lt;chr&gt; &quot;Jordan&quot;, &quot;Macdonard&quot;, &quot;Sergio&quot;, &quot;Jamesom&quot;, &quot;Darren&quot;, &quot;Okwaro&quot;, &quot;Steven&quot;, &quot;Edwin&quot;, &quot;Lindsey&quot;, &quot;Derek&quot;, &quot;Ra… ## $ name.last &lt;chr&gt; &quot;Chipangama&quot;, &quot;Ondara&quot;, &quot;Reyes&quot;, &quot;Mora&quot;, &quot;Brown&quot;, &quot;Raura&quot;, &quot;Underwood&quot;, &quot;Figueroa&quot;, &quot;Scherf&quot;, &quot;Bradley&quot;, &quot;… ## $ City &lt;chr&gt; &quot;Flagstaff&quot;, &quot;Grand Prairie&quot;, &quot;Palmdale&quot;, &quot;Arroyo Grande&quot;, &quot;Solana Beach&quot;, &quot;Oceanside&quot;, &quot;Encinitas&quot;, &quot;Comm… ## $ State &lt;chr&gt; &quot;AZ&quot;, &quot;TX&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;AZ&quot;, &quot;?&quot;,… ## $ Division &lt;chr&gt; &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile M … ## $ Class.Position &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 4, 1, 3, 2, 3, 1, 4, 1, 2, 1, 2, 5, 1, 1, 1, 5, 3, 1, 3, 1, 2… ## $ Overall.Place &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 1, 8, 9, 2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2… ## $ Age &lt;chr&gt; &quot;25&quot;, &quot;29&quot;, &quot;32&quot;, &quot;30&quot;, &quot;28&quot;, &quot;39&quot;, &quot;26&quot;, &quot;42&quot;, &quot;27&quot;, &quot;33&quot;, &quot;60&quot;, &quot;34&quot;, &quot;33&quot;, &quot;39&quot;, &quot;26&quot;, &quot;32&quot;, &quot;41&quot;, &quot;24&quot;… ## $ Zip &lt;chr&gt; &quot;86004&quot;, &quot;75054&quot;, &quot;93551&quot;, &quot;93420&quot;, &quot;92075&quot;, &quot;92057&quot;, &quot;92024&quot;, &quot;90040&quot;, &quot;12440&quot;, &quot;92024&quot;, &quot;91016&quot;, &quot;92078&quot;… ## $ Sex.place &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 1, 1, 8, 2, 9, 10, 11, 2, 12, 13, 3, 14, 15, 16, 4, 17, 18, 19, 20, 5, 6, 7, 8, 21, 2… ## $ Div.tot &lt;dbl&gt; 3239, 3239, 3239, 3239, 3239, 3239, 3239, 13, 3239, 3239, 13, 3239, 3239, 3239, 3239, 3239, 3239, 3239, 32… ## $ Sex.tot &lt;dbl&gt; 1179, 1179, 1179, 1179, 1179, 1179, 1179, 11, 2060, 1179, 11, 1179, 1179, 1179, 2060, 1179, 1179, 2060, 11… ## $ AG.tot &lt;dbl&gt; 5, 5, 5, 5, 5, 118, 85, 1, 5, 139, 1, 139, 139, 118, 5, 139, 3, 5, 3, 3, 167, 5, 178, 167, 191, 191, 5, 81… ## $ `5.25.mile.rank` &lt;chr&gt; &quot;3&quot;, &quot;2&quot;, &quot;4&quot;, &quot;6&quot;, &quot;5&quot;, &quot;7&quot;, &quot;8&quot;, &quot;1&quot;, &quot;9&quot;, &quot;10&quot;, &quot;INC&quot;, &quot;11&quot;, &quot;13&quot;, &quot;15&quot;, &quot;12&quot;, &quot;16&quot;, &quot;19&quot;, &quot;14&quot;, &quot;18&quot;, … ## $ `5.25.mile.time` &lt;chr&gt; &quot;25:12:00&quot;, &quot;25:12:00&quot;, &quot;25:31:00&quot;, &quot;26:47:00&quot;, &quot;26:46:00&quot;, &quot;27:41:00&quot;, &quot;27:54:00&quot;, &quot;29:10:00&quot;, &quot;28:41:00&quot;… ## $ `5.25.mile.pace` &lt;chr&gt; &quot;4:48/M&quot;, &quot;4:48/M&quot;, &quot;4:52/M&quot;, &quot;5:06/M&quot;, &quot;5:06/M&quot;, &quot;5:16/M&quot;, &quot;5:19/M&quot;, &quot;5:33/M&quot;, &quot;5:28/M&quot;, &quot;5:28/M&quot;, NA, &quot;5… ## $ `10.mile.rank` &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;1&quot;, &quot;8&quot;, &quot;9&quot;, &quot;13&quot;, &quot;10&quot;, &quot;12&quot;, &quot;15&quot;, &quot;16&quot;, &quot;13&quot;, &quot;11&quot;, &quot;25&quot;, &quot;18&quot;, &quot;1… ## $ `10.mile.time` &lt;chr&gt; &quot;22:47&quot;, &quot;22:52&quot;, &quot;23:58&quot;, &quot;24:14:00&quot;, &quot;24:36:00&quot;, &quot;25:44:00&quot;, &quot;25:47:00&quot;, &quot;24:50:00&quot;, &quot;26:07:00&quot;, &quot;26:35:… ## $ `10.mile.pace` &lt;chr&gt; &quot;2:17/M&quot;, &quot;2:17/M&quot;, &quot;2:24/M&quot;, &quot;2:25/M&quot;, &quot;2:28/M&quot;, &quot;2:34/M&quot;, &quot;2:35/M&quot;, &quot;2:29/M&quot;, &quot;2:37/M&quot;, &quot;2:40/M&quot;, &quot;5:32/… From this we learn lots of things, the most immediately relevant are: the names of the variables (columns); we could have gotten just this part by running names(cal1020)) cal1020 is a data frame with 22 variables (columns) and 3252 observations (rows); we could have gotten just this part by running dim(cal1020) all the variables are saved as either a “chr” (a character string), an “int” (an integer), or a “time” variable. (if we had used the base R read.tsv, many of the strings would have been converted to factors, and the variables recognized as “time” would have been treated as strings as well). Before we work on fixing this, let’s look at some of the rows in the data. Looking at 3252 observations at once is silly, so let’s just look at the first 10 using the head() command. Because there are 22 variables, the printout doesn’t fit. In R markdown we get a fancy scrollable table, but in the console you would just get a truncated list (this behavior differs slightly between data frames and “tibbles,” with tibbles yielding more legible console output). head(cal1020, n = 10) ## # A tibble: 10 x 22 ## time.gun time.chip bib name.first name.last City State Division Class.Position Overall.Place Age Zip Sex.place Div.tot ## &lt;time&gt; &lt;time&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 48&#39;00&quot; 48&#39;00&quot; 1205 Jordan Chipangama Flagst… AZ 10 Mile O… 1 1 25 86004 1 3239 ## 2 48&#39;05&quot; 48&#39;05&quot; 9 Macdonard Ondara Grand … TX 10 Mile O… 2 2 29 75054 2 3239 ## 3 49&#39;30&quot; 49&#39;30&quot; 13 Sergio Reyes Palmda… CA 10 Mile O… 3 3 32 93551 3 3239 ## 4 51&#39;03&quot; 51&#39;02&quot; 15 Jamesom Mora Arroyo… CA 10 Mile O… 4 4 30 93420 4 3239 ## 5 51&#39;24&quot; 51&#39;23&quot; 1303 Darren Brown Solana… CA 10 Mile O… 5 5 28 92075 5 3239 ## 6 53&#39;26&quot; 53&#39;26&quot; 1213 Okwaro Raura Oceans… CA 10 Mile M… 1 6 39 92057 6 3239 ## 7 53&#39;43&quot; 53&#39;42&quot; 3 Steven Underwood Encini… CA 10 Mile M… 1 7 26 92024 7 3239 ## 8 54&#39;09&quot; 54&#39;01&quot; 1055 Edwin Figueroa Commer… CA Wheelchai… 1 1 42 90040 1 13 ## 9 54&#39;49&quot; 54&#39;49&quot; 12 Lindsey Scherf High F… NY 10 Mile O… 1 8 27 12440 1 3239 ## 10 58&#39;49&quot; 55&#39;18&quot; 1351 Derek Bradley Encini… CA 10 Mile M… 1 9 33 92024 8 3239 ## # … with 8 more variables: Sex.tot &lt;dbl&gt;, AG.tot &lt;dbl&gt;, 5.25.mile.rank &lt;chr&gt;, 5.25.mile.time &lt;chr&gt;, 5.25.mile.pace &lt;chr&gt;, ## # 10.mile.rank &lt;chr&gt;, 10.mile.time &lt;chr&gt;, 10.mile.pace &lt;chr&gt; What we want to do rename some poorly named columns convert variables saved as time objects into numbers (in seconds) extract the runner’s sex from the “Division” string, or other clues extract the runner’s “corral” number from the bib number fix the age column, which, for some reason is a “character,” which means that some entries could not be parsed as numbers. get time out of the somewhat mangled 5.25 and 10.mile columns (the pace columns are really mangled for 10 miles, so we will recalculate them ourselves). get rid of unnecessary columns. calculate any other convenience variables rename columns To rename columns, we will use the dplyr rename function. cal1020 &lt;- rename(cal1020, time.mile.5.25 = `5.25.mile.time`, time.mile.10 = `10.mile.time`) There’s lots more columns to rename, but let’s worry about those later, after we drop all the columns we won’t use. I wanted to rename these now, to avoid dealing working with the backticks when converting these two columns into seconds. convert time columns into seconds. The time.gun and time.chip columns seem to have been correctly parsed into time objects, so we can just cast them as.numeric to get the number of seconds they measure. We will use the dplyr::mutate function to change these columns (if we used a new column name, this command would just make a new column, but since those names exist, it assigns new values to them). cal1020 &lt;- mutate(cal1020, time.gun = as.numeric(time.gun), time.chip = as.numeric(time.chip)) The times recorded part of the way through the race are kind of messed up, so they could not be parsed into time objects automatically. Let’s try to figure out exactly how they are messed up. Three asides here: we use the dplyr %&gt;% pipe operator to pass the output of one command as the first input into the next command. so the command above is equivalent to glimpse(select(cal1020, starts_with(\"time.mile\"))), but is much easier to read. we use the dplyr select() function to choose a subset of columns, by name. we use the dplyr starts_with() command to identify column names that start with “time.mile” So, this allows us to just look at those two columns of interest. cal1020 %&gt;% select(starts_with(&quot;time.mile&quot;)) %&gt;% glimpse() ## Rows: 3,252 ## Columns: 2 ## $ time.mile.5.25 &lt;chr&gt; &quot;25:12:00&quot;, &quot;25:12:00&quot;, &quot;25:31:00&quot;, &quot;26:47:00&quot;, &quot;26:46:00&quot;, &quot;27:41:00&quot;, &quot;27:54:00&quot;, &quot;29:10:00&quot;, &quot;28:41:00&quot;, … ## $ time.mile.10 &lt;chr&gt; &quot;22:47&quot;, &quot;22:52&quot;, &quot;23:58&quot;, &quot;24:14:00&quot;, &quot;24:36:00&quot;, &quot;25:44:00&quot;, &quot;25:47:00&quot;, &quot;24:50:00&quot;, &quot;26:07:00&quot;, &quot;26:35:00… So, the 5.25 mile time measurement seems to have recorded minutes in the first part, then seconds, then zeros. the 10 mile time measurement seems to have done something similar, but dropped the zeros for some cases. However, these are just the first few people we are looking at, if we look at the tail() of the data, we see that this coding seems to change for people who took more than an hour! cal1020 %&gt;% select(starts_with(&quot;time.mile&quot;)) %&gt;% tail() ## # A tibble: 6 x 2 ## time.mile.5.25 time.mile.10 ## &lt;chr&gt; &lt;chr&gt; ## 1 1:31:59 1:36:12 ## 2 ? 3:08:42 ## 3 1:46:27 1:30:38 ## 4 1:34:21 1:55:48 ## 5 1:34:21 1:55:54 ## 6 ? 3:30:18 So, to these strings are sometimes “?” (missing data), sometimes in a “mm:ss:00” format, sometimes in a “mm:ss” format, and sometimes in a “hh:mm:ss” format. To convert these into seconds, we will need to detect whether the string is missing, which format is being used, and do the appropriate calculation. We do this in a later section, as it is a bit more complicated than everything else we are undertaking… Extract sex from division, and other clues We do not yet have an explicit coding of runner sex, but the division column gives us a pretty big hint. Let’s look at the unique division names: unique(cal1020$Division) ## [1] &quot;10 Mile Overall&quot; &quot;10 Mile M 35-39&quot; &quot;10 Mile M 25-29&quot; &quot;Wheelchair Top Fin&quot; &quot;10 Mile M 30-34&quot; &quot;10 Mile Masters&quot; ## [7] &quot;10 Mile M 50-54&quot; &quot;10 Mile M 45-49&quot; &quot;10 Mile M 40-44&quot; &quot;10 Mile F 20-24&quot; &quot;10 Mile F 35-39&quot; &quot;10 Mile F 30-34&quot; ## [13] &quot;10 Mile F 25-29&quot; &quot;10 Mile M 15-19&quot; &quot;10 Mile M 55-59&quot; &quot;10 Mile F 15-19&quot; &quot;10 Mile F 40-44&quot; &quot;10 Mile F 50-54&quot; ## [19] &quot;10 Mile M 20-24&quot; &quot;10 Mile F 45-49&quot; &quot;10 Mile M 12-14&quot; &quot;10 Mile M 60-64&quot; &quot;10 Mile F 55-59&quot; &quot;10 Mile M 65-69&quot; ## [25] &quot;10 Mile F 60-64&quot; &quot;Wheelchair M 1-99&quot; &quot;10 Mile M 0- 0&quot; &quot;10 Mile M 70-74&quot; &quot;Wheelchair F 1-99&quot; &quot;10 Mile F 65-69&quot; ## [31] &quot;10 Mile F 12-14&quot; &quot;10 Mile M 75-79&quot; &quot;10 Mile F 70-74&quot; &quot;10 Mile F 75-79&quot; &quot;10 Mile M 80-99&quot; It seems that if the Division name includes \" M \" (with spaces!), we are talking about a male. If it includes \" F \", we are talking about a female. We can use the stringr::str_detect command to do this. We will assign these two values independently, to make sure we didn’t overlook something, before merging them into one column. cal1020 &lt;- cal1020 %&gt;% mutate(is.male = stringr::str_detect(cal1020$Division, &quot; M &quot;), is.female = stringr::str_detect(cal1020$Division, &quot; F &quot;), sex = case_when( is.male &amp; is.female ~ &quot;both&quot;, is.male ~ &quot;male&quot;, is.female ~ &quot;female&quot;, TRUE ~ &quot;neither&quot;)) Now let’s see how many of each category we have. In theory, we should have only “male” and “female” labels (not because of some claim about transgender folks in the world at large, but because that’s how the division sex categorization works in this data set). We use the dplyr::count(X) command, which is a shortcut for group_by(X) %&gt;% summarize(n=n()). cal1020 %&gt;% count(sex) ## # A tibble: 3 x 2 ## sex n ## &lt;chr&gt; &lt;int&gt; ## 1 female 2054 ## 2 male 1180 ## 3 neither 18 Ok, so we don’t have any folks who were categorized as both male and female, but we have 18 folks who were not categorized as either based on division name. Who are they, and can we figure out their sex? cal1020 %&gt;% filter(sex == &#39;neither&#39;) %&gt;% select(Overall.Place, name.first, Division, Sex.tot) %&gt;% head(18) ## # A tibble: 18 x 4 ## Overall.Place name.first Division Sex.tot ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Jordan 10 Mile Overall 1179 ## 2 2 Macdonard 10 Mile Overall 1179 ## 3 3 Sergio 10 Mile Overall 1179 ## 4 4 Jamesom 10 Mile Overall 1179 ## 5 5 Darren 10 Mile Overall 1179 ## 6 1 Edwin Wheelchair Top Fin 11 ## 7 8 Lindsey 10 Mile Overall 2060 ## 8 2 Ralph Wheelchair Top Fin 11 ## 9 13 Natasha 10 Mile Overall 2060 ## 10 15 Jeff 10 Mile Masters 1179 ## 11 16 Sarah 10 Mile Overall 2060 ## 12 17 Brian 10 Mile Masters 1179 ## 13 18 Robert 10 Mile Masters 1179 ## 14 20 Ariana 10 Mile Overall 2060 ## 15 25 Tori 10 Mile Overall 2060 ## 16 40 Deeann 10 Mile Masters 2060 ## 17 43 Theresa 10 Mile Masters 2060 ## 18 46 Celestine 10 Mile Masters 2060 These people all have really high placements, and Division names that reflect that they were in a special Division (professionals, masters, etc). We can try to guess their sex based on their name, but we can also notice that the “Sex.tot” column seems to be correlated with sex. It should, since it should tell us exactly how many competitors there were of the same sex as a particular runner. Let’s check: cal1020 %&gt;% count(sex,Sex.tot) ## # A tibble: 8 x 3 ## sex Sex.tot n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 female 2 2 ## 2 female 2060 2052 ## 3 male 5 5 ## 4 male 11 9 ## 5 male 1179 1166 ## 6 neither 11 2 ## 7 neither 1179 8 ## 8 neither 2060 8 So, those that were labeled as female (based on division name), have either 2 or 2060 as Sex.tot, and males have either 5, 11, or 1179. This is a bit weird. Let’s try to figure out what these different Sex.tot groups mean. cal1020 %&gt;% filter(Sex.tot %in% c(2, 5, 11)) %&gt;% select(Sex.tot, name.first, Division, sex) %&gt;% arrange(Sex.tot) %&gt;% head(20) ## # A tibble: 18 x 4 ## Sex.tot name.first Division sex ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2 Lauren Wheelchair F 1-99 female ## 2 2 Karen Wheelchair F 1-99 female ## 3 5 Unknown 10 Mile M 0- 0 male ## 4 5 Unknown 10 Mile M 0- 0 male ## 5 5 Unknown 10 Mile M 0- 0 male ## 6 5 Unknown 10 Mile M 0- 0 male ## 7 5 Unknown 10 Mile M 0- 0 male ## 8 11 Edwin Wheelchair Top Fin neither ## 9 11 Ralph Wheelchair Top Fin neither ## 10 11 N. Mauricio Wheelchair M 1-99 male ## 11 11 Chuck Wheelchair M 1-99 male ## 12 11 Laird Wheelchair M 1-99 male ## 13 11 Riley Wheelchair M 1-99 male ## 14 11 James Wheelchair M 1-99 male ## 15 11 David Wheelchair M 1-99 male ## 16 11 Jake Wheelchair M 1-99 male ## 17 11 Randy Wheelchair M 1-99 male ## 18 11 Douglas Wheelchair M 1-99 male Ok, so Sex.tot=2 are the female Wheelchair racers, Sex.tot=11 are the male Wheelchair racers, Sex.tot=5 are some class of weird male runners who are missing a bunch of information, sex.tot=1179 is just male runners, and Sex.tot 2060 is female runners. Let’s assign a new sex column based on Sex.tot, and compare the two, to make sure we didn’t mess anything up. cal1020 &lt;- cal1020 %&gt;% mutate(sex.2 = case_when( Sex.tot %in% c(2, 2060) ~ &#39;female&#39;, Sex.tot %in% c(11, 5, 1179) ~ &#39;male&#39;, TRUE ~ &#39;neither&#39;)) cal1020 %&gt;% count(sex, sex.2, match=sex==sex.2) ## # A tibble: 4 x 4 ## sex sex.2 match n ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; ## 1 female female TRUE 2054 ## 2 male male TRUE 1180 ## 3 neither female FALSE 8 ## 4 neither male FALSE 10 So: we get exactly the same categorization from Sex.tot, except we can also correctly classify the folks who could not be identified from the Division name. Great, let’s reassign the sex.2 column to sex. While we are at it, let’s make a column that tells us whether someone was a wheelchair racer. cal1020 &lt;- cal1020 %&gt;% mutate(sex = sex.2, wheelchair = stringr::str_detect(Division, &quot;Wheelchair&quot;)) Get corral from bib number We are told that bib numbers correspond to starting corral, such that numbers 0-999 are in the first corral, 1000-1999 are in the second, etc. Let’s make a new column indicating corral number: cal1020 &lt;- mutate(cal1020, corral = as.integer(floor(cal1020$bib/1000))) fix the age column (which is a string for some reason) Let’s see what’s happening with the age column, because it is a character, that means that at least some elements could not be interpreted as a number. If we force the age column to be a number, they will show up as NAs. So let’s see what information we would be throwing out by converting to NA: cal1020 %&gt;% filter(is.na(as.numeric(Age))) %&gt;% select(Age, name.first) ## # A tibble: 5 x 2 ## Age name.first ## &lt;chr&gt; &lt;chr&gt; ## 1 ? Unknown ## 2 ? Unknown ## 3 ? Unknown ## 4 ? Unknown ## 5 ? Unknown Ah, so the only problem are the missing values, so we can just convert age to a number, since we want the “?” cells to be NA. cal1020 &lt;- mutate(cal1020, Age = as.numeric(Age)) Get time out of the mangled 5.25 and 10 mile columns. cal1020 %&gt;% select(starts_with(&quot;time.mile&quot;)) %&gt;% tail() ## # A tibble: 6 x 2 ## time.mile.5.25 time.mile.10 ## &lt;chr&gt; &lt;chr&gt; ## 1 1:31:59 1:36:12 ## 2 ? 3:08:42 ## 3 1:46:27 1:30:38 ## 4 1:34:21 1:55:48 ## 5 1:34:21 1:55:54 ## 6 ? 3:30:18 Remember, these strings are in a weird format: either “?” (missing data), “mm:ss:00” , “mm:ss,” or “hh:mm:ss.” We need to detect the format, and do the appropriate calculation. The function below does exactly that. parseWeirdTime &lt;- function(string){ if(string == &quot;?&quot;){ return(NA) } else { sub.strings &lt;- as.numeric(stringr::str_split(string, &quot;:&quot;)[[1]]) if((length(sub.strings) == 2) || (sub.strings[3]==0 &amp; sub.strings[1]&gt;15)){ # we are either in &quot;mm:ss&quot; or &quot;mm:ss:00&quot; format hours = 0 minutes &lt;- sub.strings[1] seconds &lt;- sub.strings[2] } else if(sub.strings[1] &lt; 5) { # we are in &quot;hh:mm:ss&quot; format hours &lt;- sub.strings[1] minutes &lt;- sub.strings[2] seconds &lt;- sub.strings[3] } else { # if we see a number like 07:34:00, # it doesn&#39;t make sense as either hh:mm:ss (too much time) # nor as mm:ss (too short for running ~5 miles) print(paste0(&quot;string format doesn&#39;t make sense: &quot;, string)) return(NA) } return(hours*60*60 + minutes*60 + seconds) } } parseWeirdTime(&quot;?&quot;) ## [1] NA parseWeirdTime(&quot;30:44:00&quot;) ## [1] 1844 parseWeirdTime(&quot;30:44&quot;) ## [1] 1844 parseWeirdTime(&quot;2:30:44&quot;) ## [1] 9044 parseWeirdTime(&quot;14:30:00&quot;) ## [1] &quot;string format doesn&#39;t make sense: 14:30:00&quot; ## [1] NA How does this work? 1. it takes a string, and checks if it is equal to “?” if so, it returns NA, which is a special R data type that means “not available,” and indicates that the data are missing. 2. stringr::str_split(string, \":\") splits strings into vectors of strings using “:” as a separator. 3. however, that function returns a list of vectors, and we just want the first element of that list with [[1]] (since our function is written to operate on just one string at a time) 4. as.numeric converts the individual strings to numbers. 5. we check if those numbers make sense as either “mm:ss” or as “hh:mm:ss,” and assign hours/minutes/second accordingly. 6. we then calculate seconds from the hours, minutes, seconds, and return it. Now we want to run this function on each element of the “time.mile” columns. However, this function is not “vectorized,” so we have to apply it to each element in isolation. the various “map” and “apply” functions are very well suited for this task. we use purr::map_dbl, to apply our function to a particular column, and return a list of numbers. cal1020 &lt;- mutate(cal1020, time.mile.5.25 = map_dbl(cal1020$time.mile.5.25, parseWeirdTime), time.mile.10 = map_dbl(cal1020$time.mile.10, parseWeirdTime)) glimpse(cal1020) ## Rows: 3,252 ## Columns: 28 ## $ time.gun &lt;dbl&gt; 2880, 2885, 2970, 3063, 3084, 3206, 3223, 3249, 3289, 3529, 3323, 3575, 3600, 3625, 3421, 3648, 3659, 3485… ## $ time.chip &lt;dbl&gt; 2880, 2885, 2970, 3062, 3083, 3206, 3222, 3241, 3289, 3318, 3320, 3363, 3388, 3413, 3421, 3435, 3445, 3485… ## $ bib &lt;dbl&gt; 1205, 9, 13, 15, 1303, 1213, 3, 1055, 12, 1351, 1054, 1216, 1352, 1218, 6, 1220, 1304, 18, 1353, 1305, 101… ## $ name.first &lt;chr&gt; &quot;Jordan&quot;, &quot;Macdonard&quot;, &quot;Sergio&quot;, &quot;Jamesom&quot;, &quot;Darren&quot;, &quot;Okwaro&quot;, &quot;Steven&quot;, &quot;Edwin&quot;, &quot;Lindsey&quot;, &quot;Derek&quot;, &quot;Ra… ## $ name.last &lt;chr&gt; &quot;Chipangama&quot;, &quot;Ondara&quot;, &quot;Reyes&quot;, &quot;Mora&quot;, &quot;Brown&quot;, &quot;Raura&quot;, &quot;Underwood&quot;, &quot;Figueroa&quot;, &quot;Scherf&quot;, &quot;Bradley&quot;, &quot;… ## $ City &lt;chr&gt; &quot;Flagstaff&quot;, &quot;Grand Prairie&quot;, &quot;Palmdale&quot;, &quot;Arroyo Grande&quot;, &quot;Solana Beach&quot;, &quot;Oceanside&quot;, &quot;Encinitas&quot;, &quot;Comm… ## $ State &lt;chr&gt; &quot;AZ&quot;, &quot;TX&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;AZ&quot;, &quot;?&quot;,… ## $ Division &lt;chr&gt; &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile M … ## $ Class.Position &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 4, 1, 3, 2, 3, 1, 4, 1, 2, 1, 2, 5, 1, 1, 1, 5, 3, 1, 3, 1, 2… ## $ Overall.Place &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 1, 8, 9, 2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2… ## $ Age &lt;dbl&gt; 25, 29, 32, 30, 28, 39, 26, 42, 27, 33, 60, 34, 33, 39, 26, 32, 41, 24, 42, 48, 51, 33, 46, 50, 44, 44, 26… ## $ Zip &lt;chr&gt; &quot;86004&quot;, &quot;75054&quot;, &quot;93551&quot;, &quot;93420&quot;, &quot;92075&quot;, &quot;92057&quot;, &quot;92024&quot;, &quot;90040&quot;, &quot;12440&quot;, &quot;92024&quot;, &quot;91016&quot;, &quot;92078&quot;… ## $ Sex.place &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 1, 1, 8, 2, 9, 10, 11, 2, 12, 13, 3, 14, 15, 16, 4, 17, 18, 19, 20, 5, 6, 7, 8, 21, 2… ## $ Div.tot &lt;dbl&gt; 3239, 3239, 3239, 3239, 3239, 3239, 3239, 13, 3239, 3239, 13, 3239, 3239, 3239, 3239, 3239, 3239, 3239, 32… ## $ Sex.tot &lt;dbl&gt; 1179, 1179, 1179, 1179, 1179, 1179, 1179, 11, 2060, 1179, 11, 1179, 1179, 1179, 2060, 1179, 1179, 2060, 11… ## $ AG.tot &lt;dbl&gt; 5, 5, 5, 5, 5, 118, 85, 1, 5, 139, 1, 139, 139, 118, 5, 139, 3, 5, 3, 3, 167, 5, 178, 167, 191, 191, 5, 81… ## $ `5.25.mile.rank` &lt;chr&gt; &quot;3&quot;, &quot;2&quot;, &quot;4&quot;, &quot;6&quot;, &quot;5&quot;, &quot;7&quot;, &quot;8&quot;, &quot;1&quot;, &quot;9&quot;, &quot;10&quot;, &quot;INC&quot;, &quot;11&quot;, &quot;13&quot;, &quot;15&quot;, &quot;12&quot;, &quot;16&quot;, &quot;19&quot;, &quot;14&quot;, &quot;18&quot;, … ## $ time.mile.5.25 &lt;dbl&gt; 1512, 1512, 1531, 1607, 1606, 1661, 1674, 1750, 1721, 1722, NA, 1752, 1754, 1760, 1753, 1790, 1816, 1758, … ## $ `5.25.mile.pace` &lt;chr&gt; &quot;4:48/M&quot;, &quot;4:48/M&quot;, &quot;4:52/M&quot;, &quot;5:06/M&quot;, &quot;5:06/M&quot;, &quot;5:16/M&quot;, &quot;5:19/M&quot;, &quot;5:33/M&quot;, &quot;5:28/M&quot;, &quot;5:28/M&quot;, NA, &quot;5… ## $ `10.mile.rank` &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;1&quot;, &quot;8&quot;, &quot;9&quot;, &quot;13&quot;, &quot;10&quot;, &quot;12&quot;, &quot;15&quot;, &quot;16&quot;, &quot;13&quot;, &quot;11&quot;, &quot;25&quot;, &quot;18&quot;, &quot;1… ## $ time.mile.10 &lt;dbl&gt; 1367, 1372, 1438, 1454, 1476, 1544, 1547, 1490, 1567, 1595, 3320, 1610, 1633, 1652, 1667, 1644, 1628, 1726… ## $ `10.mile.pace` &lt;chr&gt; &quot;2:17/M&quot;, &quot;2:17/M&quot;, &quot;2:24/M&quot;, &quot;2:25/M&quot;, &quot;2:28/M&quot;, &quot;2:34/M&quot;, &quot;2:35/M&quot;, &quot;2:29/M&quot;, &quot;2:37/M&quot;, &quot;2:40/M&quot;, &quot;5:32/… ## $ is.male &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, F… ## $ is.female &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, … ## $ sex.2 &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, … ## $ wheelchair &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ corral &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1… Let’s now check if this gave us sensible results. In theory, time for the first 5.25 miles, and the time between mile 5.25 and mile 10, should add up to the total time (time.chip; modulo some rounding error). If they do not, something is wrong. off.by.2sec = abs((cal1020$time.mile.5.25 + cal1020$time.mile.10) - cal1020$time.chip) &gt; 2 sum(is.na(off.by.2sec)) # number of rows with missing data which precluded calculation ## [1] 66 sum(!(off.by.2sec), na.rm=T) # number of rows where we were not off by more than 2 seconds ## [1] 3186 sum((off.by.2sec), na.rm=T) # number of rows where we were off by more than 2 seconds ## [1] 0 So it looks like everything worked out! Get rid of some unnecessary columns So, here’s where we are now: glimpse(cal1020) ## Rows: 3,252 ## Columns: 28 ## $ time.gun &lt;dbl&gt; 2880, 2885, 2970, 3063, 3084, 3206, 3223, 3249, 3289, 3529, 3323, 3575, 3600, 3625, 3421, 3648, 3659, 3485… ## $ time.chip &lt;dbl&gt; 2880, 2885, 2970, 3062, 3083, 3206, 3222, 3241, 3289, 3318, 3320, 3363, 3388, 3413, 3421, 3435, 3445, 3485… ## $ bib &lt;dbl&gt; 1205, 9, 13, 15, 1303, 1213, 3, 1055, 12, 1351, 1054, 1216, 1352, 1218, 6, 1220, 1304, 18, 1353, 1305, 101… ## $ name.first &lt;chr&gt; &quot;Jordan&quot;, &quot;Macdonard&quot;, &quot;Sergio&quot;, &quot;Jamesom&quot;, &quot;Darren&quot;, &quot;Okwaro&quot;, &quot;Steven&quot;, &quot;Edwin&quot;, &quot;Lindsey&quot;, &quot;Derek&quot;, &quot;Ra… ## $ name.last &lt;chr&gt; &quot;Chipangama&quot;, &quot;Ondara&quot;, &quot;Reyes&quot;, &quot;Mora&quot;, &quot;Brown&quot;, &quot;Raura&quot;, &quot;Underwood&quot;, &quot;Figueroa&quot;, &quot;Scherf&quot;, &quot;Bradley&quot;, &quot;… ## $ City &lt;chr&gt; &quot;Flagstaff&quot;, &quot;Grand Prairie&quot;, &quot;Palmdale&quot;, &quot;Arroyo Grande&quot;, &quot;Solana Beach&quot;, &quot;Oceanside&quot;, &quot;Encinitas&quot;, &quot;Comm… ## $ State &lt;chr&gt; &quot;AZ&quot;, &quot;TX&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;AZ&quot;, &quot;?&quot;,… ## $ Division &lt;chr&gt; &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile M … ## $ Class.Position &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 4, 1, 3, 2, 3, 1, 4, 1, 2, 1, 2, 5, 1, 1, 1, 5, 3, 1, 3, 1, 2… ## $ Overall.Place &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 1, 8, 9, 2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2… ## $ Age &lt;dbl&gt; 25, 29, 32, 30, 28, 39, 26, 42, 27, 33, 60, 34, 33, 39, 26, 32, 41, 24, 42, 48, 51, 33, 46, 50, 44, 44, 26… ## $ Zip &lt;chr&gt; &quot;86004&quot;, &quot;75054&quot;, &quot;93551&quot;, &quot;93420&quot;, &quot;92075&quot;, &quot;92057&quot;, &quot;92024&quot;, &quot;90040&quot;, &quot;12440&quot;, &quot;92024&quot;, &quot;91016&quot;, &quot;92078&quot;… ## $ Sex.place &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 1, 1, 8, 2, 9, 10, 11, 2, 12, 13, 3, 14, 15, 16, 4, 17, 18, 19, 20, 5, 6, 7, 8, 21, 2… ## $ Div.tot &lt;dbl&gt; 3239, 3239, 3239, 3239, 3239, 3239, 3239, 13, 3239, 3239, 13, 3239, 3239, 3239, 3239, 3239, 3239, 3239, 32… ## $ Sex.tot &lt;dbl&gt; 1179, 1179, 1179, 1179, 1179, 1179, 1179, 11, 2060, 1179, 11, 1179, 1179, 1179, 2060, 1179, 1179, 2060, 11… ## $ AG.tot &lt;dbl&gt; 5, 5, 5, 5, 5, 118, 85, 1, 5, 139, 1, 139, 139, 118, 5, 139, 3, 5, 3, 3, 167, 5, 178, 167, 191, 191, 5, 81… ## $ `5.25.mile.rank` &lt;chr&gt; &quot;3&quot;, &quot;2&quot;, &quot;4&quot;, &quot;6&quot;, &quot;5&quot;, &quot;7&quot;, &quot;8&quot;, &quot;1&quot;, &quot;9&quot;, &quot;10&quot;, &quot;INC&quot;, &quot;11&quot;, &quot;13&quot;, &quot;15&quot;, &quot;12&quot;, &quot;16&quot;, &quot;19&quot;, &quot;14&quot;, &quot;18&quot;, … ## $ time.mile.5.25 &lt;dbl&gt; 1512, 1512, 1531, 1607, 1606, 1661, 1674, 1750, 1721, 1722, NA, 1752, 1754, 1760, 1753, 1790, 1816, 1758, … ## $ `5.25.mile.pace` &lt;chr&gt; &quot;4:48/M&quot;, &quot;4:48/M&quot;, &quot;4:52/M&quot;, &quot;5:06/M&quot;, &quot;5:06/M&quot;, &quot;5:16/M&quot;, &quot;5:19/M&quot;, &quot;5:33/M&quot;, &quot;5:28/M&quot;, &quot;5:28/M&quot;, NA, &quot;5… ## $ `10.mile.rank` &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;1&quot;, &quot;8&quot;, &quot;9&quot;, &quot;13&quot;, &quot;10&quot;, &quot;12&quot;, &quot;15&quot;, &quot;16&quot;, &quot;13&quot;, &quot;11&quot;, &quot;25&quot;, &quot;18&quot;, &quot;1… ## $ time.mile.10 &lt;dbl&gt; 1367, 1372, 1438, 1454, 1476, 1544, 1547, 1490, 1567, 1595, 3320, 1610, 1633, 1652, 1667, 1644, 1628, 1726… ## $ `10.mile.pace` &lt;chr&gt; &quot;2:17/M&quot;, &quot;2:17/M&quot;, &quot;2:24/M&quot;, &quot;2:25/M&quot;, &quot;2:28/M&quot;, &quot;2:34/M&quot;, &quot;2:35/M&quot;, &quot;2:29/M&quot;, &quot;2:37/M&quot;, &quot;2:40/M&quot;, &quot;5:32/… ## $ is.male &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, F… ## $ is.female &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, … ## $ sex.2 &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, … ## $ wheelchair &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ corral &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1… Let’s get rid of the redundant columns, to make this more manageable. Here are the columns I don’t think we need: drop &lt;- c(&#39;time.gun&#39;, &#39;bib&#39;, &#39;Class.Position&#39;, &#39;Overall.Place&#39;, &#39;Sex.place&#39;, &#39;Div.tot&#39;, &#39;Sex.tot&#39;, &#39;AG.tot&#39;, &#39;5.25.mile.rank&#39;, &#39;5.25.mile.pace&#39;, &#39;10.mile.rank&#39;, &#39;10.mile.pace&#39;, &#39;is.male&#39;, &#39;is.female&#39;, &#39;sex.2&#39;) cal1020 &lt;- cal1020 %&gt;% select(-one_of(drop)) Let’s also convert all the names to lower case. names(cal1020) &lt;- tolower(names(cal1020)) glimpse(cal1020) ## Rows: 3,252 ## Columns: 13 ## $ time.chip &lt;dbl&gt; 2880, 2885, 2970, 3062, 3083, 3206, 3222, 3241, 3289, 3318, 3320, 3363, 3388, 3413, 3421, 3435, 3445, 3485, … ## $ name.first &lt;chr&gt; &quot;Jordan&quot;, &quot;Macdonard&quot;, &quot;Sergio&quot;, &quot;Jamesom&quot;, &quot;Darren&quot;, &quot;Okwaro&quot;, &quot;Steven&quot;, &quot;Edwin&quot;, &quot;Lindsey&quot;, &quot;Derek&quot;, &quot;Ralp… ## $ name.last &lt;chr&gt; &quot;Chipangama&quot;, &quot;Ondara&quot;, &quot;Reyes&quot;, &quot;Mora&quot;, &quot;Brown&quot;, &quot;Raura&quot;, &quot;Underwood&quot;, &quot;Figueroa&quot;, &quot;Scherf&quot;, &quot;Bradley&quot;, &quot;Pi… ## $ city &lt;chr&gt; &quot;Flagstaff&quot;, &quot;Grand Prairie&quot;, &quot;Palmdale&quot;, &quot;Arroyo Grande&quot;, &quot;Solana Beach&quot;, &quot;Oceanside&quot;, &quot;Encinitas&quot;, &quot;Commer… ## $ state &lt;chr&gt; &quot;AZ&quot;, &quot;TX&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;AZ&quot;, &quot;?&quot;, &quot;… ## $ division &lt;chr&gt; &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile M 35… ## $ age &lt;dbl&gt; 25, 29, 32, 30, 28, 39, 26, 42, 27, 33, 60, 34, 33, 39, 26, 32, 41, 24, 42, 48, 51, 33, 46, 50, 44, 44, 26, … ## $ zip &lt;chr&gt; &quot;86004&quot;, &quot;75054&quot;, &quot;93551&quot;, &quot;93420&quot;, &quot;92075&quot;, &quot;92057&quot;, &quot;92024&quot;, &quot;90040&quot;, &quot;12440&quot;, &quot;92024&quot;, &quot;91016&quot;, &quot;92078&quot;, … ## $ time.mile.5.25 &lt;dbl&gt; 1512, 1512, 1531, 1607, 1606, 1661, 1674, 1750, 1721, 1722, NA, 1752, 1754, 1760, 1753, 1790, 1816, 1758, 18… ## $ time.mile.10 &lt;dbl&gt; 1367, 1372, 1438, 1454, 1476, 1544, 1547, 1490, 1567, 1595, 3320, 1610, 1633, 1652, 1667, 1644, 1628, 1726, … ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;m… ## $ wheelchair &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ corral &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, … Getting new columns from simple transformations Let’s specify that time is in seconds in the column name, and calculate the pace and speed for the total race, and each half. min/mile from time in seconds We can get total pace over the 10 miles via division: cal1020 &lt;- cal1020 %&gt;% mutate(time.sec = time.chip, speed.mph = 10/(time.sec/60/60), pace.min = (time.sec/60)/10, speed.mph.first = 5.25/(time.mile.5.25/60/60), speed.mph.second= (10-5.25)/(time.mile.10/60/60), pace.min.first = (time.mile.5.25/60)/5.25, pace.min.second = (time.mile.10/60)/(10-5.25)) %&gt;% select(-time.chip) glimpse(cal1020) ## Rows: 3,252 ## Columns: 19 ## $ name.first &lt;chr&gt; &quot;Jordan&quot;, &quot;Macdonard&quot;, &quot;Sergio&quot;, &quot;Jamesom&quot;, &quot;Darren&quot;, &quot;Okwaro&quot;, &quot;Steven&quot;, &quot;Edwin&quot;, &quot;Lindsey&quot;, &quot;Derek&quot;, &quot;Ra… ## $ name.last &lt;chr&gt; &quot;Chipangama&quot;, &quot;Ondara&quot;, &quot;Reyes&quot;, &quot;Mora&quot;, &quot;Brown&quot;, &quot;Raura&quot;, &quot;Underwood&quot;, &quot;Figueroa&quot;, &quot;Scherf&quot;, &quot;Bradley&quot;, &quot;… ## $ city &lt;chr&gt; &quot;Flagstaff&quot;, &quot;Grand Prairie&quot;, &quot;Palmdale&quot;, &quot;Arroyo Grande&quot;, &quot;Solana Beach&quot;, &quot;Oceanside&quot;, &quot;Encinitas&quot;, &quot;Comm… ## $ state &lt;chr&gt; &quot;AZ&quot;, &quot;TX&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;AZ&quot;, &quot;?&quot;,… ## $ division &lt;chr&gt; &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile M … ## $ age &lt;dbl&gt; 25, 29, 32, 30, 28, 39, 26, 42, 27, 33, 60, 34, 33, 39, 26, 32, 41, 24, 42, 48, 51, 33, 46, 50, 44, 44, 26… ## $ zip &lt;chr&gt; &quot;86004&quot;, &quot;75054&quot;, &quot;93551&quot;, &quot;93420&quot;, &quot;92075&quot;, &quot;92057&quot;, &quot;92024&quot;, &quot;90040&quot;, &quot;12440&quot;, &quot;92024&quot;, &quot;91016&quot;, &quot;92078&quot;… ## $ time.mile.5.25 &lt;dbl&gt; 1512, 1512, 1531, 1607, 1606, 1661, 1674, 1750, 1721, 1722, NA, 1752, 1754, 1760, 1753, 1790, 1816, 1758, … ## $ time.mile.10 &lt;dbl&gt; 1367, 1372, 1438, 1454, 1476, 1544, 1547, 1490, 1567, 1595, 3320, 1610, 1633, 1652, 1667, 1644, 1628, 1726… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, … ## $ wheelchair &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ corral &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ time.sec &lt;dbl&gt; 2880, 2885, 2970, 3062, 3083, 3206, 3222, 3241, 3289, 3318, 3320, 3363, 3388, 3413, 3421, 3435, 3445, 3485… ## $ speed.mph &lt;dbl&gt; 12.500000, 12.478336, 12.121212, 11.757022, 11.676938, 11.228946, 11.173184, 11.107683, 10.945576, 10.8499… ## $ pace.min &lt;dbl&gt; 4.800000, 4.808333, 4.950000, 5.103333, 5.138333, 5.343333, 5.370000, 5.401667, 5.481667, 5.530000, 5.5333… ## $ speed.mph.first &lt;dbl&gt; 12.500000, 12.500000, 12.344873, 11.761045, 11.768369, 11.378688, 11.290323, 10.800000, 10.981987, 10.9756… ## $ speed.mph.second &lt;dbl&gt; 12.509144, 12.463557, 11.891516, 11.760660, 11.585366, 11.075130, 11.053652, 11.476510, 10.912572, 10.7210… ## $ pace.min.first &lt;dbl&gt; 4.800000, 4.800000, 4.860317, 5.101587, 5.098413, 5.273016, 5.314286, 5.555556, 5.463492, 5.466667, NA, 5.… ## $ pace.min.second &lt;dbl&gt; 4.796491, 4.814035, 5.045614, 5.101754, 5.178947, 5.417544, 5.428070, 5.228070, 5.498246, 5.596491, 11.649… Fixing states. There are a few puzzling things remaining; the states look weird. sort(unique(cal1020$state)) ## [1] &quot;?&quot; &quot;AB&quot; &quot;AL&quot; &quot;AP&quot; &quot;ARIZONA&quot; &quot;AZ&quot; &quot;B.C.N&quot; &quot;BC&quot; &quot;CA&quot; ## [10] &quot;CALIFORNIA&quot; &quot;CO&quot; &quot;CT&quot; &quot;DC&quot; &quot;EUR&quot; &quot;FL&quot; &quot;FLORIDA&quot; &quot;GA&quot; &quot;HI&quot; ## [19] &quot;ID&quot; &quot;IL&quot; &quot;KY&quot; &quot;MA&quot; &quot;MD&quot; &quot;ME&quot; &quot;MEXICO&quot; &quot;MI&quot; &quot;MN&quot; ## [28] &quot;MO&quot; &quot;MT&quot; &quot;ND&quot; &quot;NE&quot; &quot;NEBRASKA&quot; &quot;NEVADA&quot; &quot;NEW JERSEY&quot; &quot;NJ&quot; &quot;NL&quot; ## [37] &quot;NV&quot; &quot;NY&quot; &quot;OH&quot; &quot;OK&quot; &quot;ON&quot; &quot;OR&quot; &quot;SD&quot; &quot;TX&quot; &quot;UT&quot; ## [46] &quot;UTAH&quot; &quot;VA&quot; &quot;VIRGINIA&quot; &quot;WA&quot; &quot;WASHINGTON&quot; &quot;WI&quot; &quot;WV&quot; We can clearly have some redundant coding, and a few mysterious things like “AP” and “B.C.N.” Let’s make a dictionary to remap these values. BCN is likely Baja – let’s call it MEXICO. BC is likely british columbia – call it CANADA. AB is probably alberta, ON is ontario, NL is newfoundland/labrador (zip code makes sense), “AP” is us military in the pacific… let’s call that NA. state.rename = c(&quot;ARIZONA&quot; = &quot;AZ&quot;, &quot;CALIFORNIA&quot; = &quot;CA&quot;, &quot;FLORIDA&quot; = &quot;FL&quot;, &quot;NEBRASKA&quot; = &quot;NE&quot;, &quot;NEVADA&quot; = &quot;NV&quot;, &quot;NEW JERSEY&quot; = &quot;NJ&quot;, &quot;VIRGINIA&quot; = &quot;VA&quot;, &quot;WASHINGTON&quot; = &quot;WA&quot;, # could be DC, but zipcode indicates WA state. &quot;B.C.N&quot; = &quot;MEXICO&quot;, &quot;BC&quot; = &quot;CANADA&quot;, &quot;AB&quot; = &quot;CANADA&quot;, &quot;NL&quot; = &quot;CANADA&quot;, &quot;ON&quot; = &quot;CANADA&quot;, &quot;AP&quot; = NA, &quot;UTAH&quot; = &quot;UT&quot;, &quot;EUR&quot; = &quot;EUROPE&quot;, &quot;?&quot; = NA) cal1020 &lt;- cal1020 %&gt;% mutate(state = ifelse(state %in% names(state.rename), state.rename[state], state)) sort(unique(cal1020$state)) ## [1] &quot;AL&quot; &quot;AZ&quot; &quot;CA&quot; &quot;CANADA&quot; &quot;CO&quot; &quot;CT&quot; &quot;DC&quot; &quot;EUROPE&quot; &quot;FL&quot; &quot;GA&quot; &quot;HI&quot; &quot;ID&quot; &quot;IL&quot; &quot;KY&quot; ## [15] &quot;MA&quot; &quot;MD&quot; &quot;ME&quot; &quot;MEXICO&quot; &quot;MI&quot; &quot;MN&quot; &quot;MO&quot; &quot;MT&quot; &quot;ND&quot; &quot;NE&quot; &quot;NJ&quot; &quot;NV&quot; &quot;NY&quot; &quot;OH&quot; ## [29] &quot;OK&quot; &quot;OR&quot; &quot;SD&quot; &quot;TX&quot; &quot;UT&quot; &quot;VA&quot; &quot;WA&quot; &quot;WI&quot; &quot;WV&quot; Well, we fixed most of them. Perhaps a more thorough check of zipcode-state consistency would make sense if we really cared about state of origin. Save the cleaned data. So here’s what we have at the end: glimpse(cal1020) ## Rows: 3,252 ## Columns: 19 ## $ name.first &lt;chr&gt; &quot;Jordan&quot;, &quot;Macdonard&quot;, &quot;Sergio&quot;, &quot;Jamesom&quot;, &quot;Darren&quot;, &quot;Okwaro&quot;, &quot;Steven&quot;, &quot;Edwin&quot;, &quot;Lindsey&quot;, &quot;Derek&quot;, &quot;Ra… ## $ name.last &lt;chr&gt; &quot;Chipangama&quot;, &quot;Ondara&quot;, &quot;Reyes&quot;, &quot;Mora&quot;, &quot;Brown&quot;, &quot;Raura&quot;, &quot;Underwood&quot;, &quot;Figueroa&quot;, &quot;Scherf&quot;, &quot;Bradley&quot;, &quot;… ## $ city &lt;chr&gt; &quot;Flagstaff&quot;, &quot;Grand Prairie&quot;, &quot;Palmdale&quot;, &quot;Arroyo Grande&quot;, &quot;Solana Beach&quot;, &quot;Oceanside&quot;, &quot;Encinitas&quot;, &quot;Comm… ## $ state &lt;chr&gt; &quot;AZ&quot;, &quot;TX&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;AZ&quot;, NA, … ## $ division &lt;chr&gt; &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile Overall&quot;, &quot;10 Mile M … ## $ age &lt;dbl&gt; 25, 29, 32, 30, 28, 39, 26, 42, 27, 33, 60, 34, 33, 39, 26, 32, 41, 24, 42, 48, 51, 33, 46, 50, 44, 44, 26… ## $ zip &lt;chr&gt; &quot;86004&quot;, &quot;75054&quot;, &quot;93551&quot;, &quot;93420&quot;, &quot;92075&quot;, &quot;92057&quot;, &quot;92024&quot;, &quot;90040&quot;, &quot;12440&quot;, &quot;92024&quot;, &quot;91016&quot;, &quot;92078&quot;… ## $ time.mile.5.25 &lt;dbl&gt; 1512, 1512, 1531, 1607, 1606, 1661, 1674, 1750, 1721, 1722, NA, 1752, 1754, 1760, 1753, 1790, 1816, 1758, … ## $ time.mile.10 &lt;dbl&gt; 1367, 1372, 1438, 1454, 1476, 1544, 1547, 1490, 1567, 1595, 3320, 1610, 1633, 1652, 1667, 1644, 1628, 1726… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, … ## $ wheelchair &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FAL… ## $ corral &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ time.sec &lt;dbl&gt; 2880, 2885, 2970, 3062, 3083, 3206, 3222, 3241, 3289, 3318, 3320, 3363, 3388, 3413, 3421, 3435, 3445, 3485… ## $ speed.mph &lt;dbl&gt; 12.500000, 12.478336, 12.121212, 11.757022, 11.676938, 11.228946, 11.173184, 11.107683, 10.945576, 10.8499… ## $ pace.min &lt;dbl&gt; 4.800000, 4.808333, 4.950000, 5.103333, 5.138333, 5.343333, 5.370000, 5.401667, 5.481667, 5.530000, 5.5333… ## $ speed.mph.first &lt;dbl&gt; 12.500000, 12.500000, 12.344873, 11.761045, 11.768369, 11.378688, 11.290323, 10.800000, 10.981987, 10.9756… ## $ speed.mph.second &lt;dbl&gt; 12.509144, 12.463557, 11.891516, 11.760660, 11.585366, 11.075130, 11.053652, 11.476510, 10.912572, 10.7210… ## $ pace.min.first &lt;dbl&gt; 4.800000, 4.800000, 4.860317, 5.101587, 5.098413, 5.273016, 5.314286, 5.555556, 5.463492, 5.466667, NA, 5.… ## $ pace.min.second &lt;dbl&gt; 4.796491, 4.814035, 5.045614, 5.101754, 5.178947, 5.417544, 5.428070, 5.228070, 5.498246, 5.596491, 11.649… Note: we save this both as a csv file (to be easy to read by all other software), and as an ‘RData’ file, which is only useful in R, but preserves all the “data types” we defined. write.csv(cal1020, file=&quot;cal1020.cleaned.csv&quot;) save(list = &quot;cal1020&quot;, file=&quot;cal1020.cleaned.Rdata&quot;) "],["descriptive.html", "Descriptive statistics", " Descriptive statistics A statistic is a measurement of some attribute of a sample, computed by applying some function or algorithm to the sample. “Statistic” can refer both to the function and to the value it yields for a particular sample. function(x){sum(sin(x))} is a statistic of the sample x, albeit not a particularly useful one. function(x){sum(x)/length(x)} is a more useful statistic (the arithmetic sample mean). Descriptive statistics are used to quantitatively summarize some features of a sample. Estimators are statistics that are used to estimate a population parameter, that is a parameter of some statistical model of the data. Test statistics are compared to their sampling distribution under the null hypothesis in significance testing. There is nothing qualitatively different between the statistics used for description, estimation, and testing. They are all functions of the sample that yield some value, and the only difference lies in how that value is used. For instance, the sample mean can be used to describe the sample, to estimate the population mean, or to test a hypothesis about the population mean. Descriptive statistics are implicit estimators. Although descriptive statistics are typically presented as non-inferential, as not relying on any statistical model of the data, in practice this is rarely the case: we choose which features of the data are worth describing based on (sometimes unstated) assumptions about the data-generating process. In other words, descriptive statistics are usually (at least implicitly) used as estimators. For instance, let’s say I show you a sequence of coin flips: HHTHTTHTTTHHTT, and ask you to summarize its most relevant feature with one descriptive statistic. Which statistic do you use? If I tell you the coin might be slightly bent, and thus more likely to come up heads than tails, you would want to describe the proportion of heads in the sequence (\\(6/14\\)). If I tell you that the sequence was generated by a machine that aims to flip the coin an even number of times so as to repeat the previous flip outcome, you would want to summarize the proportion of repetitions (\\(8/13\\)). If I tell you that the sequence was generated by a machine that is designed to flip a coin indefinitely, but sometimes it goes awry and drops the coin, thus cutting the sequence short, you would want to summarize the length of the sequence (\\(14\\)). In short, our choice of which aspects of the data are worth describing depend on how we think the data were generated, and thus which features of the data might be most useful to estimate properties of the data generating process. Although we are providing descriptive statistics, we really want to use them as estimators. Describing central tendency. The usual thing we want to do with some sample is calculate the “average,” by which we mean some measure of the “central tendency.” Central tendency Is a value that describes the “middle” of our data, in some sense. Examples of such values include various types of mean, the median, and the mode. There are many types of “average,” which are more or less useful depending on our assumptions about where the data came from, but the conventional “average” is the arithmetic mean, which we will just call the “mean.” Arithmetic mean (or just “mean”) The arithmetic mean is usually denoted with a bar (as in \\(\\bar x\\), “x-bar” is the sample mean of x): \\(\\bar x = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\\) Let’s take the yearly number of miles travelled on airlines between 1937 and 1960 as an example: library(ggplot2) # using ggplot package for graphics x = as.vector(airmiles) # casting time-series as vector -- ignore this ggplot(data=data.frame(x=x), aes(x=x)) + geom_histogram(binwidth=1000, fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. geom_rug(color=&quot;blue&quot;) + # add rug of tick marks for unique observations ggtitle(&quot;Histogram of yearly miles travelled by air (1937-1960&quot;) + theme_bw() # nicer theme We can calculate the sample mean in a number of ways: sum(x)/length(x) ## [1] 10527.83 or mean(x) ## [1] 10527.83 Sensitivity to outliers You can think of the mean as the “balance” point of the data: if we think of each data point as having equal mass and each being arranged on a see-saw accordint to their values, then the further the value is away from the fulcrum, the more torque it will provide, and thus will push the see-saw down harder. The point where we would need to put the fulcrum to balance the see-saw is the sample mean. This means that outliers (measurements that are far away from the rest of the data), will exert a big influence on what the mean is. Median The median is the value such that 50% of the data are bigger than that value, and 50% are smaller. (There is no conventional notation for the sample median.) Thus, the median literally picks out the point that’s in the middle of the sorted list of numbers, and does not care how much larger or smaller the values on either side are, so long as there are 50% of them on each side (very much unlike the mean). median(x) ## [1] 6431 The median is also known as the 50th percentile, or the 0.5th quantile, so we can also estimate it with the empirical quantile function: quantile(x, 0.5) ## 50% ## 6431 Mode The mode is the most common value in the data. A “mode” function does not exist in R, because it is a rather odd statistic that is very unstable for most numerical data. Nonetheless, we can write a function to give us the most frequent values in a sample: Mode = function(x){ ux = unique(x) # find all unique values fx = tabulate(match(x,ux)) # count frequency of each unique value return(ux[fx == max(fx)]) # return unique value(s) that have the highest frequency } (Note that this function will return all the values that have the greatest frequency, so there may be many modes). The mode is the only available measure of central tendency for categorical data, and often makes quite a bit of sense if we have discrete numerical data that have a small number of unique values compared to the number of total observations. For instance, the number of cylinders in car models surveyed in the mtcars data set, is conducive to a description of its mode: ggplot(data=mtcars, aes(x=cyl)) + geom_histogram(binwidth=1, fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. geom_rug(color=&quot;blue&quot;) + # add rug of tick marks for unique observations ggtitle(&quot;Histogram of number of cylinders in mtcars$cyl&quot;) + theme_bw() Mode(mtcars$cyl) ## [1] 8 For continuous, numerical data, the mode is very tricky to formalize because most of the values are unique. Consider our yearly air miles (still represented in x). There are length(x)=24 data points, of which length(unique(x))=24 are unique; consequently, repeated values of number of airmiles travelled per year, specified to the nearest mile, are very unlikely. Nonetheless, if we look at the histogram, there does appear to be a mode – a peak that is higher than the others. However, this peak arises from binning the values. ggplot(data=data.frame(x=x), aes(x=x)) + geom_histogram(binwidth=1000, fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. geom_rug(color=&quot;blue&quot;) + # add rug of tick marks for unique observations ggtitle(&quot;Histogram of yearly miles travelled by air (1937-1960&quot;) + theme_bw() # nicer theme If we try to calculate the mode for airmiles, it returns all the values, because they are all “most frequent,” occuring only once in the sample. Mode(x) ## [1] 412 480 683 1052 1385 1418 1634 2178 3362 5948 6109 5981 6753 8003 10566 12528 14760 16769 19819 22362 25340 ## [22] 25343 29269 30514 We can try to coax the Mode to give us something more meaningful, by rounding, say to the nearest 10: x.10 = round(x/10)*10 Mode(x.10) ## [1] 25340 The value this gives us is a little weird, because it so happened to be the only value that repeated in the set of airmiles rounded to 10: there are length(unique(x.10))=23 unique values out of 24 observations, meaning that the “mode” we got just happened to be the one number that repeated twice. Perhaps more rounding will help: x.100 = round(x/100)*100 Mode(x.100) ## [1] 1400 25300 Now we have two modes again (both of them repeat twice). More rounding? x.1000 = round(x/1000)*1000 Mode(x.1000) ## [1] 1000 This is more reasonable, with the “mode” occurring 4 times in the sample. But Maybe we should keep rounding? x.10000 = round(x/10000)*10000 Mode(x.10000) ## [1] 0 So these data either have 24 modes (no rounding), a mode at 25,340 (rounding to nearest 10), two modes 1400 and 25300 (rounding to nearest 100), a mode at 1000 (rounding to nearest 1000), or a mode at 0 (rounding to nearest 10000). If we consider that a mode at 0 when rounding to 10000 covers the range [-5000,5000], and rounding to 1000 covers [500,1500], we have a somewhat coherent story about the most frequent interval, but a most frequent value does not exist. Describing dispersion Once we have established some measure of central tendency, we usually want to know something about the dispersion. Dispersion How spread out are the data around its central tendency. The variability of the data. Common examples include standard deviation/variance, range, interquartile range, etc. Standard deviation and variance When we summarize the central tendency with the mean, the natural measure of dispersion is the standard deviation or the variance. The sample variance is the average squared deviation from the mean. \\(s^2 = \\frac{1}{n-1}\\sum\\limits_{i=1}^n (x_i-\\bar x)^2\\) in R: var(x) (For somewhat technical reasons we calculate the sample variance by dividing by \\(n-1\\) rather than \\(n\\)) We can calculate this manually, or using the var function: # x still refers to the airmiles data. sum((x-mean(x))^2)/(length(x)-1) # calculating sample variance manually ## [1] 100667654 var(x) ## [1] 100667654 The variance is intimately related to two other quantities: The sum of squares Is the sum of squared deviations from (in this context) the mean, without taking the average by dividing by \\(n-1\\). We usually denote the sum of squares as the function \\(\\operatorname{SS}(x)\\), or simply as SSX. \\(\\operatorname{SS}[x] = \\sum\\limits_{i=1}^n (x_i-\\bar x)^2\\) We can calculate the sum of squares manually with sum((x-mean(x))^2) ## [1] 2315356053 The sample standard deviation Is the square root of the variance, thus making it interpretable in units of \\(x\\). \\(s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1}\\sum\\limits_{i=1}^n (x_i-\\bar x)^2}\\) in R: sd(x) Which we can calcuate either manually or using sd: sqrt(sum((x-mean(x))^2)/(length(x)-1)) # calculating sample standard deviation manually ## [1] 10033.33 sd(x) ## [1] 10033.33 Just to be clear here about the relationships of the sum of squares of x, the sample variance of x, and the standard deviation of x. \\(\\operatorname{SS}[x] = \\sum\\limits_{i=1}^n (x_i-\\bar x)^2\\) \\(s^2 = \\frac{1}{n-1} \\operatorname{SS}[x]\\) \\(s = \\sqrt{s^2}\\) Or in R: ss.x = sum((x-mean(x))^2) var.x = ss.x / (length(x) - 1) sd.x = sqrt(var.x) Although the standard deviation and variance are perhaps the most common descriptions of the dispersion of numerical data, the sum of squares is not a useful summary of dispersion, because it will mostly reflect the sample size: it will grow as \\(n\\) increases because it is a sum over all data points without dividing by \\(n\\) in any way. Standard deviation and variance care very much about extreme values, because they are based on the sum of squared deviations, thus a value very far from the mean will exert a great influence, as its squared deviation will be very large. In this sense, these measures are not robust to extreme values or outliers. Median (or mean) absolute deviation (MAD) If we calculate central tendency as the median, likely because we want to take advantage of its robustness to extreme values, we probably also want to calculate dispersion around the median in a manner that is somewhat robust to extreme values. Median absolute deviation (from the median) is a measure of dispersion that is as described: the median absolute deviation from the median \\(\\operatorname{MAD}[x] = \\operatorname{Median}\\left[{\\lvert{x_i-m}\\rvert}\\right]\\), (where \\(\\lvert x \\rvert\\) denote the absolute value, and \\(m\\) denotes the median of \\(x\\), and \\(\\operatorname{Median}[w]\\) yields the median of \\(w\\).) or in R: mad(x) We can calculate this in R manually or using mad(): median(abs(x-median(x))) ## [1] 5563.5 mad(x) ## [1] 8248.445 Oops – these give us different answers. That’s because the mad() function by default adjusts the raw median absolute deviation by a constant (~1.4826) to try to make it an estimator for the standard deviation of normally distributed data. median(abs(x-median(x)))*1.4826 ## [1] 8248.445 The median absolute deviation is more robust to outliers and skewed distributions than the mean because it (a) considers the absolute value of deviations, rather than the squared deviations, and (b) calculates the median absolute deviation, rather than the average, thus effectively not caring how large the largest deviations are. There is ample opportunity for confusion because “MAD” sometimes refers to the “mean absolute deviation,” and in general an “average absolute deviation” may refer to deviations from any measure of central tendency (mean, median, mode, or something else), and the average may be the mean or the median, or even something else. We will try to stick to using MAD to refer to the median absolute deviation from the median. Quantile measures of dispersion (e.g., Range and interquartile range). There are some other measures of dispersion that rely on the distribution of data in the sample without reference to the central tendency: they are concerned with the distance between percentiles (quantiles) in the sample, rather than the distance to some measure of central tendency. The most straight forward of these would be the range, but the interquartile range is usually more useful. Range is the distance between the smallest and largest value in the sample. in R: max(x)-min(x) In R, the range(x) function returns the smallest and largest values in x, rather than the distance between them, but we can calculate the range in a number of ways: range(x) ## [1] 412 30514 diff(range(x)) ## [1] 30102 max(x)-min(x) ## [1] 30102 The range is obviously very sensitive to outliers and extreme values, since it only considers the two most extreme values in its definition. This is a bit undesirable, so we might want to consider the range that disregards the extremes. The interquartile range (IQR) Is the distance between the 25th and 75th percentile In R: IQR(x) The interquartile range considers the 1st and 3rd “quartile,” that is the first and third boundary between data points if we divide all the values into four equal-n bins. These would more commonly be called the 25th and 75th percentile, or the 0.25th and 0.75th quantile. quantile(x, c(0.25, 0.75)) # the 25th and 75th percentile. (1st and 3rd quartile) ## 25% 75% ## 1580.0 17531.5 diff(quantile(x, c(0.25, 0.75))) # the distance between them: the interquartile range ## 75% ## 15951.5 IQR(x) # R&#39;s built-in interquartile range function. ## [1] 15951.5 Because IQR only considers the values at the 25th and 75th percentile, it does not care how extreme the extreme values are, and is thus quite robust to outliers. Entropy There are no conventional measures of dispersion for categorical data: no measures of average deviation from the central tendency, or even rank-based measures of range, make sense. However, sometimes it might be desirable to quantify the ‘dispersion’ of a categorical distribution as a measure of how “not peaky” it is. Let’s consider the HairEyeColor data set, which we will manipulate a bit to extract a vector of category labels corresponding to hair color: haireye.df = as.data.frame(HairEyeColor) # convert table to data frame. haireye.df = haireye.df[rep(row.names(haireye.df), haireye.df$Freq), c(&quot;Sex&quot;, &quot;Hair&quot;, &quot;Eye&quot;)] # replicate to have unique observations, rather than counts. ggplot(data=haireye.df, aes(x=Hair)) + geom_bar(fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. ggtitle(&quot;Histogram of hair color&quot;) + theme_bw() The mode of hair color makes quite a bit of sense: Mode(hair)=Brown, but how do we measure how “peaky” this distribution is? I would advocate using a measure of “entropy” over the sample frequencies: Entropy A measure of how “unpredictable” a categorical random variable is – it indicates how much information (in bits) you get from making an observation. \\(H(X) = - \\sum\\limits_x P(x) \\log_2\\left[P(x)\\right]\\), where \\(\\log_2\\) denotes the base 2 logarithm. We can write a function in R to calculate the entropy of a sample: Entropy = function(x){ ux = unique(x) # find all unique values fx = tabulate(match(x,ux)) # count frequency of each unique value px = fx/sum(fx) # convert frequencies to proportions (probabilities) return(-sum(px*log2(px))) # return entropy over unique values } Entropy(haireye.df$Hair) ## [1] 1.798227 Entropy is a measure of information, rather than dispersion, so a variable that can take on more unique values, will usually have a higher entropy. For instance, consider participant sex in the HairEyeColor data: ggplot(data=haireye.df, aes(x=Sex)) + geom_bar(fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. ggtitle(&quot;Histogram of sex&quot;) + theme_bw() Entropy(haireye.df$Sex) ## [1] 0.9976193 While the distribution of hair color looks quite a bit “peakier,” it’s entropy (1.7982269) is higher than the entropy of the very not-peaky distribution of sex (0.9976193), because there are four different hair colors, and only two sexes in the data. Perhaps a more intuitive measure of “peakiness” would be entropy normalized to the maximum entropy for that number of alternatives: RelativeEntropy = function(x){ # calculate maximum entropy for a variable with the same number of unique values. uniform.p = rep(1/length(unique(x)), length(unique(x))) maxent = -sum(uniform.p*log2(uniform.p)) # return relative entropy return(Entropy(x)/maxent) } Relative entropy gives us a more intuitive measure of lack-of-peakiness that reflects the number of possible values: RelativeEntropy(haireye.df$Hair)=0.8991135 and RelativeEntropy(haireye.df$Eye)=0.9139308. Although neither the entropy nor relative entropy statistics we defined here are conventional, I find them to be sometimes useful. Moreover, they highlight the fact that a descriptive statistic can be whatever we choose: if we think some particular measure of a sample is a useful description of some feature of the data, we can use it as a statistic. Descriptions of shape. For numerical data we often want to describe not just the central tendency and dispersion, but also its shape. Consider for instance the two samples x and y (that we create below to have mean \\(m\\) and standard deviation \\(s\\)): library(gridExtra) # used to make plots side by side. s = 7 m = 50 x = rnorm(1000) x = (x-mean(x))/sd(x)*s + m y = rexp(1000) y = (y-mean(y))/sd(y)*s + m sprintf(&quot;mean(x): %0.2f mean(y): %0.2f&quot;, mean(x), mean(y)) ## [1] &quot;mean(x): 50.00 mean(y): 50.00&quot; sprintf(&#39; sd(x): %0.2f sd(y): %0.2f&#39;, sd(x), sd(y)) ## [1] &quot; sd(x): 7.00 sd(y): 7.00&quot; g1 = ggplot(data.frame(x=x), aes(x=x))+ geom_histogram(binwidth=0.5, fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. ggtitle(&quot;Histogram of x&quot;) + theme_bw() g2 = ggplot(data.frame(y=y), aes(x=y))+ geom_histogram(binwidth=0.5, fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. ggtitle(&quot;Histogram of y&quot;) + theme_bw() grid.arrange(g1,g2, ncol=2) They have exactly the same sample mean and sample standard deviation, but clearly look different. That difference in how they look is called the “shape.” There are two statistics that are most commonly used to describe the shape of a distribution: its skewness and kurtosis. Skewness The skewness of a distribution measures whether the positive or negative tail is heavier. it is roughly the average of the cubed “z-scores” (see expectations) (although calculating it from a sample requires some correction) in R, we calculate it using skewness() from the moments package. The skewness of the variable x should be around 0, as it is symmetric, the skewness of y should be positive, as it has a heavy “positive” tail. library(moments) skewness(x) ## [1] -0.05165289 skewness(y) ## [1] 2.004362 We can construct a new variable that will have negative skewness: z = -1*rexp(1000) z = (z-mean(z))/sd(z)*s + m ggplot(data.frame(z=z), aes(x=z))+ geom_histogram(binwidth=0.5, fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. ggtitle(&quot;Histogram of z&quot;) + theme_bw() skewness(z) ## [1] -2.090862 Skew and central tendency. Since the mean very much cares about extreme values, the median only cares about the rank of values, and the mode only cares about the most common value, they are differently influenced by the skew of the data. Consider the geometrically distributed variable X: X = rgeom(10000, 0.1) df_stats = data.frame(statistic = c(&quot;Mode&quot;, &quot;Median&quot;, &quot;Mean&quot;), val=c(Mode(X), median(X), mean(X))) ggplot(data.frame(X=X), aes(x=X))+ geom_histogram(binwidth=5, fill=&#39;lightgray&#39;) + # plot histogram. ggtitle(&quot;Histogram of X&quot;) + geom_vline(data=df_stats, aes(xintercept=val, color=statistic), size=2, show_guide=TRUE)+ scale_color_manual(values = c(&quot;#dd0000&quot;,&quot;#00bb00&quot;, &quot;#0000ff&quot;), name = &quot;Central tendency statistic&quot;)+ theme_bw() The mean is most drawn to the heavy positive tail, the median second, and the mode least of all. Kurtosis The (excess) kurtosis of a distribution measures how heavy the tails are (compared to a normal distribution) it is roughly the average of the “z-scores” to the fourth power (for more see expectations) (although calculating it from a sample requires some correction) in R, we calculate it using kurtosis()-3 from the moments package. The excess kurtosis describes whether the tails are much like a normal distribution (kurtosis=0), whether the tails are heavy/long (meaning the distribution is quite peaky for its mean and variance; kurtosis&gt;0), or whether its tails are short (meaning the distribution is quite boxy for its mean and variance; kursosis&lt;0). Let’s consider a few variables with different kurtosis (but all with roughly the same mean, sd and skew): library(PearsonDS) s = 7 m = 50 k = c(-1, 0, 5) X = list() gplots = list() X.kurtosis = c() for(i in 1:length(k)){ varname = paste(&#39;X.&#39;, i, sep=&quot;&quot;) X[[i]] = rpearson(1000, moments=c(0, 1, 0, k[i]+3)) X[[i]] = (X[[i]]-mean(X[[i]]))/sd(X[[i]])*s + m gplots[[i]] = ggplot(data.frame(x=X[[i]]), aes(x=x))+ geom_histogram(fill=&#39;red&#39;, color=&quot;black&quot;) + # plot histogram. ggtitle(paste(&quot;Histogram of &quot;, varname, sep=&quot;&quot;)) + theme_bw() X.kurtosis[varname] = kurtosis(X[[i]])-3 } grid.arrange(gplots[[1]], gplots[[2]], gplots[[3]], ncol=3) These variables (stored in the list X) have kurtosis (stored in the named vector X.kurtosis increasing from left to right: X.kurtosis ## X.1 X.2 X.3 ## -1.030450 -0.162459 2.057358 "],["visualization.html", "Visualizations General rules for scientific data visualization. Picking a plot (what’s convention) Categorical ~ 0 numerical ~ 0 numerical ~ categorical numerical ~ numerical (2 x numerical ~ 0) categorical ~ numerical 2 x categorical and categorical ~ categorical Extra plot notes. ggplot", " Visualizations These notes are largely there to provide quick examples of ggplot2 code that generates graphs for particular data. I assume that the ggplot2 introduction from R4DS got you oriented, but if not, we have a few more notes on the basics of ggplot. General rules for scientific data visualization. The order of these rules indicates their priority (as I see it): rules further down are superceded by higher up rules if they are in conflict. Everything should be labeled, and interpretable without consulting a figure caption or having to solve a puzzle. Graphs should facilitate relevant quantitative interpretation and comparisons. Graphs should represent variability and uncertainty to permit inferential statistics by eye Graphs should follow conventions for the kind of information/data being presented. Graphs should not waste ink and should otherwise look pretty. Picking a plot (what’s convention) When you make a plot, you are trying to show the relationship between one or more response/outcome variables, and some explanatory variables (in experimental settings, these are often called dependent and independent variables, respectively). These variables can be classified as either categorical or numerical. I find it helpful to think of conventional plot options in terms of a pseudo formula: what kind of response variable is being explained by what types of explanatory variables. e.g., categorical ~ numerical would mean you are showing how a categorical response variable changes with some numerical explanatory variable. Most of this document is structured based on such formulas. Categorical ~ 0 How do we show the distribution of some categorical variable (that’s what I mean by “~0”: no explanatory variables)? The three main plot types to consider here are: histogram, pie chart, and stacked area plot. Histograms make comparisons of frequency across categories easiest (you just have to compare how tall two bars are). Pie charts make this somewhat difficult, because we are pretty bad at visually comparing similar angles; however, pie charts do effectively convey the absolute proportion of a category (which is hard to assess in a histogram). Stacked area plots are a sort of compromise of both: comparisons across categories are harder than in a histogram, but easier than a pie chart; absolute proportion is easier than in a histogram, but harder than a pie chart. For our plots of a categorical variable with no expanatory variables, we will use the SPSP demographics data. library(tidyverse) spsp &lt;- read_csv(&#39;http://vulstats.ucsd.edu/data/spsp.demographics.cleaned.csv&#39;) spsp %&gt;% group_by(ethnicity) %&gt;% summarize(n=n()) %&gt;% knitr::kable() ethnicity n Arab 23 Asian 504 Black 133 Latino 128 Native American 33 Other 344 White 2586 Histogram The classic distribution plot is a histogram: one bar per category, usually arranged along the x-axis, with the y-axis showing frequencies. ( plot1 &lt;- ggplot(spsp, aes(x=ethnicity, fill=ethnicity)) + geom_bar()+ ## the lines above form the basis of the plot, ## the lines below just make it look nicer scale_fill_brewer(palette = &quot;Set1&quot;)+ theme_bw()+ theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle=90, hjust=1)) ) Pie chart Pie charts show the proportion of each category as wedges on a circle. (Pie charts are generally frowned upon, so you better have a really good reason to use them.) Also, because they are out of favor, making them in ggplot is a bit weird: you have to make a stacked area plot, then convert the coordinates to polar. ( plot2 &lt;- ggplot(spsp, aes(x=factor(1),fill=ethnicity))+ geom_bar(aes(y = (..count..)/sum(..count..)), width=1)+ coord_polar(theta=&quot;y&quot;)+ ## the lines above form the basis of the plot, ## the lines below just make it look nicer scale_fill_brewer(palette = &quot;Set1&quot;)+ theme_bw()+ theme(legend.position = &quot;none&quot;, axis.title = element_blank(), axis.text = element_blank(), panel.grid = element_blank()) ) Stacked area Think of a stacked area plot as a histogram, with all the bars stacked on top of each other (hopefully, with different colors!). This plot connects to methods of plotting a categorical variable as a function of some other variables. ( plot3 &lt;- ggplot(spsp, aes(x=factor(1),fill=ethnicity))+ geom_bar(width=0.75, stat=&#39;count&#39;)+ ## the lines above form the basis of the plot, ## the lines below just make it look nicer scale_fill_brewer(palette = &quot;Set1&quot;)+ theme_bw()+ guides(fill=guide_legend(title=&quot;SPSP\\nethnicity&quot;))+ theme(axis.text.x = element_blank(), axis.title.x = element_blank()) ) numerical ~ 0 To look at the distribution of a numeric variable, we will generally need to do something to smooth or bin over the number line*. Histograms have explicit bins, and show the frequency of numbers in each bin. Density plots smooth over the numberline not in explicit bins, but with a continuous kernel. Histograms do a better job of coneying how noisy your data are, but are quite sensitive to bin placement: they will give you a different impression depending on bin width and bin shifts (e.g., bin 1-2, 2-3, … vs 0.5-1.5, 1.5-2.5, ….). Because density plots do not have discrete bins, they are smoother, so there is no notion of shifting a bin, and they tend to be less sensitive to bandwidth; however, they tend to make your data look clean even when they are not. Let’s use some data from a 10 mile race to plot the distribution of a numeric variable. load(url(&quot;http://vulstats.ucsd.edu/data/cal1020.cleaned.Rdata&quot;)) glimpse(cal1020) ## Rows: 3,234 ## Columns: 11 ## $ name.first &lt;fct&gt; Jordan, Macdonard, Sergio, Jamesom, Darren, Okwaro, Steven, Lindsey, Derek, Daniel, Andrew, David, Natasha, Tim,… ## $ name.last &lt;fct&gt; Chipangama, Ondara, Reyes, Mora, Brown, Raura, Underwood, Scherf, Bradley, Seidel, Corman, Kloz, Labeaud Anzures… ## $ City &lt;fct&gt; &quot;Flagstaff&quot;, &quot;Grand Prairie&quot;, &quot;Palmdale&quot;, &quot;Arroyo Grande&quot;, &quot;Solana Beach&quot;, &quot;Oceanside&quot;, &quot;Encinitas&quot;, &quot;High Falls… ## $ State &lt;chr&gt; &quot;AZ&quot;, &quot;TX&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;AZ&quot;, NA, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, … ## $ Zip &lt;fct&gt; 86004, 75054, 93551, 93420, 92075, 92057, 92024, 12440, 92024, 92078, 92130, 92056, 92123, 92121, 85048, ?, 9202… ## $ age &lt;dbl&gt; 25, 29, 32, 30, 28, 39, 26, 27, 33, 34, 33, 39, 26, 32, 41, 24, 42, 48, 51, 33, 46, 50, 44, 44, 26, 23, 38, 30, … ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;mal… ## $ time.sec &lt;dbl&gt; 2880, 2885, 2970, 3062, 3083, 3206, 3222, 3289, 3318, 3363, 3388, 3413, 3421, 3435, 3445, 3485, 3489, 3513, 3515… ## $ corral &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ pace.min &lt;dbl&gt; 4.800000, 4.808333, 4.950000, 5.103333, 5.138333, 5.343333, 5.370000, 5.481667, 5.530000, 5.605000, 5.646667, 5.… ## $ speed.mph &lt;dbl&gt; 12.500000, 12.478336, 12.121212, 11.757022, 11.676938, 11.228946, 11.173184, 10.945576, 10.849910, 10.704728, 10… Histogram &amp; density plot1 &lt;- ggplot(cal1020, aes(x=time.sec/60))+ geom_histogram(binwidth = 1)+ ggtitle(&#39;histogram&#39;)+ scale_x_continuous(&#39;Minutes&#39;, breaks=seq(0,240, by=30))+ theme_minimal() plot2 &lt;- ggplot(cal1020, aes(x=time.sec/60))+ geom_density(fill=&#39;gray&#39;, alpha=0.5)+ ggtitle(&#39;density&#39;)+ scale_x_continuous(&#39;Minutes&#39;, breaks=seq(0,240, by=30))+ theme_minimal() gridExtra::grid.arrange(plot1, plot2, nrow=1) numerical ~ categorical This is the most common data visualization category I see in psychology: how does some numerical variable change as a function of condition (category). Usually it is portrayed as a bar plot (hopefully with error bars). But we can do better. First, let’s make all the variations, and put them in one figure (below we just make each plot, and then show them all together.) Bar plot with error bars We adopt the common approach of using the mean +/- 1 standard error of the mean as the error bars here. plot1 &lt;- ggplot(cal1020, aes(x=sex, fill=sex, y=speed.mph))+ stat_summary(fun.y = mean, geom=&quot;bar&quot;)+ stat_summary(fun.data = mean_se, geom=&quot;errorbar&quot;, width=0.5)+ scale_y_continuous(&#39;Avg speed (mph; +/- s.e.m.)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;Barplot&#39;)+ theme_minimal()+ coord_cartesian(ylim=c(5,7))+ theme(legend.position = &#39;none&#39;) Jittered data points. Data points are jittered, because otherwise they would fall on top of each other, and would be impossible to discern. plot2 &lt;- ggplot(cal1020, aes(x=sex, color=sex, y=speed.mph))+ geom_jitter(size=0.1, alpha=0.5)+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;Jittered&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) Viola/Violin plot Like a smoothed density histogram, except it’s symmetric, so it looks nicer when many are arrayed side by side. plot3 &lt;- ggplot(cal1020, aes(x=sex, fill=sex, y=speed.mph))+ geom_violin()+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;Violin&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) Box and whiskers plot This is a visualization of a bunch of summary statistics of the distribution. By default, these summary statistics are: the median (middle line), the 25th and 75th percentile (edges of the box), 25th percentile - 1.5(IQR), and 75th percentile + 1.5(IQR) (the whiskers); and it shows the “outliers” (data points that are beyond those IQR intervals. plot4 &lt;- ggplot(cal1020, aes(x=sex, fill=sex, color=sex, y=speed.mph))+ geom_boxplot(alpha=0.5, outlier.alpha = 0.1)+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;Boxplot&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) Overlayed densities (here we flip the coordinates so we can picture them along side the other graphs) plot5 &lt;- ggplot(cal1020, aes(x=speed.mph, fill=sex, color=sex))+ geom_density(alpha=0.5)+ coord_flip()+ scale_x_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;Densities&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) Empirical cumulative distribution (here we flip the coordinates so we can picture them along side the other graphs) plot6 &lt;- ggplot(cal1020, aes(x=speed.mph, fill=sex, color=sex))+ stat_ecdf(geom=&#39;line&#39;, size=1, alpha=0.75)+ coord_flip()+ scale_x_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ scale_y_continuous(&#39;CDF&#39;, breaks=c(0, 0.5, 1.0))+ ggtitle(&#39;ECDF&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) Comparisons gridExtra::grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, nrow=1) The mean + standard error bar plot makes the implicit statistical comparison (t-test) easy to do by eye, but it obscures what the actual data look like (both the underlying variability, as well as it’s messiness). The jittered plot is very faithful to the underlying data, but it’s tricky to figure out how the distributions compare. The violin plot hides some of the data messiness, but does make comparisons easier. The boxplot is useful for showing the five-number summary, but not much else. Overlayed density plots are more intuitive than violin plots, but they only work well for a small number of categories. Empirical CDFs are also only good for a few categories, and their strength is in showing differences in the tails of the distributions. Recommendations My suggestion is to use a point+range to indicate the mean and standard error (thus facilitating comparisons), and to overlay that on some representaiton of the underlying data. I recommend using a jittered display of individual data points if you have relatively little data, and a violin plot if you have a lot of data (such that the jitter is very hard to make informative). Below, to show the few-data-points case, I sub-sample the data. # recommendation for few data points plot1 &lt;- cal1020 %&gt;% sample_n(50) %&gt;% ggplot(aes(x=sex, fill=sex, color=sex, y=speed.mph))+ geom_jitter(width=0.25, size = 0.75, alpha=0.7)+ stat_summary(fun.data = mean_se, geom=&quot;pointrange&quot;, fatten = 2, size=1)+ scale_y_continuous(&#39;Speed (mph +/1 sem)&#39;, breaks = seq(0, 15, by=1))+ theme_minimal()+ theme(legend.position = &#39;none&#39;) ## Here we will write a function to generate a 95% confidence interval mean_ci &lt;- function(x){ m = mean(x) se = sd(x)/sqrt(length(x)) ql = qnorm(1-0.025) c(&#39;y&#39;=m, &#39;ymin&#39;=m-ql*se, &#39;ymax&#39;=m+ql*se) } plot2 &lt;- cal1020 %&gt;% ggplot(aes(x=sex, fill=sex, color=sex, y=speed.mph))+ geom_violin(alpha=0.3)+ stat_summary(fun.data = mean_ci, geom=&quot;pointrange&quot;, fatten = 2, size=1)+ scale_y_continuous(&#39;Speed (mph +/- 2 sem)&#39;, breaks = seq(0, 15, by=1))+ theme_minimal()+ theme(legend.position = &#39;none&#39;) gridExtra::grid.arrange(plot1, plot2, nrow=1) numerical ~ numerical (2 x numerical ~ 0) To show the joint distribution of two numerical variables, or one numerical variable as a function of another, the obvious choice is a scatterplot. However, a scatterplot, just like a jittered display of data points, is difficult to make usable when you have a lot of data (you need to tinker with point size and point transparency so that the display isn’t just an undifferentiated blob). A 2D histogram, mapping frequency onto color, is a better choice if you have a lot of data. Scatter and heatmap plot1 &lt;- cal1020 %&gt;% ggplot(aes(x=age, y=speed.mph, alpha=0.5)) + geom_point(size=0.5, position=position_jitter(0.25))+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;scatterplot&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) plot2 &lt;- cal1020 %&gt;% ggplot(aes(x=age, y=speed.mph)) + geom_bin2d()+ scale_fill_continuous(low=&quot;#DDDDDD&quot;, high=&quot;#000000&quot;)+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;heatmap histogram&#39;)+ theme_minimal() gridExtra::grid.arrange(plot1, plot2, nrow=1) Conditional means The other popular way to show the relationship between a numerical response (y) and a numerical explanatory variable (x) is to show the mean (plus error bars) of y for a given range of x. Such conditional means are typically connected with a line, and sometimes even the error bars are connected in a ribbon. This is the most direct way of showing the “conditional” means, but is generally impractical unless you have a lot of data. The most common way to show conditional means is to show the fitted y~x line; this takes us further away from the data (because it assumes that the conditional means follow a line), and usually gives us a false impression of how linear the relationship is. The other common alternative is to do “LOESS” smoothing (locally weighted regression); for each point we are plotting, the LOESS method fits a function to all the data, but weights the data based on how far away it is from the point we are interested; the consequence is a smooth, wiggly line that generally looks nice, but often misleads folks into thinking that they have evidence for non-linearities in their data. Generally, I would recommend showing the raw data distribution (as in the subsection above), plus a representation of whatever model you fit to those data (if you fit a line, show a line; if you fit a parabola, show the parabola). ## make bins and calculate mean, se cal1020.binned &lt;- cal1020 %&gt;% group_by(age.bin = floor(age/5)*5+2.5) %&gt;% summarize(mean = mean(speed.mph), sem=sd(speed.mph)/sqrt(n())) plot1 &lt;- cal1020.binned %&gt;% ggplot(aes(x=age.bin, y=mean, alpha=0.5)) + geom_point()+ geom_line()+ geom_pointrange(aes(y=mean, ymin=mean-sem, ymax=mean+sem))+ scale_y_continuous(&#39;Mean speed (mph +/- sem)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;y|x&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) plot2 &lt;- cal1020.binned %&gt;% ggplot(aes(x=age.bin, y=mean, alpha=0.5)) + geom_line()+ geom_point()+ geom_ribbon(aes(ymin=mean-sem, ymax=mean+sem))+ scale_y_continuous(&#39;Mean speed (mph +/- sem)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;y|x Ribbon&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) plot3 &lt;- cal1020 %&gt;% ggplot(aes(x=age, y=speed.mph, alpha=0.5)) + geom_smooth(method=&#39;lm&#39;, color=&#39;red&#39;, fill=&#39;red&#39;, alpha=0.5)+ scale_y_continuous(&#39;Fitted speed (mph + 95% CI)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;Line&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) plot4 &lt;- cal1020 %&gt;% ggplot(aes(x=age, y=speed.mph, alpha=0.5)) + geom_smooth(method=&#39;loess&#39;, color=&#39;blue&#39;, fill=&#39;blue&#39;, alpha=0.5)+ scale_y_continuous(&#39;Fitted speed (mph + 95% CI)&#39;, breaks = seq(0, 15, by=1))+ ggtitle(&#39;LOESS&#39;)+ theme_minimal()+ theme(legend.position = &#39;none&#39;) gridExtra::grid.arrange(plot1, plot2, plot3, plot4, nrow=1) numerical ~ numerical + categorical If you add a categorical variable to those numerical~numerical plots, the obvious choice would be to color the points/fitted lines. However, if you have a lot of data, the scatterplots become unusable, so sometimes it is worth splitting the different categories into facets, as shown below. plot1 &lt;- cal1020 %&gt;% ggplot(aes(x=age, y=speed.mph, fill=sex, color=sex, group=sex)) + geom_point(size=0.5, alpha=0.5, position=position_jitter(0.25))+ geom_smooth(method=&#39;lm&#39;,alpha=0.5)+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ theme_minimal()+ theme(legend.position = &#39;none&#39;) plot2 &lt;- cal1020 %&gt;% ggplot(aes(x=age, y=speed.mph, fill=sex, color=sex, group=sex)) + facet_grid(~sex)+ geom_point(size=0.5, alpha=0.5, position=position_jitter(0.25))+ geom_smooth(method=&#39;lm&#39;,alpha=0.5)+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ theme_minimal()+ theme(legend.position = &#39;none&#39;) gridExtra::grid.arrange(plot1, plot2, nrow=1, widths = c(2, 3)) categorical ~ numerical It’s not very common to see a category distribution as a function of a numerical variable in experimental data, because such visualizations look like noise unless without a lot of data. However, the two obvious options are variations of a stacked area plot. I recommend the “filled” variety, unless the distribution of the explanatory variable is really important to show. plot1 &lt;- ggplot(cal1020, aes(x=round(speed.mph*2)/2, fill=sex))+ geom_bar(position=&#39;stack&#39;)+ scale_x_continuous(&#39;Avg speed (mph)&#39;, breaks = seq(0, 15, by=1))+ theme_minimal()+ theme(legend.position = &#39;none&#39;) plot2 &lt;- ggplot(cal1020, aes(x=round(speed.mph*2)/2, fill=sex))+ geom_bar(position=&#39;fill&#39;)+ scale_x_continuous(&#39;Avg speed (mph)&#39;, breaks = seq(0, 15, by=1))+ ylab(&#39;proportion&#39;)+ theme_minimal() gridExtra::grid.arrange(plot1, plot2, nrow=1, widths = c(3,4)) 2 x categorical and categorical ~ categorical Heatmap To show the joint distribution of two categorical variables, the best option is a heatmap. plot1 &lt;- spsp %&gt;% mutate(stage = factor(stage, levels = c(&quot;Undergrad&quot;, &quot;Grad&quot;, &quot;Early Career&quot;, &quot;Regular Member&quot;, &quot;Retired&quot;))) %&gt;% ggplot(aes(x=stage, y=ethnicity))+ geom_bin2d() + scale_fill_continuous(low=&quot;#DDDDDD&quot;, high=&quot;#000000&quot;)+ theme_minimal()+ theme(panel.grid = element_blank(), axis.text.x = element_text(angle=90, hjust = 1, vjust=1)) categorical ~ categorical To show how distribution of one categorical variable changes as a function of another, we should use filled stacked area plots. Note, that since these plots are “filled” along y, we get no sense of the distribution of the x variable, but in such cases we generally care much less about the distribution of x, then about the conditional distribution of y|x. plot1 &lt;- spsp %&gt;% mutate(stage = factor(stage, levels = c(&quot;Undergrad&quot;, &quot;Grad&quot;, &quot;Early Career&quot;, &quot;Regular Member&quot;, &quot;Retired&quot;))) %&gt;% ggplot(aes(x=stage, fill=ethnicity))+ geom_bar(position=&#39;fill&#39;) + scale_fill_brewer(palette = &quot;Set1&quot;)+ theme_minimal()+ theme(panel.grid = element_blank(), axis.text.x = element_text(angle=90, hjust = 1, vjust=1)) Extra plot notes. numerical ~ 2 x categorical When plotting one numerical variable as a function of two categorical variables, the most common method is to use color to differentiate adjacent categories on the x-axis. plot1 &lt;- cal1020 %&gt;% mutate(first.1 = substr(name.first, 1, 1)) %&gt;% ggplot(aes(x=sex, fill=first.1, color=first.1, y=speed.mph))+ geom_violin(alpha=0.3, position=&quot;dodge&quot;)+ # geom_jitter(width=0.25, size = 0.75, alpha=0.7, # position=position_dodge())+ stat_summary(fun.data = mean_se, geom=&quot;pointrange&quot;, fatten = 2, position=position_dodge(width=0.9), size=1)+ scale_y_continuous(&#39;Speed (mph)&#39;, breaks = seq(0, 15, by=1))+ theme_minimal()+ theme(legend.position = &#39;none&#39;) Frequency vs Proportion splots When showing how a categorical variable changes as a function of another variable, we use stacked area plots of one sort or another. We have a choice about whether to ‘stack’ or ‘fill’ the counts. Meaning, do we show the proportion of category y, without showing the distribution of category x (if we fill), or do we do a worse job conveying the proportions of y, but give a better sense of the distribution of x (if we stack). Generally, the correct answer is to fill. plot1 &lt;- ggplot(spsp, aes(x=ethnicity, fill=ethnicity)) + geom_bar()+ ## the lines above form the basis of the plot, ## the lines below just make it look nicer scale_fill_brewer(palette = &quot;Set1&quot;)+ theme_bw()+ theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle=90, hjust=1)) plot2 &lt;- ggplot(spsp, aes(x=ethnicity, fill=ethnicity)) + geom_bar(aes(y = (..count..)/sum(..count..)))+ ## the lines above form the basis of the plot, ## the lines below just make it look nicer ylab(&#39;proportion&#39;)+ scale_fill_brewer(palette = &quot;Set1&quot;)+ theme_bw()+ theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle=90, hjust=1)) bin width and bandwidths The plots below will show how histograms and density plots change as a function of bin/band width. plots = list() i = 1 for(W in c(1, 5, 15)){ plots[[i]] &lt;- ggplot(cal1020, aes(x=time.sec/60))+ geom_histogram(binwidth = W)+ scale_x_continuous(&#39;Minutes&#39;, breaks=seq(0,240, by=30))+ theme_minimal() i &lt;- i+1 } for(W in c(1, 5, 15)){ plots[[i]] &lt;- ggplot(cal1020, aes(x=time.sec/60))+ geom_density(bw = W, fill=&#39;gray&#39;, alpha=0.5)+ scale_x_continuous(&#39;Minutes&#39;, breaks=seq(0,240, by=30))+ theme_minimal() i &lt;- i+1 } #do.call(gridExtra::grid.arrange, c(plots, ncol=3)) ggplot We will teach you to use ggplot for data visualization. It has a number of advantages once you are familiar with it. However, it has a somewhat steep learning curve. Making plots entirely by writing code will seem weird at first – you will want to interact with the plot by clicking on it in various ways, and you can’t. However, the advantage of writing code to make your plots is that once the data changes (you get more, or you decide to filter it one way or another, or adopt one transformation or another), you will not need to recreate the plots manually. This will seem like a negligible advantage to you now, but trust me, this will save you time in the end. Generally, many of the things we cover are described a bit differently in this ggplot tutorial and on the ggplot cheat sheet. Lots more resources may be found by googling. Installation. If you do not already have ggplot installed, install it with install.packages(&#39;ggplot2&#39;) You will need to load the ggplot library (and also grid, and gridExtra): library(ggplot2) library(grid) library(gridExtra) Basic overview. ggplot works on data frames: you give it a data frame and tell it how the various columns of the data frame should be mapped on to display properties. We specify the aesthetic mapping using the aes() to say how various data.frame columns should be used to generate a display. For instance, the x coordinate is one aesthetic property, the color is another property, as is size, or fill, or style of point. We then specify what kinds of geometric entities to add to the plot, which will follow the aesthetic mapping we described. Thus, to produce a scatter plot of two vectors x and y, instead of running a scatterplot(x,y) command of some sort, we would need to put them into a data frame, specify that ggplot should use that data frame, should map the x column on to x, and the y column on to y, and then add a “point” geometric entity. x = rnorm(100) y = 0.5*x + rnorm(100)*0.25 df.xy = data.frame(x=x, y=y) ggplot(data = df.xy, mapping = aes(x=x, y=y)) + geom_point() The ggplot(data = df.xy, mapping = aes(x=x, y=y)) + geom_point() line is doing a few things: ggplot(data = df.xy, tells ggplot to use df.xy as the data frame (the data= part can often be dropped) mapping = aes(x=x, y=y)) specifies that the x coordinate should be obtained from the x column of the data frame, and the y coordinate from the y column. + geom_point() says that we should add points that follow that aesthetic mapping. If we change the geometric entity from geom_point, we will produce a different graph. For instance, geom_line() will connect the points with lines: ggplot(data = df.xy, mapping = aes(x=x, y=y)) + geom_line() And we can add multiple geometric entities all following the same aesthetic mapping: ggplot(df.xy, aes(x=x, y=y)) + geom_point() + geom_line() Note that here we did two things to make the code eaiser to read. (1) we dropped the data = and mapping = part, as the order is sufficient to indicate which input to ggplot() is the data frame, and which is the mapping), and (2) we made the command span multiple lines by entering line breaks after the +s. While this is not strictly necessary, it makes the code easier to work with. Changing labels. We can add a title, and change the x and y label text. ggplot(df.xy, aes(x=x, y=y)) + geom_point() + geom_line() + ggtitle(&quot;Variable y as a function of variable x&quot;)+ xlab(&quot;This is the x variable&quot;) + ylab(&quot;This is the y variable&quot;) Themes and elements and properties. We can change the way in which various aspects of the display look, which ggplot refers to as the theme of the plot. There are many properties to the theme that are hierarchically organized. For instance the title property applies to all title text in the plot, including the plot title (plot.title), the axis titles (axis.title), the legend title (legend.title); thus if we were to set the title property to have green font, that would apply to all titles (unless we issued specific instructions to override this for more specific titles). Changing such theme properties is achieved with the theme() command, in which we specify which property of the plot we want to alter, and we set it by invoking a particular element type, initiated with whatever we want to change. Titles are text, so we change them by setting title=element_text(...) where the stuff in ... includes the element_text properties and the values we set them to. Such a command will override the properties that we set, but preserve the ones we did not set. ggplot(df.xy, aes(x=x, y=y)) + geom_point() + geom_line() + ggtitle(&quot;Variable y as a function of variable x&quot;)+ xlab(&quot;This is the x variable&quot;) + ylab(&quot;This is the y variable&quot;) + theme(title = element_text(color=&quot;green&quot;)) Despite the fact that this is hideous, a few things are worth noting: (a) by setting the color of the title property, we influenced the color of the plot title, the x axis title, and the y axis title, and (b) we only changed the color of all of those, and they retained their differences in font size. The values we set at a particular level of the plot hierarchy propagate down to all children, but this only influences the property that we changed. The theme properties can be partitioned into properties of the axes (axis.*, like axis.text for the labels for axis tick marks, axis.title for axis title, or more specific ones like axis.title.x), properties of the legend (legend.*), properties of the graph area (panel.*), properties of the whole image (plot.*), and properties of the “strip” that gives identifying information in multi-panel plots (strip.*). A fairly decent publication-grade plot theme can be created by simply invoking the black and white theme (theme_bw()): ggplot(df.xy, aes(x=x, y=y)) + geom_point() + geom_line() + ggtitle(&quot;Variable y as a function of variable x&quot;)+ xlab(&quot;This is the x variable&quot;) + ylab(&quot;This is the y variable&quot;) + theme_bw() Generally, I like somewhat larger axis text and titles, and no grid lines, so I might favor something like: ggplot(df.xy, aes(x=x, y=y)) + geom_point() + geom_line() + ggtitle(&quot;Variable y as a function of variable x&quot;)+ xlab(&quot;This is the x variable&quot;) + ylab(&quot;This is the y variable&quot;) + theme_bw() + theme(panel.grid = element_blank(), axis.text = element_text(size=12), title = element_text(size=16), axis.title = element_text(size=14)) We can also save a particular theme to reuse later: eds_theme &lt;- theme_bw() + theme(panel.grid = element_blank(), axis.text = element_text(size=12), title = element_text(size=16), axis.title = element_text(size=14)) ggplot(df.xy, aes(x=x, y=y)) + geom_point() + geom_line() + ggtitle(&quot;Variable y as a function of variable x&quot;)+ xlab(&quot;This is the x variable&quot;) + ylab(&quot;This is the y variable&quot;) + eds_theme Changing display properties of plotted elements. We can also change properties like size and color of the plot elements. There are two ways to do so: we might make those properties vary with some aspect of the data by including them in aes(), or we can make them be fixed by including them outside of aes(). For instance, if we want the size and color of the points to vary with the y value, we might write: ggplot(df.xy, aes(x=x, y=y)) + geom_point(aes(color=y, size=y)) + eds_theme But if we wanted to simply set a particular size and color for those points, we would note it outside of the aesthetic mapping in aes(): ggplot(df.xy, aes(x=x, y=y)) + geom_point(color=&quot;red&quot;, size=6) + eds_theme Other notable properties of geom_point() include their shape, their border color and fill color (for shapes 21-25), and their transparency (alpha). Besides color, width (“size”), and transparency (“alpha”), geom_line() has a ‘linetype’ property. So we can make quite a crazy plot by manipulating them all (and this is without even mapping them to data values in aes!) ggplot(df.xy, aes(x=x, y=y)) + geom_point(color=&quot;navy&quot;, size=10, alpha=0.5, shape=22, fill=&quot;steelblue&quot;) + geom_line(color=&quot;green&quot;, size=2, linetype=&quot;dotdash&quot;, alpha=0.5) + eds_theme Other geometric elements. Besides geom_line and geom_point, there are many other geometric elements we might plot (enumerated on the main reference page). We will cover these as we need to as we go over specific visualization types. Assembling multiple plots in one figure Often we want to make a few plots and attach them together. For instance, we might want to see the histograms of x, y, and their scatterplot. This can be accomplished with the gridExtra package, which you can install with install.packages('gridExtra'). The strategy is to define the various plots, and save them to variables, then put them together with grid.arrange(). library(gridExtra) hx &lt;- ggplot(df.xy, aes(x=x)) + geom_histogram(fill=&#39;green&#39;) + theme_bw() hy &lt;- ggplot(df.xy, aes(x=y)) + geom_histogram(fill=&#39;purple&#39;) + theme_bw() xy &lt;- ggplot(df.xy, aes(x=x, y=y)) + geom_point(color=&quot;red&quot;, size=6) + theme_bw() grid.arrange(hx,hy,xy,ncol=3) We might rather want the x and y histograms to align with the x and y axes of the scatter plot, so we might want to arrange this as a 2x2 grid, while flipping the coordinates of the y histogram (note that we need a blank panel for the upper right). hx &lt;- ggplot(df.xy, aes(x=x)) + geom_histogram(fill=&#39;green&#39;) + theme_bw() hy &lt;- ggplot(df.xy, aes(x=y)) + geom_histogram(fill=&#39;purple&#39;) + coord_flip() + # this flips the histogram sideways. theme_bw() xy &lt;- ggplot(df.xy, aes(x=x, y=y)) + geom_point(color=&quot;red&quot;, size=3) + theme_bw() grid.arrange(hx, grid.rect(gp=gpar(col=&quot;white&quot;)), # this is just a white blank. xy, hy, ncol=2) There is still lots left to fix here, and I will walk through the process of making this figure presentable here. In the meantime, lets move on. "],["probability.html", "Probability Probability terms Foundations of probability Conditional probability and Bayes Simulation, Sampling and Monte Carlo. Random variables Distribution functions: PDF, CDF, Quantile Expectation and moments Central limit theorem and the normal distribution", " Probability Probability terms Absolute probability statements {prob} A probability of something is a number between 0 and 1: \\(p \\in [0,1]\\). In epidemilogy probability of some outcome of a particular group/behavior is also called risk. e.g., if smokers have a 0.125 probability of getting lung cancer, their risk of cancer is 0.125. The corresponding percent is between 0 and 100 = \\(p*100\\). e.g., a 12.5% chance of lung cancer for smokers. The corresponding odds are the ratio of the probability of thing happening, and the probability of it not happening: \\(\\mbox{odds} = p/(1-p)\\). E.g., odds of a smoker getting lung cancer is 0.125/(1-0.125) = 1/7 (often expressed as 1:7). The corresponding log-odds are the logarithm of the odds: \\(\\mbox{log-odds} = \\log(\\mbox{odds})\\). e.g., log odds of lung cancer for smokers is -0.845. From this we can get a number of relationships: \\(p = \\mbox{odds}/(1+\\mbox{odds})\\) \\(\\mbox{odds} = \\exp(\\mbox{log-odds})\\) \\(\\mbox{log-odds} = \\log\\left({\\frac{p}{1-p}}\\right)\\) (this is known as the logit transform, going from probability to log-odds) \\(p = \\frac{1}{1+\\exp(-\\mbox{log-odds})}\\) (this is the logistic transform, going from log-odds to probability) A proportion is a descriptive statistic if \\(k\\) of my \\(n\\) observations were “fish,” then I can say that the proportion of fish in my sample is \\(k/n\\). These proportions are also often treated directly as probabilities, risks and converted into percents, odds, etc. A hazard rate (also failure rate, or hazard function) is a time-varying probability associated with some survival function: what is the probability that someone will die right now, given that they have survived up to now? Probability comparisons The relative risk is a ratio of two probabilities, usually some “treatment” and some “baseline.” For instance risk of lung cancer for smokers is 0.125, for non-smokers it is 0.003, so the relative risk of lung cancer for smoking is 0.125/0.003 = 41. The odds ratio is the ratio of odds between some treatment and baseline. for instance, the odds ratio of lung cancer for smoking (compared to not) is (0.125/(1-0.125)) / (0.003/(1-0.003)) = 47.5. The log odds ratio is the same as the difference in log odds: log((0.125/(1-0.125)) / (0.003/(1-0.003))) = log(0.125/(1-0.125)) - log(0.003/(1-0.003)) = 1.67 We might also calculate the linear difference in probabilities (or more often) percent; in this case, we should make it clear that we are talking about a difference in percentage points: e.g., the chance of lung cancer among smokers (12.5%) is 12.2 percentage points higher than in non-smokers (0.3%). Proportional magnitudes and confusion Unfortunately, we use the language of “proportions” not only to describe the sample statistic corresponding to a probability estimate, but we also use it when describing the relative magnitudes of two quantities. For instance, my dog’s weight is 50 lbs, my weight is 160 lbs. So, proportional to my weight, my dog is 5/16ths, or 0.3 times, or 30% of my weight, we might also say my dog weighs 70% less than me. My weight is 16/5ths, or 3.2 times, or 320% of my dogs weight, and we might say that I weight 220% more than my dog. The unfortunate similarities between the words used for probabilities and the words used for proportional magnitude comparisons, along with the natural tendency to compare the proportional magnitudes of probabilities (e.g., relative risk, odds ratio), tends to create a bit of a mess when probabilities are discussed in public. Watch out for confusing statements like “the chance is x% higher.” Does the x% refer to a linear difference in percentage points, or a claim about the magnitude of relative risk? Foundations of probability Probability theory is the extension of propositional logic to operate over uncertain truth values. If propositions are not either true or false, but are true with some probability, how can we combine multiple propositions to deduce the probability associated with some derived proposition? The basic rules of probability are built by assigning probability to “outcomes” of a possible “sample space.” We are interested in “events,” which are subsets of the sample space. Since so much of this formalism is in set notation, so we will start there. Set notation for combinations of outcomes. We can build up sophisticated probability rules by combining the outcomes of the sample space in various ways. These correspond to set operations of union, intersection, and complement. The sample space is the set of all possible outcomes of an “experiment.” Each possible outcome of an experiment is an “elementary event,” in the sense that all the outcomes are mutually exclusive. The sample space is usually denoted as \\(\\Omega\\). So if we consider the roll of a six-sided die, there are six possible outcomes (1, 2, 3, 4, 5, 6), so we would describe the sample space as \\(\\Omega=\\{1,2,3,4,5,6\\}\\). outcomes = c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;) # writing as characters to make it clear these are symbols corresponding to outcomes. Set union corresponds to a disjunction or an OR operation. The set union of A and B is the set of elements that appear in either A or B. \\(x \\in (A \\cup B) \\mbox{ if } x \\in A \\mbox{ or } x \\in B\\) The set union is denoted with a \\(\\cup\\) operator. For instance the union of outcomes a and b is the set that includes both: \\(a \\cup b = \\{a,b\\}\\). The union of two sets is the set of all elements that appear in either set: \\(\\{a,b,c\\} \\cup \\{b,c,d,e,f\\} = \\{a,b,c,d,e,f\\}\\); note that the union operation returns a set, so elements that appear in both input sets are not “double counted” – each element will appear only once in a set. We will often want to express a union of many sets, which we can write as \\(\\bigcup_{i=1}^n x_i = x_1 \\cup x_2 \\cup ... \\cup x_n\\). union(c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;), c(&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;)) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; Set intersection corresponds to a conjunction or an AND operation The set intersection of A and B is the set of elements that appear in both A and B. \\(x \\in (A \\cap B) \\mbox{ if and only if } x \\in A \\mbox{ and } x \\in B\\) Set intersection is denoted with a \\(\\cap\\) operator. The intersection of two sets is the set of elements that appear in both sets. For instance \\(\\{a,b,c\\} \\cap \\{b,c,d,e,f\\} = \\{b,c\\}\\). The intersection of two sets that share no elements is the null, or empty, set \\(\\{a,b,c\\} \\cap \\{d,e,f\\} = \\emptyset\\), these sets are called disjoint. intersect(c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;), c(&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;)) ## [1] &quot;b&quot; &quot;c&quot; Set complement is negation: the set of possible elements that are NOT in the set. \\(\\neg A = \\Omega \\setminus A\\) Generally, it is clearer to explicitly refer to a relative set complement, or set difference, to specify which “possible” elements to consider, this is denoted \\(B \\setminus A\\) – the set of elements in B that are not in A: \\(\\{a,b,c\\} \\setminus \\{b,c,d,e,f\\}=\\{a\\}\\). In our context, we will talk about the absolute set comlement, which we will denote with the logical negation operator \\(\\neg A\\) (conventionally this would be written with the superscript c: \\(A^\\complement\\)). The absolute set complement has an implicit relative complement to the set of all possible outcomes, in our case \\(\\neg A = \\Omega \\setminus A\\). setdiff(outcomes, c(&#39;2&#39;, &#39;4&#39;, &#39;6&#39;)) ## [1] &quot;1&quot; &quot;3&quot; &quot;5&quot; It may now be apparent that the set operations we consider also correspond to the basic building blocks of propositional logic: disjunctions, conjunction, and negation. We will see that the rules of probability are effectively the rules of logic extended to apply to uncertain truth values (as probabilities). Basic probability definition and axioms Probability is assigned to each outcome, and usually written as \\(P(\\cdot)\\). So we would write the probability of a particular outcome (say, rolling a 6) as \\(P(\\mbox{&quot;6&quot;})\\), but usually we would just substitute a symbol to stand in for a specific outcome (e.g., \\(u = \\mbox{&quot;6&quot;}\\), so probability of rolling a six would be \\(P(u)\\)). Probability is always non-negative (and as we will see soon, no larger than 1, meaning it falls in the interval \\([0,1]\\)). Probability is a number between 0 and 1 assigned to every possible outcome in the sample space \\(P(x) \\in [0, 1] \\mbox{ for all } x \\in \\Omega\\). p.outcomes = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6) names(p.outcomes) &lt;- outcomes p.outcomes ## 1 2 3 4 5 6 ## 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 The probability of a union of two outcomes is the sum of their probabilities. This basic axiom of probability allows us to calculate the probability of one of a set of outcomes happening based on the probabilities of the individual elements in that set, from this axiom we can build many of the other laws of probability. Probability of a union of two outcomes is the sum of their probabilities \\(P(a \\cup b) = P(\\{a,b\\}) = P(a) + P(b)\\). Note that this simple addition applies to the union of outcomes because those are necessarily different and non-overlapping isolated elements. This will generally not hold true for disjunctions of events, which may consist of overlapping sets of outcomes. p.1.2 = sum(p.outcomes[c(&#39;1&#39;, &#39;2&#39;)]) The final axiom of probability is that it sums to 1, meaning that the total probability being distributed over the sample space is 1. Total probability of the sample space is 1.0 \\(P(\\Omega) = P\\left( {\\bigcup\\limits_{x \\in \\Omega} x }\\right) = \\sum\\limits_{x \\in \\Omega} P(x) = 1\\) Basically, this means all the probabilities of isolated outcomes have to sum to 1. sum(p.outcomes) ## [1] 1 Applying basic probability axioms So far we have dealt with a uniform probability distribution: each side of the die has the same chance (1/6), or more generally \\(1/|\\Omega|\\) where \\(|x|\\) indicates the number of elements in \\(x\\). We could define such a uniform probability distribution over outcomes more generally as: p.outcomes = rep(1/length(outcomes), length(outcomes)) names(p.outcomes) &lt;- outcomes p.outcomes ## 1 2 3 4 5 6 ## 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 Of course, this assumes we have a fair die (all sides have equal probability). Instead, our die might be biased. Let’s say it is biased so that 1 is twice as likely to come up as any of the other five outcomes (which all are equally likely). We can do some algebra to figure out what this means about the probabilities assigned to each outcome: \\(P(\\mbox{&#39;1&#39;}) = 2*P(\\mbox{&#39;other&#39;})\\) And since – all probabilities must sum to 1: \\(P(\\mbox{&#39;1&#39;}) + 5*P(\\mbox{&#39;other&#39;}) = 1\\) so \\(2 * P(\\mbox{other}) + 5 * P(\\mbox{other}) = 1\\) \\(P(\\mbox{other}) = 1/7\\) and \\(P(\\mbox{1}) = 2/7\\). We can also calculate this quickly in R. Here we define “unnormalized” probabilities (which do not sum to 1, but have the appropriate relative relationships). We can normalize a vector of numbers by dividing every element by the sum of all elements, thus returning probabilities with the appropriate relationships that all sum to 1: unnormalized = c(2, 1, 1, 1, 1, 1) p.outcomes = unnormalized/sum(unnormalized) names(p.outcomes) &lt;- outcomes p.outcomes ## 1 2 3 4 5 6 ## 0.2857143 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 These axioms are sufficient to derive an assortment of probability rules that we are used to for describing events. Events and the rules of probability. Probability becomes useful when we start considering events. An event is a subset of outcomes from the sample space. \\(E \\subset \\Omega\\) For instance, in our die-rolling example, an event might be “rolling an even number” which is a subset of the sample space: \\(E = \\{2, 4, 6\\}\\). By our axiom about the probability of a union of outcomes, we know that The probability of an event is the sum of the probabilities of the outcomes included in it: \\(P(E) = \\sum\\limits_{x \\in E} P(x)\\). For instance, the probability of “even” is P(even) = P(2)+P(4)+P(6). p.event = function(subset){ sum(p.outcomes[subset]) } even = c(&#39;2&#39;, &#39;4&#39;, &#39;6&#39;) p.event(even) ## [1] 0.4285714 (remember, we are still dealing with the unfair dice roll with a ‘1’ twice as likely as the other outcomes) We can combine events with conjunctions and disjunctions. While we will mostly refer to conjunctions and disjunctions using the familiar terms “and” (\\(\\&amp;\\)) and “or” (\\(\\lor\\)) instead of the set notation “intersection” (\\(\\cap\\)) and “union” (\\(\\cup\\)), it is useful to flesh out how they are related. Consider the events even (\\(E=\\{2,4,6\\}\\)), and greater than 3 (\\(G=\\{4, 5, 6\\}\\)). A conjunction (and) of two events is also an event defined as their set intersection. For instance if we consider C to be the conjunction of events E (even, \\(E=\\{2,4,6\\}\\)) and G (greater than 3, \\(G = \\{4,5,6\\}\\)), C = E and G = \\(E \\cap G = \\{2,4,6\\} \\cap \\{4,5,6\\} = \\{4,6\\}\\). even = c(&#39;2&#39;, &#39;4&#39;, &#39;6&#39;) greater = c(&#39;4&#39;, &#39;5&#39;, &#39;6&#39;) intersect(even, greater) ## [1] &quot;4&quot; &quot;6&quot; p.event(intersect(even,greater)) ## [1] 0.2857143 A disjunction (or) of two events is also an event defined as their set union. For instance even OR “greater than 3”: \\(B = E \\cup G = \\{2, 4, 6\\} \\cup \\{4, 5, 6\\} = \\{2, 4, 5, 6\\}\\). union(even,greater) ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;5&quot; p.event(union(even,greater)) ## [1] 0.5714286 A negation (not) of an event is the set complement of that event. For instance “not even” is defined as the set of outcomes in the sample space that are not even. \\(\\neg E = \\neg \\{2,4,6\\} = \\{1,2,3,4,5,6\\} \\\\ \\{2,4,6\\} = \\{1,3,5\\}\\). setdiff(outcomes, even) ## [1] &quot;1&quot; &quot;3&quot; &quot;5&quot; p.event(setdiff(outcomes, even)) ## [1] 0.5714286 Fortunately, we need not always carry out complicated set operations and tabulation to get the probability of an event resulting from these operations. Probability of a disjunction (OR) The probability of A or B is \\(P(A \\lor B) = P(A) + P(B) - P(A \\&amp; B)\\) Subtracting the conjunction should make sense. Consider for instance “even” OR “greater than 3” = \\(\\{2,4,6\\} \\cup \\{4,5,6\\} = \\{2,4,5,6\\}\\). \\(P(\\{2,4,5,6\\}) = 4/7\\). If we don’t subtract the conjunction we get an incorrect answer: \\(P(\\{2,4,6\\}) + P(\\{4,5,6\\}) = 3/7 + 3/7 = 6/7\\). We have to subtract the conjunction (\\(P(\\{4,6\\})=2/7\\)) to get the correct answer, because otherwise we end up “double counting” the outcomes that appear in both events. Note that for disjoint events – meaning events that are mutually exclusive, thus they cannot co-occur – \\(P(A \\&amp; B)=0\\), so we need not subtract it. This is why when we were calculating the probability of a disjunction of outcomes (which are necessarily disjoint) we omitted that part. p.event(even) + p.event(greater) - p.event(intersect(even,greater)) ## [1] 0.5714286 p.event(union(even,greater)) ## [1] 0.5714286 Probability of negation (NOT) The probability of not A is \\(P(\\neg A) = 1-P(A)\\). This should also make sense: We know that “not A” is the complement of A relative to the sample space. We also know that the total probability of the sample space is 1. Thus, the probability of outcomes not in A (A’s complement) must be 1 less the probability of the outcomes in A. Here we calculate the probability of “not even” two different ways: 1-P(even) and by defining a “not even” event which is the set of outcomes not included in the “even” event. As they should, they give us the same result. (We obtain the outcomes in the “not even” event via the setdiff(outcomes, even) function, which returns all the elements in outcomes that are not in even.) 1-p.event(even) ## [1] 0.5714286 p.event(setdiff(outcomes, even)) ## [1] 0.5714286 To discuss the probability of a conjunction we need to first start with Conditional probability. Conditional probability and Bayes Let’s start by remembering where we were with our unfair die roll. outcomes = c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;) unnormalized = c(2, 1, 1, 1, 1, 1) p.outcomes = unnormalized/sum(unnormalized) names(p.outcomes) &lt;- outcomes p.outcomes ## 1 2 3 4 5 6 ## 0.2857143 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 p.event = function(subset){ sum(p.outcomes[subset]) } even = c(&#39;2&#39;, &#39;4&#39;, &#39;6&#39;) greater = c(&#39;4&#39;, &#39;5&#39;, &#39;6&#39;) The Conditional probability of event A given that event B has occurred is the probability of A and B divided by the probability of B: \\(P(A \\mid B) = \\frac{P(A \\&amp; B)}{P(B)}\\) Which might be read as: of all the ways in which B might occur, how many of them co-occur with A? For instance, the probability of “even” given “greater than 3” is asking: of all the ways the dice could come up greater than 3, which of them also yield an even roll? \\(P(\\mbox{even} \\mid \\mbox{greater than 3}) = P(\\{2,4,6\\} \\mid \\{4,5,6\\}) = \\frac{P(\\{4,6\\})}{P(\\{4,5,6\\})}\\) # probability of even given greater than 3 p.event(intersect(even,greater)) / p.event(greater) ## [1] 0.6666667 From this definition, we can find a general expression for the probability of a conjunction. The probability of a conjunction of two events (A and B): \\(P(A \\&amp; B) = P(A \\mid B) * P(B) = P(B \\mid A) * P(A)\\) There are two special cases worth mentioning: Two events are disjoint (mutually exclusive) if their interesction is null: \\(A \\cap B = \\emptyset\\); consequently: \\(P(A \\&amp; B) = P(A \\mid B) = P(B \\mid A) = 0\\), and \\(P(A \\lor B) = P(A) + P(B)\\) If two events are independent then: \\(P(A \\&amp; B) = P(A) * P(B)\\), and \\(P(A \\mid B) = P(A)\\), and \\(P(B \\mid A) = P(B)\\) Note that often this special case (of independent events) is described as the rule for the probability of a conjunction, but in most cases, events are not independent. Chain rule We can calculate complex conjunctions by repeating the same process of calculating a conjunction many times via the chain rule: \\(P(a \\&amp; b \\&amp; c \\&amp; d) = P(a \\mid b \\&amp; c \\&amp; d) * P(b \\mid c \\&amp; d) * P(c \\mid d) * P(d)\\) Partitions and total probability To express Bayes rule as it is commonly used, we first need to introduce two concepts. A partition of sample space \\(\\Omega\\) is a set of events that are disjoint, nonempty, and their union is \\(\\Omega\\). In other words, \\(A\\) is a partition of \\(\\Omega\\) if every element of \\(\\Omega\\) is in one and only one event in A. \\(\\emptyset \\not\\in A\\) \\(\\bigcup\\limits_{E \\in A} E = \\Omega\\) \\(E_1 \\cap E2 = \\emptyset \\mbox{ for all } E_1, E_2 \\in A \\mbox{ and } E_1 \\not= E_2\\) For instance, the events {“even” and “odd”} form a partition of the sample space of die rolls. In contrast, {“even,” “odd,” and “greater than 3”} do not form a partition, as they include overlapping events (they are not disjoint). Similarly, {“1 or 2,” and “4 or 6”} are not a partition as they do not cover the full sample space (their union is not \\(\\Omega\\)). The law of total probability says that if events \\(A_1,A_2, ..., A_n\\) are a partition of \\(\\Omega\\), then \\(P(B) = \\sum\\limits_{i=1}^n P(B|A_i)P(A_i) = \\sum\\limits_{i=1}^n P(B \\&amp; A_i)\\). We can confirm that this works in the simple case of B = “greater than 3,” and the partition is “even” and “odd.” P(“greater than 3”) = P(“greater than 3”|even) P(even) + P(“greater than 3”|odd) P(odd) P(“greater than 3”) = (2/3)(3/7) + (1/4)(4/7) = 3/7 greater = c(&#39;4&#39;, &#39;5&#39;, &#39;6&#39;) even = c(&#39;2&#39;, &#39;4&#39;, &#39;6&#39;) odd = c(&#39;1&#39;, &#39;3&#39;, &#39;5&#39;) p.greater.even = p.event(intersect(even,greater)) / p.event(even) p.greater.odd = p.event(intersect(odd,greater)) / p.event(odd) p.even = p.event(even) p.odd = p.event(odd) p.greater.even * p.even + p.greater.odd * p.odd ## [1] 0.4285714 p.event(greater) ## [1] 0.4285714 Bayes’ rule You can find an “excruciatingly gentle” introduction to Bayes’ rule. Bayes rule follows directly from our definitions of conjunctions and conditional probabilities \\(P(B|A)P(A) = P(A \\&amp; B) = P(A|B)P(B)\\) Bayes rule provides a way to invert conditional probabilities: to go from P(B|A) to P(A|B) \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\) More often, in practice we substitute the law of total probability for the denominator of the Bayes rule expression: \\(P(A|B) = \\frac{P(B|A)P(A)}{\\sum\\limits_{A&#39;}P(B|A&#39;)P(A&#39;)}\\), where \\(\\sum\\limits_{A&#39;}\\) denotes a sum over all the events in the partition of which A is a member. Bayes’ theorem is the basis for an entire branch of statistics, as well as a number of artificial intelligence applications. In statistics, Bayes’ theorem is often written in terms of individual hypotheses \\(h\\) from the hypothesis space \\(\\mathcal{H}\\) and data \\(D\\), yielding: \\(P(h|D) = \\frac{P(D|h)P(h)}{P(D)} = \\frac{P(D|h)P(h)}{\\sum\\limits_{q \\in \\mathcal{H}} P(D|q) P(q)}\\) The individual components of this equation have names: \\(P(h)\\) is the prior: how much you believe hypothesis \\(h\\) is true before you saw data \\(D\\). \\(P(D|h)\\) is the likelihood: how likely are the data \\(D\\) to have been observed if hypothesis \\(h\\) were true. \\(P(h|D)\\) is the posterior: how much you ought to believe hypothesis \\(h\\) is true given that you have seen data \\(D\\). \\(P(D)\\) is often called the normalizing constant because it is constant regardless of which \\(h\\) you consider. Consequently, it is often dropped and Bayes’ theorem is written as a proportionality: \\(P(h|D) \\propto P(D|h)P(h)\\) We will come back to all of these at a later point. In the meantime, we can use Bayes rule to calculate the probability that a published significant finding is truly not null (H1 rather than H0). P(‘significant’|H0) = \\(\\alpha\\) = 0.05 P(‘significant’|H1) = “power” = 0.6 P(H1) = 0.3 = 1-P(H0) p.sig.H0 = 0.05 # conventional alpha p.sig.H1 = 0.6 # assumption about power. p.H1 = 0.3 # assumption about baserate of tested effects p.H0 = 1-p.H1 p.sig = p.sig.H0*p.H0 + p.sig.H1*p.H1 p.H1.sig = p.sig.H1*p.H1 / p.sig p.H1.sig ## [1] 0.8372093 Simulation, Sampling and Monte Carlo. Sampling is a very helpful tool to understand probability. When I say that the probability of a die coming up x is P(x), it implies that P(x) is the long-run frequency of the outcome: how often x would happen if we rolled this die many times. So, instead of going through the math to calculate various properties of this die-roll, we can simulate die rolls, and work on those simulations. This works on account of various “laws of large numbers,” and in particular, the “Monte Carlo” theorem, which basically says: if you want to calculate the expectation of some function on a random variable, you can calculate it by averaging that function’s output on a bunch of samples drawn with frequency proportional to their probability under the random variable. Let’s start by setting up the same die probabilities we worked with when discussing the foundations of probability: library(dplyr) outcomes &lt;- c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;) unnormalized = c(2, 1, 1, 1, 1, 1) p.outcomes = unnormalized/sum(unnormalized) names(p.outcomes) &lt;- outcomes p.outcomes ## 1 2 3 4 5 6 ## 0.2857143 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 Sampling, long-run frequency, and the law of large numbers. Another way to think about this probability is as the long-run frequency of a given outcome. So we can set up a function that rolls a die and returns the side that it landed on. In this case, we can implement such sampling with the sample function. roll.die = function(){ return(sample(outcomes, 1, replace=T, prob = p.outcomes)) } Every time we roll the die, we get a random outcome, we would call this a sample roll.die() ## [1] &quot;1&quot; roll.die() ## [1] &quot;1&quot; Borel’s law of large numbers is the intuitive rule that if we repeat a random experiment many many times, the proportion of those times that had a particular outcome occur will match the probability of that outcome occurring in any one run of the experiment. In our case, we can generate many rolls of the die, then calculate what fraction of them yielded a particular outcome: rolls = replicate(10000,roll.die()) ( p.1 = sum(rolls==1)/length(rolls) ) ## [1] 0.2824 ( p.4 = sum(rolls==4)/length(rolls) ) ## [1] 0.1435 ( p.8 = sum(rolls==8)/length(rolls) ) ## [1] 0 Sampling to estimate event probabilities. An event in probability theory is a subset of outcomes. For instance, for our rolled die, we might define an event as “getting an even roll.” It is useful to us to think of an event as a predicate, which we apply to the outcomes to see whether or not they fit into the event. To make these kinds of calculations explicit, we can make a data frame listing all outcomes, their probabilities, and whether or not they are included in a particular event: is.even = function(outcome){ return((as.numeric(outcome) %% 2)==0) # x %% y is the modulus operator which returns the remainder after dividing x by y. } Just as we can calculate the probability of a particular outcome from samples, we can do the same with an event. We generate many samples, see which fall in an event, and estimate their relative frequency: die.samples = data.frame(outcome = replicate(10000, roll.die()), stringsAsFactors = FALSE) die.samples$even = is.even(die.samples$outcome) head(die.samples) ## outcome even ## 1 1 FALSE ## 2 6 TRUE ## 3 4 TRUE ## 4 4 TRUE ## 5 5 FALSE ## 6 6 TRUE ( p.even = sum(die.samples$even) / nrow(die.samples) ) ## [1] 0.4275 Note that this formulation of the explicit probability calculation (data frame of outcomes with associated probabilities, or data frame of samples) makes it clear what the relationship is. Basically, we treat the probability of each sampled outcome as 1/n where n is the number of samples. Probability of a conjunction of events The conjunction of events A and B may be thought of as another event – another subset of the outcomes that is the set intersection of A and B. Alternatively, we can just think of it as a conjunction predicate that returns true if both events are true of this outcome. First let’s define a second event “greater than 3” (which is the subset 4,5,6 of dice outcomes). is.greaterthan3 = function(outcome){ return(outcome &gt; 3) } die.samples$greaterthan3 = is.greaterthan3(die.samples$outcome) head(die.samples,10) ## outcome even greaterthan3 ## 1 1 FALSE FALSE ## 2 6 TRUE TRUE ## 3 4 TRUE TRUE ## 4 4 TRUE TRUE ## 5 5 FALSE TRUE ## 6 6 TRUE TRUE ## 7 5 FALSE TRUE ## 8 1 FALSE FALSE ## 9 6 TRUE TRUE ## 10 6 TRUE TRUE Now we can define the conjunction event, which we can calculate in two ways – either by defining a new conjunction predicate, or by simply taking the logical conjunction of our two event columns. is.even.n.greaterthan3 = function(outcome){ return( is.even(outcome) &amp; is.greaterthan3(outcome) ) } die.samples$even.n.greaterthan3 = is.even.n.greaterthan3(die.samples$outcome) ( p.even.n.greaterthan3 = mean(die.samples$even.n.greaterthan3) ) ## [1] 0.2831 Note that here we take the shortcut of taking the mean of the logical vector even.n.greaterthan3 to calculate the proportion that is true. This works because true=1, false=0, consequently sum(TF) = n.true, and mean(TF) = n.true/n.total – the proportion that were true. Sampling to get probability of disjunction is.even.or.greaterthan3 = function(outcome){ return( is.even(outcome) | is.greaterthan3(outcome) ) } die.samples$even.or.greaterthan3 = is.even.or.greaterthan3(die.samples$outcome) ( p.even.or.greaterthan3 = mean(die.samples$even.or.greaterthan3) ) ## [1] 0.5666 Sampling to calculate conditional probability We can calculate a conditional probability in two ways (say, P(‘even’ | ‘greater than 3’)) We can make a subset of the samples, for which the condition is true, and do our regular calculations on it. ( p.even_greaterthan3 = filter(die.samples, greaterthan3 == TRUE) %&gt;% summarize(p.even = mean(even)) ) ## p.even ## 1 0.6705353 Or we can take the shortcut from our understanding the definition of conditional probability (P(A|B) = P(A&amp;B)/P(B)) ( p.even_greaterthan3 = sum(die.samples$even.n.greaterthan3)/sum(die.samples$greaterthan3) ) ## [1] 0.6705353 Random variables While it is easiest to explain the basic rules of probability in terms of sample spaces, outcomes, and events, in practice, we want to know about probability for describing random variables. A random variable is a variable with a value obtained by measuring some stochastic process. We can consider a complicated sample space in which we roll hundreds of dice (that is our stochastic process), but we can define a random variable \\(X\\) as the total number of dots on two specific dice (die A, and die B). This is very useful, since the world we measure is very complicated, but we simplify our lives by only dealing with simple observations of small aspects of it. A random variable is a variable with a value that is subject to random variation. For instance, if we roll a die twice, and define \\(X\\) to be the sum of the two rolls, \\(X\\) is a random variable. The set of values the random variable can take on is called its support. The variable \\(X\\) (sum of two dice rolls) can take on integers between 2 and 12, so its support is {2,3,4,5,6,7,8,9,10,11,12}. The probability distribution function of a random variable (e.g., \\(f_X(x)\\) or \\(P(X=x)\\) for \\(X\\)), describes the probability that the variable will take on any of its possible values. We can calculate the probability distribution function for the sum of two dice rolls (with our weird, uneven die) using the rules of probability by summing over all the ways that X will take on a particular value. outcomes = c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;) unnormalized = c(2, 1, 1, 1, 1, 1) p.outcomes = unnormalized/sum(unnormalized) names(p.outcomes) &lt;- outcomes p.outcomes ## 1 2 3 4 5 6 ## 0.2857143 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 P(X=2) = P(‘roll-1’ = 1 &amp; ‘roll-2’ = 1) = (2/7)(2/7) = 4/49 P(X=3) = P(‘roll-1’ = 1 &amp; ‘roll-2’ = 2) + P(‘roll-1’ = 2 &amp; ‘roll-2’ = 1) = (2/7)(1/7) + (1/7)(2/7) = 4/49 etc. library(dplyr) # this line creates a data frame with every combination of die a and die b outcomes. joint.outcomes &lt;- tidyr::expand(tibble(die.a = outcomes, die.b = outcomes), die.a, die.b) joint.outcomes &lt;- joint.outcomes %&gt;% mutate(X = as.numeric(die.a) + as.numeric(die.b), # add column for sum of both dice. p.die.a = p.outcomes[die.a], # add columns corresponding to the independent probabilities of die 1 and die 2 p.die.b = p.outcomes[die.b], p.conjunction = p.die.a*p.die.b) # add column corresponding to joint probability (product, given independence) head(joint.outcomes) ## # A tibble: 6 x 6 ## die.a die.b X p.die.a p.die.b p.conjunction ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 0.286 0.286 0.0816 ## 2 1 2 3 0.286 0.143 0.0408 ## 3 1 3 4 0.286 0.143 0.0408 ## 4 1 4 5 0.286 0.143 0.0408 ## 5 1 5 6 0.286 0.143 0.0408 ## 6 1 6 7 0.286 0.143 0.0408 rv.X &lt;- joint.outcomes %&gt;% group_by(X) %&gt;% # group by value of X (sum of the two dice) summarize(p.X = sum(p.conjunction)) # sum up the probability of all outcomes that have that sum head(rv.X, 11) ## # A tibble: 11 x 2 ## X p.X ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.0816 ## 2 3 0.0816 ## 3 4 0.102 ## 4 5 0.122 ## 5 6 0.143 ## 6 7 0.163 ## 7 8 0.102 ## 8 9 0.0816 ## 9 10 0.0612 ## 10 11 0.0408 ## 11 12 0.0204 # just to confirm, let&#39;s make sure the probabilities sum to 1, since they ought to sum(rv.X$p.X) ## [1] 1 library(ggplot2) rv.X %&gt;% ggplot(aes(X, p.X))+geom_bar(stat=&quot;identity&quot;) In short, we can obtain the probability distribution of a random variable by applying the rules of probability to a description of the stochastic process presumed to determine the value of that random variable. Notice that the probability distribution \\(P(X)\\) is not uniform: \\(P(X=12) \\neq P(X=7)\\). This will be the case for most distributions we consider. We can define lots of other variables from the same sample space (of the outcomes of two dice rolls). \\(A\\) might be the number of dots on die A (range \\(\\{1, ..., 6\\}\\)); \\(B\\) could be the number of dots on die B (range \\(\\{1, ..., 6\\}\\)); \\(Y\\) could be \\(A-B\\) (range is \\(\\{-5, -4, ... , 4, 5\\})\\) \\(Z\\) could be \\(A \\cdot B\\), etc. Each of these variables will have a different probability distribution, and all of them are usually written as \\(P(\\cdot)\\), which can create confusion. One way to make this less confusing is to spell it out, for instance \\(P(X=x)\\) is the probability distribution over the random variable \\(X\\) which assigns probability to all possible outcomes, denoted in the lower case \\(x\\). This ends up being long-winded, and invariably people fall back to abbreviating to something like \\(P(x)\\), but this sometimes causes confusion when dealing with multiple variables. I try to deal with this by subscripting the probability functions, so in place of \\(P(X=x)\\), I would write \\(P_X(x)\\). This has its own host of problems, but I think it is sometimes useful. Joint, conditional, and marginal probabilities Let’s consider the random variables \\(X\\) (sum of two dice rolls) and \\(Y\\) (difference of roll A minus roll B). The probability distribution of each of these variables in isolation (that is, if we disregard the value of the other variables) is called the marginal distribution of that variable (because we are effectively “marginalizing” over the other variable). For instance, the marginal distribution \\(P(X=x)\\) (the sum of two dice rolls) will be the distribution we calculated above when we disregarded the difference between the two rolls. If we consider the distribution over one variable (say \\(X\\)) while holding another one fixed, we get the conditional distribution. So if we consider the distribution of the sum of the two dice, given that die A - die B = 2, we are interested in the conditional distribution of X given \\(Y=2\\): \\(P(X=x|Y = 2)\\). If the variables are not independent, this will be quite different from the marginal distribution \\(P(X)\\). # let&#39;s add a column for the values of X and Y to our joint outcomes data frame joint.outcomes &lt;- joint.outcomes %&gt;% mutate(Y = as.numeric(die.a) - as.numeric(die.b)) # make a new column for the value of Y # now we will do the same &quot;group_by(X) and sum probabilities&quot; procedure, but only for cases where Y==2 rv.X.Y_2 = joint.outcomes %&gt;% filter(Y==2) %&gt;% # consider only outcomes where Y==2 group_by(X) %&gt;% # group by value of X (sum of the two dice) summarize(p = sum(p.conjunction)) # sum up the probability of all outcomes that have that sum head(rv.X.Y_2, 11) ## # A tibble: 4 x 2 ## X p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 0.0408 ## 2 6 0.0204 ## 3 8 0.0204 ## 4 10 0.0204 # note that the sum is no longer 1. Indeed, the sum will be P(Y==2), since those are the only outcomes we are counting sum(rv.X.Y_2$p) ## [1] 0.1020408 # consequently, we need to divide by P(Y==2) to get the probability P(X|Y==2) rv.X.Y_2 &lt;- mutate(rv.X.Y_2, p=p/sum(p)) head(rv.X.Y_2, 11) ## # A tibble: 4 x 2 ## X p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 0.4 ## 2 6 0.2 ## 3 8 0.2 ## 4 10 0.2 rv.X.Y_2 %&gt;% ggplot(aes(X, p))+geom_bar(stat=&quot;identity&quot;) Finally, we might consider the joint distribution which distributes probability over conjunctions of variables. For instance, the joint distribution \\(P(X=x, Y=y)\\) (which I will often write as \\(P_{XY}(x,y)\\)), distributes probability over the conjunctions of X and Y. So \\(P_{XY}(x=5,y=-1)\\) is the probability that the two die sum to 5 and die A is 1 smaller than die B (which only happens when A = 2, B=3, which has a probability of (1/7)(1/7)=1/49). # now we will do the same &quot;group_by and sum probabilities&quot; procedure, but will group by all combinations of X and Y rv.XY = joint.outcomes %&gt;% group_by(X,Y) %&gt;% # group by value of X and Y summarize(p = sum(p.conjunction)) # sum up the probability of all outcomes that have that sum head(rv.XY) ## # A tibble: 6 x 3 ## # Groups: X [3] ## X Y p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0 0.0816 ## 2 3 -1 0.0408 ## 3 3 1 0.0408 ## 4 4 -2 0.0408 ## 5 4 0 0.0204 ## 6 4 2 0.0408 rv.XY %&gt;% ggplot(aes(x=X, y=Y, fill=p))+ geom_tile() + scale_x_continuous(breaks = 2:12)+ scale_y_continuous(breaks = -6:6) Since random variables are partitions over the sample space, we can use the law of total probability to calculate the marginal distribution of X from a joint distribution of X and Y by summing (or integrating for continuously valued variables) over all values of Y: \\(P_X(x) = \\sum\\limits_{y \\in Y}P_{XY}(x,y)\\) rv.XY %&gt;% group_by(X) %&gt;% summarize(p = sum(p)) ## # A tibble: 11 x 2 ## X p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.0816 ## 2 3 0.0816 ## 3 4 0.102 ## 4 5 0.122 ## 5 6 0.143 ## 6 7 0.163 ## 7 8 0.102 ## 8 9 0.0816 ## 9 10 0.0612 ## 10 11 0.0408 ## 11 12 0.0204 It is possible for two sets of variables to have very different joint distributions while having the same marginal distributions. Consider two scenarios with identical marginal distributions: We roll two fair dice, A and B, and these dice are independent. The marginal distributions of \\(P(A)\\) and \\(P(B)\\) are uniform over the integers 1 through 6 (assigning each one 1/6 probability). The joint distribution \\(P(A,B)\\) is uniform over all 36 conjunctions (assigning each one 1/36 probability), consequently the conditional distributions show independence in that they are equal to the marginal distributions: \\(P(A|B)=P(A)\\) and \\(P(B|A)=P(B)\\). We roll two magical magnetic dice, A and B, which always come up equal, but are equally likely to come up as any integer. The marginal distributions \\(P(A)\\) and \\(P(B)\\) are the same as in (1), they are uniform over the integers 1 through 6. However, the joint distribution \\(P(A,B)\\) is not uniform: it assigns 0 probability to every conjunction where \\(A \\neq B\\), and the six outcomes where \\(A=B\\) (\\(\\{(1,1), (2,2), (3,3), (4,4), (5,5), (6,6)\\}\\)) each have probability 1/6. In this scenario, the variables are not independent, and the conditional distributions are not equal to the marginal distributions: \\(P(A|B=3)\\) is not uniform over the integers 1 through 6, but assigns 100% probability to \\(A=3\\) and 0 probability to all other outcomes. Bernoulli distribution Let’s consider a single flip of a bent coin. If it comes up heads, we will say random variable \\(y=1\\), if it is tails, \\(y=0\\). The coin is bent, and comes up heads with probability \\(p\\). Using the various rules of probability, we can show that: \\(P(y=1) = p\\) \\(P(y=0) = (1-p)\\) If we write down a function of \\(y\\) that returns these probabilities when we give it a particular value of y, we have written down the probability distribution function of y. Usually we would write this as \\(f(y)\\). In this case, \\(f(y)\\) is the “Bernoulli” distribution, typically written as: \\(f(Y) = \\operatorname{Bernoulli}(y|p) = \\{p \\mbox{ if } y = 0 \\mbox{ and } (1-p) \\mbox{ if } y=0 \\}\\) This trivial example shows that the probability distribution function of a random variable is obtained by using the rules of probability. Rather than writing out a long description about how coin flips determine the value of \\(y\\), it is much more convenient to simply write out the probability distribution function of \\(y\\), as we did above. Moreover, since the Bernoulli distribution is widely known, we could just refer to it by name, and informed people will know that it means the function spelled out above. \\(f(y \\mid p) = \\operatorname{Bernoulli}(p)\\) It is even more convenient to write this out in “sampling” notation: \\(y \\sim \\operatorname{Bernoulli}(p)\\) This basically says “random variable y is a random draw from a Bernoulli distribution with parameter p”; in other words, random variable y takes on values with probability given by Bernoulli(p). Combinations and the binomial distribution If we flip a coin that comes up heads with probability \\(p=0.8\\), and tails with probability \\((1-p)=0.2\\), the probability of getting HHT after three independent flips is (under the justifiable assumption that the different flips are independent): \\(P(H)P(H)P(T) = p \\cdot p \\cdot (1-p) = 0.8 \\cdot 0.8 \\cdot 0.2 = 0.128\\). What is the probability of getting 2 heads out of 3 flips? To answer this question, we must sum up all the different ways in which we can get 2 out of 3 heads: HHT, HTH, THH: \\(P(2\\mbox{ H out of } 3) = P(\\mbox{HHT}) + P(\\mbox{HTH}) + P(\\mbox{THH})\\) We should be able to convince ourselves that: \\(P(\\mbox{HHT}) = P(\\mbox{HTH}) = P(\\mbox{THH}) = p^2 \\cdot (1-p)\\). In general, we see that the probability of any particular sequence with n heads (and therefore, necessarily, 3-n tails) will be \\(p^n * (1-p)^{3-n}\\). So \\(P(2\\mbox{ H out of } 3) = (3)\\cdot p^2 \\cdot (1-p)\\), which we can partition into the constant \\((3)\\), which indicates how many 3-flip outcomes result in two heads, and the probability of any one such outcome occurring. Now, let’s say define \\(x\\) to be a random variable equal to the number of heads in our sequence of 3 coin flips. \\(x\\) can have 4 values (0, 1, 2, 3). The probability of \\(x\\) taking on any particular value is the probability of the disjunction of the outcomes (sequences) that have that number of heads. \\(P(x=0) = P(\\mbox{TTT})\\) \\(P(x=1) = P(\\mbox{HTH} \\lor \\mbox{THT} \\lor \\mbox{TTH})\\) \\(P(x=2) = P(\\mbox{HHT} \\lor \\mbox{HTH} \\lor \\mbox{THH})\\) \\(P(x=3) = P(\\mbox{HHH})\\) And we can use our disjunction rule to calculate the probabilities (with the understanding that the unique sequences are disjoint; i.e., \\(P(\\mbox{HTH} \\land \\mbox{HHT})=0\\)). \\(P(x=0) = P(\\mbox{TTT}) = (1-p)^3\\) \\(P(x=1) = P(\\mbox{HTH} \\lor \\mbox{THT} \\lor \\mbox{TTH}) = P(\\mbox{HTH}) + P(\\mbox{THT}) + P(\\mbox{TTH}) = 3 * p * (1-p)^2\\) \\(P(x=2) = P(\\mbox{HHT} \\lor \\mbox{HTH} \\lor \\mbox{THH}) = P(\\mbox{HHT}) + P(\\mbox{HTH}) + P(\\mbox{THH}) = 3 * p^2 * (1-p)^1\\) \\(P(x=3) = P(\\mbox{HHH}) = p^3\\) In general, if we define \\(x\\) to be the number of coins that were heads out of a sequence of \\(n\\) flips, each coming up heads with probability \\(p\\), the probability that \\(x=k\\) will be given by \\(\\mbox{(number of unique sequences that give k out of n heads)} * p^k * (1-p)^{n-k}\\) We can enumerate all the ways in which we would get \\(k\\) of \\(n\\), and count, but let’s try to come up with a faster, general scheme for doing so. The number of unique sequences that have \\(k\\) out of \\(n\\) heads is given by the binomial coefficient. This is generally written as \\({n \\choose k}\\), which reads as “n choose k,” and describes the number of different, but order invariant, combinations of \\(k\\) objects selected from a set of \\(n\\). \\({n \\choose k} = \\frac{n!}{k!(n-k)!}\\) (The exclamation mark is the factorial operation: \\(x! = x \\cdot (x-1) \\cdot (x-2) \\cdot ... \\cdot (1)\\).) So we can write a general expression for the probability that \\(x\\) will take on a particular value \\(k\\): \\(P(x \\mid p,n) = f(x \\mid p,n) = {n \\choose k}p^k(1-p)^{n-k}\\) This is the binomial distribution, which distributes probability over \\(k\\) – the number of successes – given two parameters: \\(n\\) (the number of attempts), and \\(p\\) (the probability of a success on any one attempt). Since this distribution is widely known, we would typically abbreviate it as \\(P(x \\mid p,n) = \\operatorname{Binomial}(p,n)\\) or in sampling notation \\(x \\sim \\operatorname{Binomial}(p,n)\\). Distribution functions: PDF, CDF, Quantile tl;dr; d* gives the probability mass/density, (e.g., dnorm) p* gives the cumulative probability, (e.g., pnorm) q* gives the quantile (inverse cdf) (e.g., qnorm) Random variables are defined by their probability distributions which describe the probability with which that variable will take on any of its possible values. Random variables may be categorized by which values they can take on: discrete random variables take on only a countable number of values (e.g., integers: 1, 2, 3, 4), while continuous random variables take on an uncountable number of values (e.g., rational numbers: 1, 1.01, 1.001, 1.0001, …). Probability distribution (mass and density) functions (p.d.f.) Discrete random variables have probability mass functions which assign some amount of probability to each possible value: \\(P(X=x) = f_X(x)\\). In R these are prefixed with d, as in dbinom. In contrast, Continuous random variables have infinitely many possible values, and the probability associated with each one of them is effectively nil. Consequently, they have probability density functions (PDF) which describe the density of probability at each value. Consequently, probability can only be defined for a possible interval of values, which is calculated by integrating the probability density within that interval \\(P(a &lt; X &lt; b) = \\int_a^b f_X(x) dx\\). In R these are also prefixed with d, as in dnorm. x = seq(-3,3,by=0.001) library(ggplot2) ggplot(data.frame(x=x, dens=dnorm(x,0,1)), aes(x,dens))+geom_line()+ggtitle(&#39;Probability density function of standard normal&#39;) For discrete variables, each candidate value has some non-zero probability assigned to it; hence we talk about the probability “mass” at each value. However, continuous variables do not take on a countable number of alternatives. For instance, the interval \\([0,1]\\) has uncountably many alternatives: 0.8, 0.83, 0.836, 0.8362, … etc. If we distribute probability over each of these alternatives, each one will get infinitesimally little probability mass (0). So, it doesn’t make sense to talk about the distribution of probability mass over these alternatives. Instead, we talk about the density of probability at a given point. Actual probability values are obtained by integrating the density over some interval. (Usually, it is most convenient to deal with the cumulative distribution of a continuous variable.) Cumulative distribution functions (c.d.f.) The cumulative distribution function (CDF) at \\(x\\) gives the probability that the random variable is less than or equal to \\(x\\): \\(F_X(x) = P(X \\leq x)\\), calculated as the sum of the probability mass function (for discrete variables) or integral of the probability density function (for continuous variables) from \\(-\\infty\\) to \\(x\\). For discrete: \\(F_X(x) = \\sum_{x&#39; \\leq x} f_X(x&#39;)\\). For continuous: \\(F_X(x) = \\int_{-\\infty}^x f_X(t)dt\\). In R CDFs are prefixed with p, as in pnorm. x = seq(-3,3,by=0.001) ggplot(data.frame(x=x, cumulative=pnorm(x,0,1)), aes(x,cumulative))+geom_line()+ggtitle(&#39;Cumulative distribution function of standard normal&#39;) For instance, IQ is normally distributed with mean 100 and standard deviation 15. The cumulative distribution of IQ at 100 is 0.50, because half of the IQ scores are less than or equal to 100, by definition. We can assess the cumulative distribution of such normal variables with pnorm(), for instance, the cumulative probability at IQ=120 is given by pnorm(120,100,15)=0.9087888. This tells us that 120 is the 90.878878th percentile of IQ. In other words the percentile of \\(x\\) is \\(100\\cdot P(X \\leq x) \\%\\), or \\(100 \\cdot F_X(x) \\%\\). Slight technicality for discrete random variables By our basic rules of probability, we should see that \\(P(X \\geq x) = 1 - P(X &lt; x)\\) (since all X values that are not greater or equal to x must be smaller than x). For continuous random variables, we can take a bit of a shortcut, and calculate \\(P(X \\geq x) = 1 - P(X \\leq x)\\), because for continuous variables \\(P(X=x)\\) is infinitesimally small, so the fact that we are counting \\(X=x\\) (rather than counting only \\(X&lt;x\\)) makes no difference, since exactly x has zero probability. Consequently, we can calculate the probability that IQ will be greater or equal to 120 as 1-pnorm(120,100,15). In contrast, for discrete variables, \\(P(X = x)\\) has some non-zero probability, so we cannot simply calculate \\(P(X \\geq x)\\) as \\(1-P(X \\leq x)\\). For instance, if we consider the number of coins that come up heads out of 10 fair flips (which is given by the binomial(10,0.5) distribution). The probability of getting 7 or more heads can be calculated as sum(dbinom(seq(7,10),10,0.5))=0.171875. However, we get the wrong answer if we try to calculate it as 1-pbinom(7,10,0.5) = 0.0546875. Instead, we must calculate it as \\(P(X \\geq x) = 1-P(X &lt; x)\\): 1-pbinom(6,10,0.5) = 0.171875. Put another way: \\(P(X \\leq 7)=0.95\\) and \\(P(X \\geq 7)=0.17\\) sum to more than 1.0 because they double-count \\(P(X=7)=0.12\\). \\(P(X &lt; 7)=P(X \\leq 6)=0.83\\) and \\(P(X \\geq 7)=0.17\\) do sum to 1.0. In short: be careful when calculating the upper cumulative distribution tail of discrete distributions. Quantile functions (inverse CDF). If we want to know what IQ score one would need to have to be in the 95th percentile, we want to find the IQ score such that the cumulative probability at that score is 0.95. These sorts of questions ask about the inverse cumulative distribution function, or the quantile function. In R we can calculate this with q* functions. For instance, to get the 95th percentile IQ: qnorm(0.95, 100, 15) = 124.6728044. We generally write the quantile function as \\(F^{-1}_X(q)\\), and this quantile function is defined in terms of the cumulative probability: \\(F^{-1}_X(q) = x \\mbox{ iff } F_X(x)=q\\). Thus the q* (quantile) and p* (cumulative probability) functions are inverses of each other: pnorm(qnorm(0.8, 100, 15), 100, 15)=0.8 and qnorm(pnorm(75, 100, 15), 100, 15)=75 q = seq(0.001,0.999,by=0.001) ggplot(data.frame(cumulative=q, x=qnorm(q,0,1)), aes(cumulative,x))+geom_line()+ggtitle(&#39;Quantile function of standard normal&#39;) Special named quantiles. Quantiles place points evenly along the c.d.f., effectively dividing the c.d.f. into even intervals. The \\(q\\)-quantiles tesselate the full c.d.f. range (0 to 1) in intervals of \\(1/q\\). The kth q-quantile refers to the value of \\(x\\) such that \\(F_X(x)=k/q\\). For instance, a 4-quantile (usually called a quartile) places four points along the c.d.f., the first at 0.25, the second at 0.5, the third at 0.75, the fourth at 1.0. There are names for various common quantile splits. ‘quartiles’ (\\(q=4\\)), ‘quintiles’ (\\(q=5\\)), ‘deciles’ (\\(q=10\\)), ‘percentiles’ (\\(q=100\\)). You will see these special names in the literature, but we will tend to just refer to quantiles by their corresponding probability. While a given quantile typically refers to the exact \\(x\\) value, it is often used to refer to the interval below that point. For instance, I might say that I scored in the “bottom” quartile, meaning that my score was somewhere between the lowest score and the score that corresponds to the first quartile (my percentile is somewhere between 0 and 25). Or that I was in the third quartile (my percentile was somewhere between 50 and 75), etc. Quantiles for discrete variables. data.frame(x=0:10, cdf=pbinom(0:10, 10, 0.5)) ## x cdf ## 1 0 0.0009765625 ## 2 1 0.0107421875 ## 3 2 0.0546875000 ## 4 3 0.1718750000 ## 5 4 0.3769531250 ## 6 5 0.6230468750 ## 7 6 0.8281250000 ## 8 7 0.9453125000 ## 9 8 0.9892578125 ## 10 9 0.9990234375 ## 11 10 1.0000000000 For discrete variables, it may be the case that a particular quantile cannot be specified exactly . For instance, the 0.5th quantile in the distribution of coin flips above falls somewhere between 5 and 6. There are a number of ways to interpolate between 5 and 6 to produce an estimate of what that quantile “really” is. Generally if you need to find a given quantile and you can’t specify it exactly, be conservative (i.e., pick the number that is less favorable to whatever it is you are trying to do). Expectation and moments Let’s consider some discrete random variable, X: ( rv.X &lt;- tibble::tibble(x = 1:10, p.x = c(0.05, 0.05, 0.1, 0.1, 0.2, 0.25, 0.1, 0.05, 0.05, 0.05)) ) ## # A tibble: 10 x 2 ## x p.x ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.05 ## 2 2 0.05 ## 3 3 0.1 ## 4 4 0.1 ## 5 5 0.2 ## 6 6 0.25 ## 7 7 0.1 ## 8 8 0.05 ## 9 9 0.05 ## 10 10 0.05 The expected value, EV, or expectation (\\(\\mathbb{E}\\left[\\cdot\\right]\\)) of some random variable \\(X\\) is the probability-weighted average of the values. For discrete random variables this is calculated as the probability-weighted sum of the values: \\(\\mathbb{E}\\left[X\\right] = \\sum\\limits_{x}x P(x)\\). For continuous random variables as the probability-weighted ingral of the values: \\(\\mathbb{E}\\left[X\\right] = \\int\\limits_{-\\infty}^{\\infty}x f_X(x)dx\\). The mean of a random variable is just its expected value: \\(\\mu_X = \\operatorname{Mean}\\left[X\\right] = \\mathbb{E}\\left[X\\right]\\). So, our variable \\(X\\) has an expected value / mean of 5.4: \\(\\operatorname{Mean}\\left[X\\right] = \\mathbb{E}[X] = 1*0.05 + 2*0.05 + 3*0.1 + 4*0.1 + 5 *0.2 + 6*0.25 + 7*0.1 + 8*0.05 + 9*0.05 + 10*0.05 = 5.4\\). ( mu.X &lt;- sum(rv.X$x * rv.X$p.x) ) ## [1] 5.4 More generally, we can calculate the expected value of some function of a random variable \\(g(X)\\) in the same way: e.g., \\(\\mathbb{E}\\left[g(X)\\right] = \\int\\limits_{-\\infty}^{\\infty}g(x)f_X(x)dx\\). (or a sum for discrete random variables) The variance of a random variable is the expected value of the squared distance of \\(x\\) from its mean: \\(\\sigma_X^2 = \\operatorname{Var}\\left[X\\right] = \\mathbb{E}\\left[(X-\\mu_X)^2\\right]\\). Sometimes it is useful to consider the equivalence: \\(\\mathbb{E}\\left[(X-\\mu_X)^2\\right] = \\mathbb{E}\\left[X^2\\right]-\\mu_X^2\\). ( var.X &lt;- sum((rv.X$x - mu.X)^2 * rv.X$p.x) ) ## [1] 4.74 The standard deviation is the square root of the variance: \\(\\sigma_X = \\sqrt{\\sigma_X^2}\\). ( sigma.X &lt;- sqrt(var.X) ) ## [1] 2.177154 Standardizing a random variable means subtracting the mean and dividing by the standard deviation. This is often called “z-scoring,” so we will refer to it with the function \\(\\operatorname{z}_X(x) = \\frac{x-\\mu_X}{\\sigma_X}\\). ( rv.X &lt;- dplyr::mutate(rv.X, z.x = (x-mu.X)/sigma.X) ) ## # A tibble: 10 x 3 ## x p.x z.x ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.05 -2.02 ## 2 2 0.05 -1.56 ## 3 3 0.1 -1.10 ## 4 4 0.1 -0.643 ## 5 5 0.2 -0.184 ## 6 6 0.25 0.276 ## 7 7 0.1 0.735 ## 8 8 0.05 1.19 ## 9 9 0.05 1.65 ## 10 10 0.05 2.11 The skewness of a random variable is the expected value of the cubed standardized score of \\(X\\) (\\(\\frac{X-\\mu_X}{\\sigma_X}^3\\)). \\(\\gamma_X = \\operatorname{Skew}\\left[X\\right] = \\mathbb{E}\\left[\\operatorname{z}_X(x)^3\\right] = \\mathbb{E}\\left[\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^3\\right]\\). ( skewness.X &lt;- sum(rv.X$z.x^3 * rv.X$p.x) ) ## [1] 0.06279246 The kurtosis of a random variable is the expected value of the standardized score of \\(X\\) raised to the 4th power (\\(\\mathbb{E}\\left[\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^4\\right]\\)). Typically, however, we consider the excess kurtosis, which subtracts 3 making it \\(0\\) for the Normal distribution. So we define \\(\\kappa_X = \\operatorname{Kurt}\\left[X\\right] = \\mathbb{E}\\left[\\operatorname{z}_X(x)^4\\right]-3 = \\mathbb{E}\\left[\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^4\\right]-3\\). ( ex.kurtosis.X &lt;- sum(rv.X$z.x^4 * rv.X$p.x)-3 ) ## [1] -0.2009827 The \\(n\\)th moment of a random variable is the expectations of a function of \\(X\\) of the form \\(g_n(x) = \\left(\\frac{x-a}{b}\\right)^n\\). Typically, “first moment” refers to the mean: the first raw moment where \\(g_1(x) = x\\); “second moment” refers to the variance: the second central moment where \\(g_2(x)=(x-\\mu_X)^2\\); “third/fourth moment” refer to skewness/kurtosis: the third/fourth standardized moments where \\(g_3(x)=\\operatorname{z}_X(x)^3\\), and \\(g_4(X)=\\operatorname{z}_X(x)^4\\). There are a few useful properties about how the Mean, Variance, Skewness, and Excess Kurtosis behave under various operations: Adding a constant \\(\\operatorname{Mean}\\left[X+a\\right]=\\operatorname{Mean}\\left[X\\right]+a\\) \\(\\operatorname{Var}\\left[X+a\\right]=\\operatorname{Var}\\left[X\\right]\\) \\(\\operatorname{Skew}\\left[X+a\\right]=\\operatorname{Skew}\\left[X\\right]\\) \\(\\operatorname{Kurt}\\left[X+a\\right]=\\operatorname{Kurt}\\left[X\\right]\\). Multiplying by a constant \\(\\operatorname{Mean}\\left[a X\\right]=a \\operatorname{Mean}\\left[X\\right]\\) \\(\\operatorname{Var}\\left[a X\\right]=a^2 \\operatorname{Var}\\left[X\\right]\\) \\(\\operatorname{Skew}\\left[a X\\right]=\\operatorname{sgn}(a) \\operatorname{Skew}\\left[X\\right]\\), where \\(\\operatorname{sgn}(\\cdot)\\) is the sign function \\(\\operatorname{Kurt}\\left[a X\\right]=\\operatorname{Kurt}\\left[X\\right]\\). Adding an independent random variable \\(Y\\): \\(\\operatorname{Mean}\\left[X+Y\\right]=\\operatorname{Mean}\\left[X\\right]+\\operatorname{Mean}\\left[Y\\right]\\) \\(\\operatorname{Var}\\left[X+Y\\right]=\\operatorname{Var}\\left[X\\right] + \\operatorname{Var}\\left[Y\\right]\\) \\(\\operatorname{Skew}\\left[X+Y\\right]=\\frac{\\operatorname{Skew}\\left[X\\right] + \\operatorname{Skew}\\left[Y\\right]}{(\\sigma_X^2 + \\sigma_Y^2)^{3/2}}\\) \\(\\operatorname{Kurt}\\left[X+Y\\right]=\\frac{\\sigma_X^4 \\operatorname{Kurt}\\left[X\\right]+\\sigma_Y^4 \\operatorname{Kurt}\\left[Y\\right]}{(\\sigma_X^2 + \\sigma_Y^2)^2}\\). It’s also useful to know what happens to the variance of X+Y when X and Y are not independent: \\(\\textbf{Var}[X+Y] = \\textbf{Var}[X] + \\textbf{Var}[Y] + 2*\\textbf{Cov}[X,Y]\\), (where \\(\\textbf{Cov}[X,Y]\\) denotes the covariance of X and Y; which is 0 for independent random variables) Sum of \\(n\\) independent random variables \\({X_1, ..., X_n}\\) all identically distributed as \\(X\\): \\(\\operatorname{Mean}\\left[\\sum_{i=1}^n X_i\\right]=n \\operatorname{Mean}\\left[X\\right]\\) \\(\\operatorname{Var}\\left[\\sum_{i=1}^n X_i\\right]=n \\operatorname{Var}\\left[X\\right]\\) \\(\\operatorname{Skew}\\left[\\sum_{i=1}^n X_i\\right]=\\frac{1}{\\sqrt{n}}\\operatorname{Skew}\\left[X\\right]\\) \\(\\operatorname{Kurt}\\left[\\sum_{i=1}^n X_i\\right]=\\frac{1}{n^2}\\operatorname{Kurt}\\left[X\\right]\\). note that these properties of the sum of independent, identically distributed random variables are symptoms of the central limit theorem in action. Central limit theorem and the normal distribution The central limit theorem shows that the mean of many independent (and roughly identically) distributed random variables will be (roughly) normally distributed. We are not going to prove this, but we can get a sense for it. Here we take the mean of 1, 2, 4, 8, … 256 samples from distributions with very different shapes (chosen to have the same mean and standard deviation, for graphical convenience), and plot a histogram of those sample means. As we see, when the sample size increases, they all converge to a normal-looking distribution of sample means. samplers= list() samplers[[&#39;beta&#39;]] = function(n){(rbeta(n, 0.2, 0.2)-0.5)/0.423} samplers[[&#39;unif&#39;]] = function(n){(runif(n, 0, 1)-0.5)/0.29} samplers[[&#39;exp&#39;]] = function(n){(rexp(n, 2)-0.5)/0.5} ns = c(1, 2, 4, 8, 16, 32, 64, 128, 256) k =10000 df = data.frame() for(dist in names(samplers)){ for(n in ns){ df = rbind(df, data.frame(dist=dist, n=n, mean=replicate(k, mean(samplers[[dist]](n))))) } } library(ggplot2) ggplot(df, aes(x=mean, color=dist, fill=dist))+ facet_wrap(~n, scales =&quot;free&quot;)+ geom_density(alpha=0.2) A similar intuition can be obtained from our handy expectation identities. The mean of \\(n\\) independent random variables \\({X_1, ..., X_n}\\) all identically distributed as \\(X\\): will have the following properties: \\(\\operatorname{Mean}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]= \\operatorname{Mean}\\left[X\\right]\\) \\(\\operatorname{SD}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]=\\frac{1}{\\sqrt{n}} \\operatorname{SD}\\left[X\\right]\\) \\(\\operatorname{Skew}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]=\\frac{1}{\\sqrt{n}}\\operatorname{Skew}\\left[X\\right]\\) \\(\\operatorname{Kurt}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]=\\frac{1}{n^2}\\operatorname{Kurt}\\left[X\\right]\\). What this tells us is that as \\(n\\) (the number of variables that go into our mean) increases, the mean stays the same, the standard deviation decreases with \\(\\sqrt{n}\\), the skew decreases with \\(n\\) (towards 0 – the skew of a normal), and the (excess) kurtosis decreases with \\(n^2\\) (towards 0 – the kurtosis of a normal). Thus, the various higher-order parameters of the shape decrease toward the values of the Normal as sample size increases. All in all: as sample size goes up, the distribution of the arithmetic mean goes to a Normal (provided the original distributions have finite variance). Requirements for CLT to hold A few things must hold for the central limit theorem to apply to specific measurements. First, for the CLT to hold, the measurement has to arise from the sum of lots of little, roughly homogenous, perturbations. For instance, lots of little genetic and environmental factors contribute additively to height, and (within gender) the distribution of heights is very close to being Normal. However, this is not true of everything. Many natural properties are not the sum of little perturbations, but rather the product. For instance, if you consider the worth of your variable-rate savings account, every time period it grows by some variable percentage (your interest rate). Your account balance is not the sum of all the interest rates, but the product: it grows exponentially (geometrically), not additively, and therefore it will not follow a Normal distribution. Lots of processes in finance, nature, etc. are actually exponential growth processes, and they end up being distributed not as a Normal distribution, but as a Log-Normal. (Note that the product of many perturbations is equivalent to the sum of the logarithms of those perturbations – hence log-normal. Note also that one way that Benford’s law might arise is from such random exponential growth processes.) Second, for the CLT to hold, the measurement has to arise from the sum of lots of little roughly homogenous perturbations. If we look at height within gender, no one factor contributes a disproportionately large amount of the variance; so the CLT holds and the distribution of within-gender heights is Normal. However, if we look at the distribution of heights across genders, we see that one factor contributes a huge amount of the variance: the gender of the person – men are quite a bit taller than women on average. Consequently, the distribution of heights across genders is not normally distributed, because that distribution is dominated by the gender variance. (One could describe the across-gender height distribution as a mixture model of two Normal distributions – we will talk about mixture models much later). When we take the sample mean of a bunch of measurements, both of the properties above always hold; thus the central limit theorem always applies to the sample mean, so many of our statistical procedures (which assume a Gaussian distribution of the sample mean) will work in many situations. However, another word of caution: while the central limit theorem always applies for the sample mean, it does not guarantee that \\(n\\) will be large enough for the mean to really be sufficiently close to Normal. In the previous section we described how quickly the Skew and Kurtosis decline to 0, but sometimes the starting Skew is very large indeed, and even a reasonably large \\(n\\) does not yield a normal distribution. The log-normal distribution, for instance, can have a very large skew, and in those cases, rarely is \\(n\\) large enough for the sample mean to end up Normal. In those cases, it is often advisable to transform the data to get rid of some of the skew. (We will talk about these sort of transformations and link functions later). Caveats aside, the CLT applies often enough that most conventional statistical procedures are based on it. We will spend a great deal of time on Z-tests, t-tests, linear regression, ANOVA, etc, all of which assume either that the errors, or that the sample means, are normally distributed, and this assumption is often valid because of the CLT. Normal distribution Since in so many cases the mean from a sufficiently large sample will be normally distributed, much of classical statistics relies on this asymptotic normality of sample means. x = seq(-5,5,by=0.01) ggplot(data.frame(x=x, dens=dnorm(x,0,1)), aes(x=x,y=dens))+geom_area() In R, we would get the probability density function of a normal with dnorm, its cumulative probability function with pnorm, and the quantile function with qnorm. Properties of normal distribution Let’s say \\(X \\sim \\operatorname{Normal}(\\mu_x, \\sigma_x)\\). For instance, \\(X\\) may be IQ scores from the population, in which case \\(X \\sim \\operatorname{Normal}(100,15)\\). The basic properties of a Normal distribution include: the mean, median, and mode are all the same the mean (\\(\\mu\\)) is the location parameter which slides the distribution along the X axis the standard deviation (\\(\\sigma\\)) is the scale/dispersion parameter. The basic rules of expectations apply to the means and standard deviations of transformations of Normal variables: If \\(X \\sim \\operatorname{Normal}(\\mu_x, \\sigma_x)\\), the following identities hold: if \\(Y = a+X\\), then \\(Y \\sim \\operatorname{Normal}(\\mu_x+a, \\sigma_x)\\) if \\(Y = aX\\), then \\(Y \\sim \\operatorname{Normal}(a \\mu_x, a \\sigma_x)\\) so if \\(Y = aX+b\\), then \\(Y \\sim \\operatorname{Normal}(a \\mu_x+b, a \\sigma_x)\\) From these rules, we can easily see that if \\(X \\sim \\operatorname{Normal}(\\mu, \\sigma)\\), and we define Z as \\(Z = (X-\\mu)/\\sigma\\), then \\(Z \\sim \\operatorname{Normal}(0,1)\\). If this is not obvious, convince yourself using the rules above that this will be true. This is the very common, and very useful “z-transformation,” which will take any normally distributed variable and convert it to have a “standard normal distribution” (which just means it is a normal distribution with mean = 0, and standard deviation = 1). "],["NHST.html", "Foundations of Statistics Frequentist statistics via simulation Null hypothesis significance testing. Sampling distributions Statistics via the Normal distribution Relationship between confidence intervals and null hypothesis tests. Null hypothesis significance testing with Normal From probability to statistics with the Binomial t-distribution", " Foundations of Statistics Frequentist statistics via simulation The logic of null hypothesis testing is based on the sampling distribution of our test statistic under the null hypothesis. I.e., what test statistics do we expect to see if our sample came from some null model? While in general we will use various analytical expressions for these sampling distributions, it may be clearer to generate them ourselves by sampling, to see what exactly we are doing. Null hypothesis significance testing. Let’s use a coin flipping example to illustrate this logic. We have some data. Here: a sequence of coin flips that are either heads (H) or tails (T). (data = c(&#39;H&#39;, &#39;H&#39;, &#39;H&#39;, &#39;T&#39;, &#39;T&#39;, &#39;H&#39;, &#39;T&#39;, &#39;H&#39;, &#39;H&#39;, &#39;H&#39;)) ## [1] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; We define some statistic on our data. Here: the number of heads. statistic = function(data){sum(ifelse(data==&#39;H&#39;, 1, 0))} We calculate this statistic on our data. (our.stat = statistic(data)) ## [1] 7 We define a null hypothesis: a generative model of our data that we want to reject. Here: flips of a fair coin. sample.from.null = function(n){sample(c(&#39;H&#39;, &#39;T&#39;), n, replace=TRUE)} We repeat many times the process of (a) generating some data under the null and (b) calculating the statistic many. This gives us the “sampling distribution of our statistic under the null hypothesis” null.samples = replicate(10000, statistic(sample.from.null(length(data)))) str(null.samples) ## num [1:10000] 4 6 5 5 5 5 6 1 4 5 ... We compare our statistic to the null samples to see what fraction of them are at least as extreme as the one we saw. Here we define extremeness as “too many heads” (so its a one tailed test). library(ggplot2, quietly = T) ggplot(data.frame(statistic = null.samples, more.extreme=ifelse(null.samples&gt;=our.stat, &quot;at least as extreme&quot;, &quot;less extreme&quot;)), aes(x=statistic, fill=more.extreme))+ geom_bar()+ scale_x_continuous(breaks=0:10) # calculate p-value based on null distribution. (p.value = sum(null.samples&gt;=our.stat)/length(null.samples)) ## [1] 0.1774 The details of this procedure will vary depending on our null hypothesis and statistic in question. Sometimes we know enough about the null hypothesis to literally generate new data. Sometimes, we only know what we think should be invariant under the null hypothesis, and we do some sort of permutation/randomization of the data to generate null samples. Nonetheless, the procedure and logic are roughly the same. Critical values, alpha, power Let’s say that we are going to run our coin-flipping experiment on a coin that we suspect is bent. We will: (1) flip the coin 10 times (2) calculate the number of heads, and (3) “reject the null hypothesis” (of a fair coin) if the number of heads is surprisingly high. What’s the probability that we will reject the null? To answer this question we need to decide a few things, and make some assumptions: what constitutes ‘surprisingly high?’ For now, let’s just say that we will declare 8 or more heads to be sufficiently “surprising” to reject the null. We will call this criterion the ‘critical value.’ critical.value = 8 Exactly how bent do we think the coin is? Does it come up heads 65% of the time? 70%? 100%? Obviously, the more bent we think the coin is, the more ‘surprising’ the outcomes we would expect to see from it. Let’s say we think our bent coin comes up heads 75% of the time. Setting up the “Alternate hypothesis” We have a way of sampling from the null (via sample.from.null), but now we need a way to sample possible outcomes we might see from the truly bent coin. sample.from.alternate = function(n){sample(c(&#39;H&#39;, &#39;T&#39;), n, prob=c(0.75, 0.25), replace=TRUE)} If we sample from the alternative many times, and calculate our statistic for each sample, we get samples from the distribution of statistics that we expect to see from the bent coin we hypothesized. alternate.samples = replicate(10000, statistic(sample.from.alternate(length(data)))) str(alternate.samples) ## num [1:10000] 9 7 9 10 10 8 8 9 8 8 ... Figuring out “power” Power is the probability that we will reject the null hypothesis for a sample taken from the “alternate” hypothesis. In our case, it just means: what proportion of statistics we simulated from the alternate hypothesis are going to be at least as big as the ‘critical value’ we chose? ( power = mean(alternate.samples &gt;= critical.value) ) ## [1] 0.5337 So there’s our answer: that’s the probability that we would reject the null in an experiment that flipped 10 times a bent coin that comes up heads with probability 0.75, given our critical value of 8. Note that to figure out power, we have to make some assumption about what the not-null alternative is. Without that, we have no way to figure out what samples from the alternate hypothesis would look like, and what fraction of them we would reject the null for. Figuring out “alpha” What’s the probability that we would reject the null hypothesis if it turned out that we were flipping a fair coin after all? I.e., what’s the ‘false positive rate,’ how often would we reject the null, even though the null was true? ( alpha = mean(null.samples &gt;= critical.value) ) ## [1] 0.0578 Showing alpha, power library(dplyr, quietly = T) all.data &lt;- rbind(data.frame(n.heads = null.samples, sampled.from = &#39;null&#39;), data.frame(n.heads = alternate.samples, sampled.from = &#39;alternate&#39;)) %&gt;% mutate(null.rejected = ifelse(n.heads &gt;= critical.value, &#39;reject null&#39;, &#39;retain null&#39;), label = paste0(&#39;sampled from &#39;, sampled.from, &quot; and &quot; , null.rejected)) ggplot(all.data, aes(x=n.heads, fill=label))+ facet_grid(sampled.from~.)+ geom_bar(position=&#39;identity&#39;, alpha=1)+ scale_x_continuous(breaks = 0:10)+ scale_fill_manual(values = c(&#39;#009900&#39;, &#39;#CC8888&#39;, &#39;#990000&#39;, &#39;#88CC88&#39;))+ geom_vline(xintercept = critical.value-0.5, color=&#39;red&#39;, size=1) The top panel shows the distribution of samples from the null hypothesis (a fair coin), the bottom panel shows samples from the alternate hypothesis (a bent coin that comes up heads with probability 0.75). Dark green corresponds to the samples from the bent coin for which we would reject the fair-coin null. These are ‘correct rejections of the null,’ or ‘hits,’ and the probability that this happens for samples from the alternate hypothesis is called “power.” Light green are samples from the null (fair coin) for which we would not reject the null. These are also correct, but they don’t have a common name. Dark red are samples from the null (fair coin) for which we do reject the null. These are often called false positives, or Type I errors, and the probability that this happens for samples from the null hypothesis is called alpha. Light red are samples from the alternate (bent coin) for which we do not reject the null. Thus they too are a mistake, often called ‘false negatives’ or Type II errors. The probability of this happening for samples from the alternate is 1 minus “power.” Figuring out the critical value. Above, we sort of just made up a critical value by saying that 8 or more heads out of 10 would be sufficiently surprising to reject. In general, we aren’t just going to make up a critical value, but we will instead pick a particular rate of false positives (Type I errors) that we are willing to tolerate. Thus, we will pick an alpha that we can be satisfied with (i.e., we will be content if we falsely reject the null hypothesis for this fraction of samples from the null hypothesis). So, let’s say that we will tolerate an alpha of 10%, so we want to find the maximum critical value, such that the proportion of samples from the null that would exceed it is 10% or less. null.distribution &lt;- data.frame(x = null.samples) %&gt;% group_by(x) %&gt;% count() %&gt;% ungroup() %&gt;% arrange(desc(x)) %&gt;% mutate(prob = n/sum(n)) %&gt;% mutate(prob.x.or.more = cumsum(prob)) head(null.distribution, 11) ## # A tibble: 11 x 4 ## x n prob prob.x.or.more ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 8 0.0008 0.0008 ## 2 9 112 0.0112 0.012 ## 3 8 458 0.0458 0.0578 ## 4 7 1196 0.120 0.177 ## 5 6 2127 0.213 0.390 ## 6 5 2358 0.236 0.626 ## 7 4 2000 0.2 0.826 ## 8 3 1194 0.119 0.945 ## 9 2 443 0.0443 0.990 ## 10 1 95 0.0095 0.999 ## 11 0 9 0.0009 1 So the probability of getting 10/10 heads is about 0.001 under the null, of getting 9 or 10 heads is about 0.01, of getting 8 9 or 10 heads is a bit over 0.05, and the probability of getting 7,8,9, or 10 heads is about 0.17. So we would use 8 as our critical value, as we would expect to see 8 or more heads from the null hypothesis fewer than 10% of the time. Sampling distributions TL; DR. Frequentist statistics are based on the distribution of statistics we might expect to see if we were to run the experiment many times. These are called ‘sampling distributions.’ The most common one is the “sampling distribution of the sample mean,” which is Normal (assuming the central limit theorem holds), has a mean equal to the mean of the underlying population, and has a standard deviation equal to the standard deviation of the underlying population divided by the square root of n (the sample size). This standard deviation of the sampling distribution of the sample mean is often called the “standard error of the mean.” Logic Our data are a sample. If we were to rerun the experiment, we would get a different sample. The sampling distribution of something (technically, of a random variable) is the probability distribution describing the probability that this random variable will take on any possible value when we sample it. For instance, let’s consider a measurement of a sampled person’s IQ. Although we actually got some measurement, from the perspective of frequentist statistics, we must consider what other measurements we could have seen – so we say that the sampling distribution of a measurement of IQ is Normal with a mean of 100 and a standard deviation of 15 (this is how IQ is defined). Just as it makes sense to talk about sampling distributions for measurements, or sets of measurements, it also makes sense to consider sampling distributions for statistics. For instance, we got a particular sample of 10 people’s IQs, and calculated the sample mean. Even though we saw one particular sample mean, we must consider what other sample means we could have seen (and their probability distribution) from carrying out our procedure of sampling 10 people and averaging their IQs (this is the sampling distribution of the sample mean). We can mathematically derive the probability distributions for sampling distributions of various statistics by relying on the statistical model we assume underlies our data. library(ggplot2) # here is a sampling function that generates a single sample iq. sample.iq = function(){round(rnorm(1,100,15))} # we can run it once to get one sample sample.iq() ## [1] 97 # or 10 times to generate a sample of 10 iqs: replicate(10, sample.iq()) ## [1] 100 131 111 112 87 118 94 85 127 95 # or 1000 times to get a sample of 1000 iqs iqs.1000 = replicate(1000, sample.iq()) ggplot(data.frame(iq = iqs.1000), aes(x=iq))+geom_bar() # we can generalize this to write a function that generates a sample of n iqs sample.iqs = function(n){replicate(n, sample.iq())} # Here is one possible sample mean of 10 sampled iqs mean(sample.iqs(10)) ## [1] 88.2 # We can generate 5 means of samples of 10 iqs: replicate(5, mean(sample.iqs(10))) ## [1] 97.1 99.8 99.9 98.5 100.3 # or a sample of 1000 sample means of 10 sampled iqs iq.means.1000 = replicate(1000, mean(sample.iqs(10))) ggplot(data.frame(mean.iq = iq.means.1000), aes(x=mean.iq))+geom_bar() # we can write a function that samples n iqs, and returns their mean: sample.iq.mean = function(n){mean(sample.iqs(n))} # now we can see how this Sampling Distribution of the Sample Mean changes as we change the sample size df = rbind(data.frame(n=4, mean.iq = replicate(1000, sample.iq.mean(4))), data.frame(n=16, mean.iq = replicate(1000, sample.iq.mean(16))), data.frame(n=64, mean.iq = replicate(1000, sample.iq.mean(64))), data.frame(n=256, mean.iq = replicate(1000, sample.iq.mean(256)))) ggplot(df, aes(x=mean.iq, color=as.factor(n), fill=as.factor(n)))+geom_density(alpha=0.2) What this is designed to illustrate is that if we take a step back from the data we actually have and consider the data we could have had, we see that many different samples are possible, and many different sample means are possible. Which sample means are more or less likely depends on the size of our sample, and the sampling distribution of the individual data points. The resulting probability distribution of sample means we could have seen is the “sampling distribution of the sample mean.” Such sampling distributions exist for every statistic we could conjure up (sample standard deviation, sample kurtosis, etc.). Expectation about the sampling distribution of the sample mean. Formally, we assume that a sample of size \\(n\\) corresponds to \\(n\\) random variables independently and identically distributed according to the sampling distribution of the data: \\(\\{x_1, ... x_n\\} \\sim P(X)\\) We can think of them all as different (independent) instantiations of the same random variable \\(X\\) – the random variable of a single data point. We might not know the details of the probability distribution of \\(X\\), but we assume that it has some defined mean and variance (here we use these to refer to the properties of the random variable, obtained by expectation, not the sample): \\(\\mu_X = \\operatorname{Mean}\\left[X\\right] = \\mathbb{E}[X] = \\int\\limits_{x \\in X} x P(X=x) dx\\) \\(\\sigma_X^2 = \\operatorname{Var}\\left[X\\right] = \\mathbb{E}\\left[{(X-\\mu_X)^2}\\right] = \\int\\limits_{x \\in X} (x-\\mu_X)^2 P(X=x) dx\\) The probability distribution of \\(X\\), which we might call the sampling distribution of a single data point, will also have some skewness, kurtosis, and higher order moments describing its shape. The mean of \\(n\\) data points is defined as: \\(\\bar x^{(n)} = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i\\), (here we superscript x-bar with \\((n)\\) to make it explicit that this is the mean of a sample of size \\(n\\)). What can we say about the sampling distribution of this sample mean? That is, what do we know about \\(P(\\bar x^{(n)})\\)? The central limit theorem tells us that if \\(n\\) is large enough, skewness, kurtosis, and higher order moments will all shrink towards their values under a normal distribution. So if \\(n\\) is large enough: \\(P(\\bar x^{(n)}) = \\operatorname{Normal}(\\operatorname{Mean}[\\bar x^{(n)}], \\sqrt{\\operatorname{Var}[\\bar x^{(n)}]})\\) In other words, if \\(n\\) is large enough the sampling distribution of the sample mean will be approximately normal, with some mean and variance. What will the mean and variance of this distribution be? To figure this out, we need to remember a few useful expectation identities: For the sum of \\(n\\) independent random variables \\({X_1, ..., X_n}\\) all identically distributed as \\(X\\): \\(\\operatorname{Mean}\\left[\\sum_{i=1}^n X_i\\right]=n \\operatorname{Mean}\\left[X\\right]\\) \\(\\operatorname{Var}\\left[\\sum_{i=1}^n X_i\\right]=n \\operatorname{Var}\\left[X\\right]\\) For the outcome of multiplying by a constant \\(a\\): \\(\\operatorname{Mean}\\left[a X\\right]=a \\operatorname{Mean}\\left[X\\right]\\) \\(\\operatorname{Var}\\left[a X\\right]=a^2 \\operatorname{Var}\\left[X\\right]\\) From this we can figure out the mean of the sampling distribution of the sample mean. \\(\\operatorname{Mean}[\\bar x^{(n)}] = \\operatorname{Mean}\\left[\\frac{1}{n}\\sum\\limits_{i=1}^n x_i\\right]\\) Note that for clarity, we can break this up a bit by defining an intermediate variable: the sum of \\(n\\) samples: \\(U^{(n)}\\) \\(\\bar x^{(n)} = \\frac{1}{n} U^{(n)}\\), where \\(U^{(n)} = \\sum\\limits_{i=1}^n x_i\\) Now what can we say about the mean and variance of \\(U^{(n)}\\)? From our expectation rules about the sums of \\(n\\) iid variables, we get: \\(\\operatorname{Mean}[U^{(n)}] = \\operatorname{Mean}\\left[\\sum\\limits_{i=1}^n x_i\\right] = n*\\operatorname{Mean}[X] = n*\\mu_X\\) \\(\\operatorname{Var}[U^{(n)}] = \\operatorname{Var}\\left[\\sum\\limits_{i=1}^n x_i\\right] = n*\\operatorname{Var}[X] = n*\\sigma_X^2\\) And using our expectation rules about multiplying a random variable by a constant we get: \\(\\operatorname{Mean}[\\bar x^{(n)}] = \\operatorname{Mean}\\left[\\frac{1}{n} U^{(n)}\\right] = \\frac{1}{n} \\operatorname{Mean}[U^{(n)}] = \\frac{1}{n}*n*\\mu_X = \\mu_X\\) \\(\\operatorname{Var}[\\bar x^{(n)}] = \\operatorname{Var}\\left[\\frac{1}{n} U^{(n)}\\right] = \\left(\\frac{1}{n}\\right)^2 \\operatorname{Var}[U^{(n)}] = \\left(\\frac{1}{n}\\right)^2*n*\\sigma_X^2 = \\frac{1}{n} \\sigma_X^2\\) So, we learned that mean and variance of the sampling distribution of the sample mean are given by: \\(\\operatorname{Mean}[\\bar x^{(n)}] = \\mu_X\\) \\(\\operatorname{Var}[\\bar x^{(n)}] = \\frac{1}{n} \\sigma_X^2\\) From our calculation of the variance of the sampling distribution of the sample mean, we can get the standard deviation: \\(\\operatorname{SD}[\\bar x^{(n)}] = \\sqrt{\\frac{1}{n} \\sigma_X^2} = \\frac{\\sigma_X}{\\sqrt{n}}\\) Let’s see if all this hard work paid off by checking our answer with some simulations: n = 20 mu.x = 100 sigma.x = 15 mu.xbar = mu.x sigma.xbar = sigma.x/sqrt(n) # so we expect the mean and standard deviation of the sample mean (xbar) to be: c(mu.xbar, sigma.xbar) ## [1] 100.000000 3.354102 # let&#39;s generate a lot of simulated sample means, and see if they have the right mean and sd sample.n = function(n){rnorm(n, mu.x, sigma.x)} sample.xbar.n = function(n){mean(sample.n(n))} sampled.xbars = replicate(10000, sample.xbar.n(n)) # do our sampled sample means have the mean and sd we predict? c(mean(sampled.xbars), sd(sampled.xbars)) ## [1] 99.964182 3.382544 Great, these are spot on, modulo some sampling variability (if we wanted to be really, really sure, we could increase the number of sample means we sample). So now we have learned something about the sampling distribution of the sample mean: \\(\\bar x^{(n)} \\sim \\operatorname{Normal}\\left({\\mu_X, \\frac{\\sigma_X}{\\sqrt{n}}}\\right)\\) This calculation about the sampling distribution of the sample mean is the basis of many statistical procedures. Standard error (of the sample mean) Often we are interested in how our sample mean (\\(\\bar x^{(n)}\\)) differs from the population mean (\\(\\mu_X\\)). What can we say about the distribution of the error of our sample mean: \\(\\bar x^{(n)} - \\mu_X\\)? Well, using our expectation rules for adding a constant, we can see that: \\(\\operatorname{Mean}\\left[{(\\bar x^{(n)}-\\mu_X)}\\right] = 0\\), and \\(\\operatorname{SD}\\left[{(\\bar x^{(n)}-\\mu_X)}\\right] = \\frac{\\sigma_X}{\\sqrt{n}}\\). Since the shape of the distribution will not change, we can say that our error is normally distributed: \\((\\bar x^{(n)}-\\mu_X) \\sim \\operatorname{Normal}\\left({0, \\frac{\\sigma_X}{\\sqrt{n}}}\\right)\\) The fact that the sampling distribution of the error of our sample mean has a mean of 0 means that the arithmetic mean is an unbiased estimate of the population mean. The standard deviation of the sampling distribution of the error of the sample mean is called the standard error of the sample mean. In general, for any estimator, the standard deviation of the sampling distribution of the error of that estimator is called the standard error (we will see such standard errors for slopes in regression, linear contrasts, etc.). Usually we will denote the standard error \\(s_{\\cdot}\\) with a subscript, or \\(\\operatorname{se}\\{\\cdot\\}\\) with brackets: \\(\\operatorname{se}\\{\\bar x^{(n)}\\} = s_{\\bar x^{(n)}} = \\frac{\\sigma_X}{\\sqrt{n}}\\) (Technically, we would be more correct to call this \\(\\sigma_{\\bar x^{(n)}}\\), since its a standard error obtained from the population standard deviation, rather than the sample standard deviation, but let’s gloss over that for now.) Statistics via the Normal distribution Based on the central limit theorem and our derivation of the properties of the sampling distribution of the sample mean, we can undertake some classical (frequentist) statistics. When we calculate the mean of \\(n\\) independent samples from a population with mean \\(\\mu_X\\) and standard deviation \\(\\sigma_X\\), the “sampling distribution of the sample mean” will follow roughly a Normal distribution, centered on the population mean, and with the standard deviation reduced by a factor of \\(\\sqrt{n}\\): \\(\\bar x^{(n)} \\sim \\operatorname{Normal}\\left({\\mu_X, \\frac{\\sigma_X}{\\sqrt{n}}}\\right)\\) If we calculate instead the sampling distribution of the error between the sample and population means, we get: \\((\\bar x^{(n)}-\\mu_X) \\sim \\operatorname{Normal}\\left({0, \\frac{\\sigma_X}{\\sqrt{n}}}\\right)\\) We use the convenient phrase standard error of the sample mean to refer to the standard deviation of the sampling distribution of the sample mean (and also the standard deviation of the sampling distribution of the error – the deviation of the sample mean from the population mean). This standard error of the sample mean (or more accurately, its estimate) is something that you will often see as error bars in graphs (and the axis label or figure caption will say something like “mean \\(\\pm\\) s.e.m.”). (Normal) Null hypothesis significance testing (NHST) Let’s say we go back in time to carefully study the case of Phineas Gage. We compose a battery to measure emotional decision-making, assemble a large number of normal/healthy patients, and administer this battery to all of them. We find that the emotional-decision making scores are distributed in the healthy population as a Normal distribution with mean=50, sd=5. We then measure Phineas Gage, and find he has a score of 39. The NHST approach postulates a null hypothesis (H0): a statistical model of our data if the effect we care about does not exist in the world. In this case, our null model might be described as “Gage’s emotional decision-making score is a random sample from the distribution of those scores in the normal population”: \\(x_{\\text{Gage}} \\sim \\operatorname{Normal}(50, 5)\\). We would then calculate a “test statistic” on our data. Let’s start by just using Gage’s score as our test statistic: \\(x_{\\text{Gage}}\\). From the null model, we can obtain the “sampling distribution of the test statistic under the null hypothesis,” in this case, it is just the distribution of emotional decision-making scores in the healthy population: \\(\\operatorname{Normal}(50, 5)\\) Now we want to assess whether the observed test statistic was sufficiently extreme compared to its null hypothesis distribution. In general this is done by assessing whether a statistic at least as extreme as the one we saw will occur with a probability smaller than \\(\\alpha\\) under the null hypothesis. The value of \\(\\alpha\\) indicates how often we are willing to falsely reject the null hypothesis (that is, reject the null hypothesis when our observation came from the null hypothesis); generally, folks use \\(\\alpha=0.05\\), meaning we are content to falsely reject the null 1 out of 20 times. We can do this in several ways: (1) define “critical” (cut-off) values which correspond to the \\(\\alpha\\) value. (2) compare our observed test statistic directly to the null distribution to obtain a p-value and see if it is less than alpha. Let’s work through such a Z-test to see if Gage’s score was significantly lower (a one-tailed test) compared to the distribution of population scores. gage = 39 H0.mu = 50 H0.sd = 5 alpha = 0.05 # Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha crit.score = qnorm(alpha, H0.mu, H0.sd) # plot things. library(ggplot2) xs = seq(20,70,by=0.1) ggplot(data.frame(x = xs, dens = dnorm(xs, H0.mu, H0.sd), reject=ifelse(xs &lt;= crit.score, &quot;reject H0&quot;, &quot;do not reject&quot;)), aes(x=x, y=dens, fill=reject))+geom_ribbon(aes(ymin=0, ymax=dens))+ geom_vline(xintercept=gage, size=2, color=&quot;blue&quot;)+ geom_vline(xintercept=crit.score, size=1, color=&quot;black&quot;) This plot shows Gage’s score (blue line), the “critical score” such that P(score \\(\\leq\\) crit.score | H0)=\\(\\alpha\\) (black line), and the null hypothesis distribution colored based on whether or not it is above or below the critical value. The area under the curve below the critical value is equal to \\(\\alpha\\) (in this case 0.05). The area under the curve at smaller values than Gage’s score (not shaded specially) corresponds to the “p-value.” It should be clear that if Gage’s score is more extreme than (in this case, below) the critical value, then the p-value will be smaller than \\(\\alpha\\); thus whether we choose to compare our test statistic to the critical value, or the p-value directly to \\(\\alpha\\), we will get the same answer. # see if gage&#39;s score is lower than the critical score: gage &lt; crit.score ## [1] TRUE # calculte p-value for gage&#39;s score by evaluating P(score &lt;= gage | H0) p.value = pnorm(gage, H0.mu, H0.sd) p.value ## [1] 0.01390345 # compare p.value to alpha p.value &lt; alpha ## [1] TRUE Two-tailed tests So far we have done a “one-tailed” test, in the sense that we were only testing whether Gage’s score was really low, compared to the normal population. In general, you should favor two-tailed tests, which can reject the null hypothesis whether the score is too extreme in either the positive or negative direction. You should favor two-tailed tests when they are possible, since there are few cases when you would actually ignore an extremely high score even though you expected an extremely low one (which is what a one-tailed test presumes). To run a two-tailed test on a Normal distribution we need to define two critical values (a positive and a negative one), such that P(score \\(\\leq\\) low.crit | H0) = \\(\\alpha/2\\) and P(score \\(\\geq\\) high.crit ) = \\(\\alpha/2\\). Note that we are “distributing” our \\(\\alpha\\) probability across both high and low tails, to maintain the same rate of falsely rejecting the null hypothesis. gage = 39 H0.mu = 50 H0.sd = 5 alpha = 0.05 # Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha low.crit = qnorm(alpha/2, H0.mu, H0.sd) high.crit = qnorm((1-alpha/2), H0.mu, H0.sd) # plot things. ggplot(data.frame(x = xs, dens = dnorm(xs, H0.mu, H0.sd), reject=ifelse(xs&lt;=low.crit, &quot;reject low&quot;, ifelse(xs&gt;=high.crit, &quot;reject high&quot;, &quot;do not reject&quot;))), aes(x=x, y=dens, fill=reject, group=reject))+geom_ribbon(aes(ymin=0, ymax=dens))+ geom_vline(xintercept=gage, size=2, color=&quot;blue&quot;)+ geom_vline(xintercept=low.crit, size=1, color=&quot;black&quot;)+ geom_vline(xintercept=high.crit, size=1, color=&quot;black&quot;) This plot shows Gage’s score (blue line), the high and low critical scores (black lines), and the null hypothesis distribution colored based on whether or not it would be rejected (by being either below the low critical score, or above the high critical score). The total area under the curve past the critical values is equal to \\(\\alpha\\). # see if gage&#39;s score is lower than the low critical score, or higher than the high critical score. gage &lt;= low.crit | gage &gt;= high.crit ## [1] TRUE # calculte p-value for gage&#39;s score by taking the minimum of the lower and upper tails, and multiplying by 2 (to get symmetric 2-tailed p-value) p.value.low = pnorm(gage, H0.mu, H0.sd) p.value.high = 1-pnorm(gage, H0.mu, H0.sd) p.value = 2*min(p.value.low, p.value.high) p.value ## [1] 0.0278069 # compare p.value to alpha p.value &lt;= alpha ## [1] TRUE Note that for the two tailed test, we see if the score is more extreme than either the low or high critical value, and we calculate a p-value by taking the minimum of the lower and upper tail probabilities, and multiplying by 2 (because of the symmetry of the Normal). So what is this p-value that we’ve been calculating? It is the probability that a score sampled from the null hypothesis will be at least as as “extreme” as the one we observed. When we calculate a one-tailed test, “at least as extreme” corresponds to extremeness in the direction of the tail we are testing. When it is a two-tailed test, we need to figure out what the corresponding “extreme” score at the other tail would be (for a Normal, this is easy, as they are symmetric, so we can just multiply by 2). What a p-value is not the probability that the null hypothesis is true (this requires the Bayesian calculation of P(H0 | data), rather than integrating over P(data | H0), as we have done). Normal tests with sample means Imagine that instead of having one Phineas Gage, there was an epidemic of exploding, scull-piercing tamping irons all piercing the frontal lobes of many railroad workers. You can administer your test to 5 such unlucky “Gages,” and you want to compare their average to the population. To do this, you will need to calculate their mean, and you will need to calculate the sampling distribution of the sample mean under the null hypothesis (that these individuals’ scores are samples from the overall population). gages = c(39, 44, 38, 40, 51) H0.mu = 50 H0.sd = 5 alpha = 0.05 # Calculate sample statistics: mean, and n n = length(gages) x.bar = mean(gages) # Calculate mean and sd of sampling distribution of the sample mean under null H0.xbar.mu = H0.mu H0.xbar.sd = H0.sd/sqrt(n) # Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha low.crit = qnorm(alpha/2, H0.xbar.mu, H0.xbar.sd) high.crit = qnorm((1-alpha/2), H0.xbar.mu, H0.xbar.sd) # plot things. ggplot(data.frame(x = xs, dens = dnorm(xs, H0.xbar.mu, H0.xbar.sd), reject=ifelse(xs&lt;=low.crit, &quot;reject low&quot;, ifelse(xs&gt;=high.crit, &quot;reject high&quot;, &quot;do not reject&quot;))), aes(x=x, y=dens, fill=reject, group=reject))+geom_ribbon(aes(ymin=0, ymax=dens))+ geom_vline(xintercept=x.bar, size=2, color=&quot;blue&quot;)+ geom_vline(xintercept=low.crit, size=1, color=&quot;black&quot;)+ geom_vline(xintercept=high.crit, size=1, color=&quot;black&quot;) Note that here we used our derived sampling distribution for the sample mean, consequently, the null distribution is skinnier. Otherwise, all the other calculations of p-values, critical statistic values, etc. is exactly the same: # see if mean score of gages is lower than the low critical score, or higher than the high critical score. x.bar &lt;= low.crit | x.bar &gt;= high.crit ## [1] TRUE # calculte p-value for mean score of gages by taking the minimum of the lower and upper tails, and multiplying by 2 (to get symmetric 2-tailed p-value) p.value.low = pnorm(x.bar, H0.xbar.mu, H0.xbar.sd) p.value.high = 1-pnorm(x.bar, H0.xbar.mu, H0.xbar.sd) p.value = 2*min(p.value.low, p.value.high) p.value ## [1] 0.0006767642 # compare p.value to alpha p.value &lt;= alpha ## [1] TRUE Z-scores and Z-tests As we see, we now have enough machinery in place to do null hypothesis tests with the normal distribution. However, it is useful to introduce the notion of a z-score, as most classical instruction in statistics use such “standardized” statistics (rather than the raw scores and means as we have). If \\(x \\sim \\operatorname{Normal}(\\mu_x, \\sigma_X)\\), and we apply a linear transformation to obtain a new variable \\(z = (x-\\mu_X)/\\sigma_X\\) we have calculated a z-score. Following our rules about expectations, we can show that this z-score will have a standard normal distribution, meaning it will be distributed with a mean of 0 and a standard deviation of 1: \\(z \\sim \\operatorname{Normal}(0,1)\\) Because such standardization is so easy to do, and yields the same standard Normal distribution whenever we apply it, it serves as the backbone of most classical statistical methods (which were developed at a time when calculating cumulative probabilities for arbitrary distributions was hard, and by standardizing our procedure, we could simply calculate the cumulative probability and quantile tables for just the standard distribution). It is critical to note that whenever we calculate a z-score, we do so relative to some distribution. We can z-score someone’s height relative to the IQ distribution (e.g., \\((69-100)/15\\)), but that would be weird and useless. One warning sign that this is weird and useless is to consider the units of the resulting number. Normally, z-scores are unitless: they take a measurement in one unit (say inches), subtracts some mean in those units (yielding a difference in inches), and divides by the standard deviation of such measurements (also in inches), and thus we get a number that has no units. However, if we take height in inches, and divide by the standard deviation of IQ, we get a number in units of inches/IQ – such “unit analyses” are a good way to catch ourselves doing something incoherent. Generally, we want to z-score some value relative to the (presumed) sampling distribution of that value. If we have an IQ score, we z-score it to the assumed sampling distribution of IQ. If we have the mean of 10 IQ scores, we z-score that sample mean to the sampling distribution of the sample mean of 10 IQ scores. Such z-scores will serve as our “test statistics” for Z-tests, and also will be used to estimate Normal confidence intervals. Z-tests To run the “Z-test” to assess whether the mean of our 5 Gages’ scores was sufficiently different from the null hypothesis distribution of the mean of 5 regular person scores we would first Z-transform it: subtract the mean of the sampling distribution of the sample mean, and divide by the standard deviation of the sampling distribution of the sample mean: \\(Z_{\\bar x} = (\\bar x - \\mu_{\\bar x})/\\sigma_{\\bar x}\\) \\(Z_{\\bar x} = (42.4 - 50)/2.236 = -3.4\\) We can now do the same null hypothesis calculation procedures we carried out earlier, but for the sampling distribution of the (appropriate) z-score: gages = c(39, 44, 38, 40, 51) H0.mu = 50 H0.sd = 5 alpha = 0.05 # Calculate sample statistics: mean, and n n = length(gages) x.bar = mean(gages) # Calculate mean and sd of sampling distribution of the sample mean under null H0.xbar.mu = H0.mu H0.xbar.sd = H0.sd/sqrt(n) Z.xbar = (x.bar - H0.xbar.mu)/H0.xbar.sd H0.z.mu = 0 # we worked this out earlier: z scores ~ Normal(0,1) H0.z.sd = 1 # Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha low.crit = qnorm(alpha/2, H0.z.mu, H0.z.sd) high.crit = qnorm((1-alpha/2), H0.z.mu, H0.z.sd) # plot things. zs = seq(-5,5,by=0.01) ggplot(data.frame(z = zs, dens = dnorm(zs, H0.z.mu, H0.z.sd), reject=ifelse(zs&lt;=low.crit, &quot;reject low&quot;, ifelse(zs&gt;=high.crit, &quot;reject high&quot;, &quot;do not reject&quot;))), aes(x=z, y=dens, fill=reject, group=reject))+geom_ribbon(aes(ymin=0, ymax=dens))+ geom_vline(xintercept=Z.xbar, size=2, color=&quot;blue&quot;)+ geom_vline(xintercept=low.crit, size=1, color=&quot;black&quot;)+ geom_vline(xintercept=high.crit, size=1, color=&quot;black&quot;) Since here we are using the sampling distribution of the z-score, the null distribution is Normal with mean 0 and standard deviation of 1. Otherwise, all the other calculations of p-values, critical statistic values, etc. is exactly the same, but we use the z-score of the sample mean as a test statistic: # see if z-score of mean score of gages is lower than the low critical z-score, or higher than the high critical z-score. Z.xbar &lt;= low.crit | x.bar &gt;= high.crit ## [1] TRUE # calculte p-value for z-score of mean score of gages by taking the minimum of the lower and upper tails, and multiplying by 2 (to get symmetric 2-tailed p-value) p.value.low = pnorm(Z.xbar, H0.z.mu, H0.z.sd) p.value.high = 1-pnorm(Z.xbar, H0.z.mu, H0.z.sd) p.value = 2*min(p.value.low, p.value.high) p.value ## [1] 0.0006767642 # compare p.value to alpha p.value &lt;= alpha ## [1] TRUE Note that if we do this z-transformation, and consider a two-tailed test, we can take a number of shortcuts: Because the z-score distribution is symmetric around 0, the low.crit score is the high.crit score multiplied by negative 1, so we can consider just the absolute critical score. Similarly, we need only consider the absolute value of our calculated z-scores to calculate two-tailed p-values. Moreover, we need not refer to the sampling distribution mean and standard deviation explicitly, since all the *norm functions in R assume that the default is the standard normal (z-score) distribution with mean=0 and sd=1. # Check for significance by comparing absolute z score to critical z-score abs.crit = abs(qnorm(alpha/2)) abs(Z.xbar) &gt;= abs.crit ## [1] TRUE # Calculate (2-tailed) p-value using absolute z-score p.value = 2*(1-pnorm(abs(Z.xbar))) p.value ## [1] 0.0006767642 # Check for significance by comparing p.value to alpha p.value &lt;= alpha ## [1] TRUE Hopefully, it is clear at this point that these procedures are all doing the same thing, just with slight mathematical transformations that make some things less transparent, but other things more convenient. They all yield the same answer. (Normal) Confidence intervals on the sample mean Recall that the sampling distribution of the deviation of the sample mean from the population mean – the sampling distribution of the error of our sample mean – is given by: \\((\\bar x^{(n)}-\\mu_X) \\sim \\operatorname{Normal}\\left({0, \\sigma_{\\bar x^{(n)}}}\\right)\\) It is helpful to calculate this as a z-score: \\(\\frac{\\bar x^{(n)}-\\mu_X}{\\sigma_{\\bar x^{(n)}}} \\sim \\operatorname{Normal}\\left({0, 1}\\right)\\) So the difference between our sample mean, and the population mean, in units of standard errors of the mean, will have a standard Normal distribution. So if we want to define an interval around our sample mean, such that an interval defined this way will contain the true population mean 95% of the time, we can do so by finding a z-score interval that contains 95% of the z-scores, and then transforming these z-scores back into the units of sample means. One z-score range that will include 95% of the means can be constructed based on the z-score such that 2.5% of the z-scores are smaller than it, and the z-score such that 2.5% are larger than it (thus 95% of z-scores are between them): low.z.crit = qnorm(0.025, 0, 1) high.z.crit = qnorm(1-0.025, 0, 1) c(low.z.crit, high.z.crit) ## [1] -1.959964 1.959964 We can convert these z-scores back to their original units by multiplying by the standard deviation, and adding the mean (reversing the calculation that yields z scores for specific x values): \\(x = z*\\sigma + \\mu\\) x.bar = mean(gages) sd.xbar = H0.sd/sqrt(length(gages)) xbar.range = c(low.z.crit, high.z.crit)*sd.xbar + x.bar xbar.range ## [1] 38.01739 46.78261 Here we called it “sd.xbar” because it is the standard deviation of the sampling distribution of the sample mean, but typically we will just refer to it as the standard error of the mean, or “se.xbar.” Because the z-score distribution is symmetric, and by convention we chose to define a symmetric confidence interval, the low and high z scores are symmetric, and we typically calculate just the absolute value: q = 0.95 # desired confidence interval z.crit = abs(qnorm((1-q)/2, 0, 1)) xbar.range = x.bar + c(-1, 1)*z.crit*sd.xbar In short, we can define the (z-score) confidence interval on the sample mean as: \\[\\bar x \\pm Z_{\\alpha/2}^*\\sigma_{\\bar x}\\] Where \\(\\alpha = 1-q\\), and \\(q\\) is the confidence interval percentile; \\(Z_{\\alpha/2}^*=\\) abs(qnorm(alpha/2,0,1)). So for a 99% confidence interval, \\(q=0.99\\), \\(\\alpha=0.01\\), and \\(Z_{\\alpha/2}^*=\\) abs(qnorm(0.01/2,0,1)) = 2.5758293. Relationship between confidence intervals and null hypothesis tests. The use of \\(\\alpha\\) as the critical value in a null hypothesis test, and as the interim calculation in confidence intervals is no accident. We declare a (2-tailed) z-test as significant when the p-value is lower than \\(\\alpha\\), in other words, when (the absolute value of) the difference between the sample mean and the null mean, in units of standard errors of the mean, is greater than \\(Z_{\\alpha/2}^*\\): \\(\\lvert\\frac{\\bar x - \\mu_X^{H0}}{\\sigma_{\\bar x}}\\rvert \\geq Z_{\\alpha/2}^*\\) With a bit of algebra, we can show that this means that we declare something as significant when: \\(\\mu_X^{H0} \\geq \\bar x + \\sigma_{\\bar x} Z_{\\alpha/2}^*\\) OR \\(\\mu_X^{H0} \\leq \\bar x - \\sigma_{\\bar x} Z_{\\alpha/2}^*\\) And as we recall, the limits of a q% confidence interval are given by: \\(\\bar x \\pm \\sigma_{\\bar x} Z_{\\alpha/2}^*\\) Thus, we see that if the null hypothesis mean does not fall in the \\((1-\\alpha)\\) confidence interval, then we can reject that null hypothesis mean with a two-tailed significance test with a Type I error rate of \\(\\alpha\\). So, checking whether the p-value for a null hypothesis z-test is less than \\(\\alpha=0.05\\) is equivalent to checking whether the null hypothesis mean falls outside of the 95% confidence interval. Special critical Z-values Everyone using statistics would benefit from knowing a few special Z-scores, since they make back-of-the-envelope calculations easy when you want to evaluate some results in a talk or a poster. \\(P(Z \\leq -1.96) = P(Z \\geq 1.96) = 0.025\\). In other words, 95% of the normal distribution is less than 1.96 standard deviations away from the mean. This means that a 95% confidence interval on a mean is \\(\\bar x \\pm 1.96 \\sigma_{\\bar x}\\). This also means that to pass a two-tailed Z-test with \\(\\alpha = 0.05\\), the sample mean has to be more than 1.96 standard errors away from the null hypothesis mean. \\(P(Z \\leq -0.6745) = P(Z \\geq 0.6745) = 0.25\\). 50% of the normal distribution is less than 0.67 standard deviations away from the mean; the first and third quartiles are the mean plus/minus 0.67 standard deviations. The interquartile range of a normal distribution will be 1.35 standard deviations. \\(P(Z \\leq -1.645) = P(Z \\geq 1.645) = 0.05\\). This defines the 90% confidence intervals, and correspond to the critical Z-value for a one-tailed test with \\(\\alpha = 0.05\\). \\(P(Z \\leq 1) = P(Z \\geq 1) = 0.15866\\). About 16% of a normal distribution is more than 1 standard deviation away from the mean in either direction, meaning that 68.3% of the normal distribution is less than 1 standard deviation away from the mean. These numbers can all be easily obtained via pnorm() and qnorm() in R, but often you might benefit from having them in your head. Rarity of Z-tests and Z- confidence intervals R doesn’t have a z-test function built in (although a few libraries offer one). This is because z-tests are so rarely done in practice, because carrying out a z-test requires that we know the population standard deviation. Consequently, when we reject the null hypothesis in a z-test, we reject the null of a particular population mean and and a particular population standard deviation. This is very rarely what we want, so we use t-tests instead (which assume that we estimate the standard deviation from the sample). Furthermore, using z-tests to define confidence intervals is even more rare, because when we define a confidence interval, we do not want to assume particular parameters of the population distribution (like its mean, and standard deviation). In the vast majority of cases, we will use the t-distribution, rather than the Normal Z-distribution for our null hypothesis tests and confidence intervals on the mean. What are these percents and probabilities? It is important to consider what these percents and probabilities are. This interpretation of confidence intervals and probabilities will be the same for every single confidence interval and p value we calculate. So we will keep reiterating it. So what is a p-value? We obtained the p value by calculating the probability with the following logic: we calculated the sampling distribution of the test statistic if we were to take many samples (of the same size as ours) from the null hypothesis population, calculate the test statistic on each of those samples, then look at the histogram of those samples. The proportion of those samples that are more extreme than the test-statistic we saw in our actual sample, is the p-value. Let’s do this explicitly: gages = c(39, 44, 38, 40, 51) H0.mu = 50 H0.sd = 5 # A function to calculate the z statistic for a sample mean given H0 mean and sd z.stat = function(sample){(mean(sample)-H0.mu)/(H0.sd/sqrt(length(sample)))} # our z statistic. (our.z.stat = z.stat(gages)) ## [1] -3.398823 # a function to sample n data points from the H0 distribution sample.from.H0 = function(n){rnorm(n,H0.mu,H0.sd)} # one sample of the same size as ours from the H0 distribution (one.H0.sample = sample.from.H0(length(gages))) ## [1] 51.38518 54.60044 50.49536 52.54444 51.82242 # the z-statistic for the H0 sample. (one.H0.z.stat = z.stat(one.H0.sample)) ## [1] 0.9702613 # sample lots of z statistics from the H0 distribution many.H0.z.stats = replicate(10000, z.stat(sample.from.H0(length(gages)))) # show a histogram of these H0-sampled z statistics. ggplot(data.frame(z = many.H0.z.stats, z.vs.ours=ifelse(abs(many.H0.z.stats)&gt;=abs(our.z.stat), &quot;more extreme&quot;, &quot;less extreme&quot;)), aes(x=z, fill=z.vs.ours))+ geom_histogram()+ geom_vline(xintercept=our.z.stat, color=&quot;blue&quot;) # our p value calculated by asking what fraction of H0-sampled z-statistics # are larger than ours (in absolute value) (p.value = sum(abs(many.H0.z.stats)&gt;=abs(our.z.stat))/length(many.H0.z.stats)) ## [1] 9e-04 Of course, when we do a z-test, we generally do this analytically, rather than numerically as we have here (by literally simulating a bunch of possible samples from the null hypothesis), which eliminates the need for sluggish computation. However, the logic of what we did is the same: we calculated the p-value as the proportion of samples from the null hypothesis that would be at least as extreme as the one we saw. So what does the p-value mean? It tells us what fraction of null hypothesis samples would be at least as extreme as ours, given our test statistic. It is a calculation based on \\(P(\\mbox{data} \\mid \\mbox{H0})\\). More generally, it tells us something about how this procedure is expected to behave when applied to the null hypothesis: if this procedure were applied to samples from some null hypothesis model, this is what we expect to see. Similarly, \\(\\alpha\\) tells us: if we reject the null based on this significance procedure, we expect the procedure to reject samples from the null hypothesis \\((100*\\alpha)\\)% of the time. What does the “percent” in a confidence interval mean? We calculated a confidence interval based on the sampling distribution of the error of the mean from the population mean. Let’s simulate this procedure by picking some random true population mean, true population sd, and some sample size. Then we simulate a sample from that population, and calculate a confidence interval from that sample. # a function to get the critical z value for a 100q% interval z.crit = function(q){abs(qnorm((1-q)/2))} # a function to calculate the standard error of the mean for a given # sample and null hypothesis sd. sem = function(x,H0.sd){H0.sd/sqrt(length(x))} # a function to get the mean and confidence interval (min, max) get.CI = function(x, q, H0.sd){ return( c(&quot;min&quot;=(mean(x)-z.crit(q)*sem(x,H0.sd)), &quot;mean&quot;=mean(x), &quot;max&quot;=(mean(x)+z.crit(q)*sem(x,H0.sd))) )} # a hypothetical true mean. true.mean = rnorm(1,0,5) # a hypothetical true standard deviation H0.sd = exp(rnorm(1,1,0.5)) # a hypothetical sample size: n = rgeom(1,0.2)+2 # a hypothetical sample of size n from this &quot;true distribution&quot; x = rnorm(n,true.mean,H0.sd) # the sample mean and 90% confidence interval for this sample (ci = get.CI(x, 0.9, H0.sd)) ## min mean max ## -18.16436 -15.66688 -13.16940 So this is one such randomly generated sample mean and resulting confidence interval. Now, we can ask whether the true population mean was contained within that confidence interval: # let&#39;s define a function to tell us whether the true mean is inside the confidence interval mean.in.ci = function(true.mean, ci){ ci[&#39;min&#39;] &lt;= true.mean &amp; true.mean &lt;= ci[&#39;max&#39;] } # is the true mean inside the confidence interval? (ignore the vector name &quot;min&quot; carryover) (mean.in.ci(true.mean, ci)) ## min ## TRUE So any one confidence interval either includes, or does not include, the true population mean. So what does the percent in a confidence interval mean? It’s a statement not about the current data, or the current population mean in question. It is a statement about the confidence interval procedure. Specifically, it tells us what fraction of all confidence intervals generated this way, in all experiments, will contain their respective population mean. Let’s simulate this. # Now let&#39;s consider many different true populations, samples from them, and the resulting CI results = data.frame() for(i in 1:100){ true.mean = rnorm(1,0,5) H0.sd = exp(rnorm(1,1,0.5)) n = rgeom(1,0.2)+2 x = rnorm(n,true.mean,H0.sd) ci = get.CI(x, 0.9, H0.sd) results = rbind(results, data.frame(&quot;experiment&quot; = i, &quot;mean&quot;=ci[&#39;mean&#39;], &quot;ci.min&quot;=ci[&#39;min&#39;], &quot;ci.max&quot;=ci[&#39;max&#39;], &quot;true.mean&quot;=true.mean, &quot;mean.in.ci&quot;=mean.in.ci(true.mean, ci))) } ggplot(results, aes(x=as.factor(experiment), y=mean, ymin=ci.min, ymax=ci.max, color=mean.in.ci))+ geom_pointrange()+ geom_point(aes(y=true.mean), size=2, color=&quot;black&quot;)+ coord_flip() Here, each horizontal line represents a particular experiment, with a particular true population mean (black dot), some H0 standard deviation, and some random sample. That random sample is used to define a mean and a 90% confidence interval (point+range). If the confidence interval contains the true mean, it is blue, otherwise it is red. So what fraction of these intervals contained the true population mean? # fraction of confidence intervals that include the true mean (sum(results$mean.in.ci)/nrow(results)) ## [1] 0.9 Note that the result of “how many of our 100 sampled intervals include their true mean” will be subject to sampling variation (binomial with n=100, p=q=0.9, here). If we increase the number of sampled experiments and intervals, we will be less likely to deviate much from 0.9. So, the percent in a confidence interval describes the probability that an interval constructed using this method will include the corresponding value (provided the distribution assumptions are met). All frequentist probabilities and percents have this sort of interpretation: they are statements about the procedure – how often will the procedure reject a sample from the null, how often will a confidence interval calculated in this way contain the true mean, etc. Null hypothesis significance testing with Normal In statistics via the Normal we covered the basic logic and application of the sampling distribution of the sample mean to the problem of testing a null hypothesis about the population. Null hypothesis testing follows this procedure: We have some structure we are interested in (the “effect”). We define a “statistic” to measure this structure. We define a “null” model of the data: a statistical model that generates data like ours, but lacking the effect we are interested in. We figure out the sampling distribution of our statistic under the null hypothesis. We compare the statistic value from our data, to its null hypothesis sampling distribution, to see if our statistic is sufficiently extreme under the null, for us to say that we “reject the null.” Type 1 error rate: alpha (\\(\\alpha\\)) To be more specific, in step 5 we calculate the probability that a statistic at least as extreme as ours would be obtained from samples from the null hypothesis – we call this the p-value. We decide on a significance level, usually called alpha (\\(\\alpha\\)): this corresponds to the largest p-value we are willing to declare significant (and thus reject the null). Consequently, the chosen alpha value (typically 0.05), corresponds to the probability that we would reject the null hypothesis for a sample from the null hypothesis. Thus, the alpha value corresponds to the rate at which we are willing to falsely reject the null hypothesis (reject it, when it is true); This is known as the rate of Type I error. We can get a sense for this via simulation. We will use the z-test statistic comparing a sample from the null to the null mean (and standard error). We will calculate its p-value, and see if it would be rejected. Since all of these samples are, by definition, sampled from the null, any sample which we declare significantly different form the null is a false rejection of the null: a type 1 error. H0.mean = 100 H0.sd = 15 alpha = 0.05 z.stat = function(x){(mean(x)-H0.mean)/(H0.sd/sqrt(length(x)))} p.value = function(z){2*pnorm(-abs(z))} is.significant = function(p.val, alpha){p.val &lt;= alpha} sample.from.null = function(n){rnorm(n,H0.mean,H0.sd)} df = data.frame(zs = replicate(1000, z.stat(sample.from.null(10)))) df$p.value = p.value(df$zs) df$significant = is.significant(df$p.value, alpha) library(ggplot2) ggplot(df, aes(x=zs, fill=significant))+geom_histogram() (type.I.error.rate = sum(df$significant)/nrow(df)) ## [1] 0.041 Of course, nothing about this simulation should reveal anything new, but perhaps it illustrates this basic point adequately well. The “alternate model” So far, we have only considered one statistical model: the null model with no effect. This is sufficient to obtain a p-value and test the null hypothesis. However, this only tells us the probability of rejecting (or not) the null hypothesis given data from the null hypothesis. It does not tell us what might happen if the null hypothesis is false. To calculate the probability of rejecting the null hypothesis when the null hypothesis is false (called power) we need a statistical model of the data in the case of a false null. This is the alternate hypothesis model. From this we can calculate power, as well as the Type II error rate (the probability of not rejecting the null hypothesis, when it is indeed false). To set up an alternate model, we will simply set up something like the null model, but with some key difference – an effect size. For simplicity, let’s say that our alternate model is a normal distribution with the same standard deviation, and with a mean that is 8 points higher than the null mean: H0.mean = 100 H0.sd = 15 H1.mean = 108 H1.sd = 15 alpha = 0.05 # we will still calculate the z score, and p-value *relative to the null hypothesis*! z.stat = function(x){(mean(x)-H0.mean)/(H0.sd/sqrt(length(x)))} p.value = function(z){2*pnorm(-abs(z))} is.significant = function(p.val, alpha){p.val &lt;= alpha} # Note: here we are using the H1 mean and sd! sample.from.alt = function(n){rnorm(n,H1.mean,H1.sd)} df = data.frame(zs = replicate(1000, z.stat(sample.from.alt(10)))) df$p.value = p.value(df$zs) df$significant = is.significant(df$p.value, alpha) library(ggplot2) ggplot(df, aes(x=zs, fill=significant))+geom_histogram() # Power: the probability that we will reject a sample from the alternate model. (power= sum(df$significant)/nrow(df)) ## [1] 0.362 # Type II error rate: the probability that we *fail* to reject the null for a sample from the alternate model (type.II.error.rate = (1-power)) ## [1] 0.638 # or alternatively: (type.II.error.rate = sum(df$significant==FALSE)/nrow(df)) ## [1] 0.638 Effect size The effect size is the magnitude of the deviation of the alternate model from the null model. While we can talk about the effect size in our case as the raw difference in means (100 for null, 108 for alternate, so 8 points), it is generally more convenient to talk about the effect in standardized units. This way, we get similar effect size estimates regardless of the units we are considering (e.g., centimeters, inches, etc). For comparing differences in means, we generally use “Cohen’s d”: the difference in means in units of standard deviation. (alternate mean minus null mean) divided by standard deviation: \\[d&#39; = (\\mu_X^{H_1} - \\mu_X^{H_0})/\\sigma_X\\] In our case, we said that the H1 mean was 108, the H0 mean was 100, and the standard deviation was 15, consequently the effect size is (108-100)/15 = 8/15. Calculating power from effect size With this definition, we can calculate power (using a normal z-test), simply by knowing the size of the sample, and the size of the true effect size (Cohen’s d). The power, or the probability that we will reject the null hypothesis, is the probability that a z-statistic obtained for a sample from the null hypothesis will exceed the critical z value. \\(P(\\mbox{significant} | H_1) = P(\\lvert z^{H_1} \\rvert \\geq \\lvert z^*_{\\alpha/2} \\rvert)\\) We must now go on a somewhat long-winded, algebraic exercise to calculate the sampling distribution of the z-statistic (relative to the null hypothesis) for samples of size n from the alternate hypothesis. \\(\\bar x_{(n)} \\mid H_1 \\sim \\operatorname{Normal}(\\mu_X^{H_1}, \\sigma_X^{H_1}/\\sqrt{n})\\) We know that: \\(\\sigma_X = \\sigma_X^{H_1} = \\sigma_X^{H_0}\\) (by virtue of the assumption that the alternate model has the same standard deviation as the null model!), and \\(\\mu_X^{H_1} = \\mu_X^{H_0} + d*\\sigma_X\\) (this is what the effect size – Cohen’s d – tells us). Consequently: \\(\\bar x_{(n)} \\mid H_1 \\sim \\operatorname{Normal}(\\mu_X^{H_0} + d*\\sigma_X, \\sigma_X/\\sqrt{n})\\) If we calculate the z-score of a sample mean from the alternate model, relative to the sampling distribution of the sample mean from the null model (as we do when we do significance testing), we get (by virtue of our rules about how to linearly transform normally distributed variables): \\(z_{\\bar x_{(n)}}^{H_1} = \\frac{\\bar x_{(n)}-\\mu_X^{H_0}}{\\sigma_X/\\sqrt{n}} \\mid H_1 \\sim \\operatorname{Normal}(d*\\sqrt{n}, 1)\\) We can then compare this to the critical z value. alpha = 0.05 z.crit = abs(qnorm(alpha/2)) n = 10 d = 8/15 # the effect size we built into the alternate model in the previous section (p.reject.H0.low = pnorm(-z.crit, d*sqrt(n),1)) # probability we would reject on the low end ## [1] 0.000132912 (p.reject.H0.high = 1-pnorm(z.crit, d*sqrt(n), 1)) # probability we would reject on the high end. ## [1] 0.3922668 (p.reject.H0 = p.reject.H0.low + p.reject.H0.high) # this is the power ## [1] 0.3923997 Notice that in this case (with some considerable effect size, and the standard deviation of the alternate equal to the standard deviation of the null model), there is a negligible probability that we would reject the null for alternate model samples on the other side of the null (in this case, rejecting alternate samples for being too low). Consequently, we can often ignore that lower tail, and simply calculate power from the tail that the effect size is on (by using the absolute value of the effect size). (p.reject.H0.high = 1-pnorm(abs(qnorm(alpha/2)), abs(d)*sqrt(n), 1)) ## [1] 0.3922668 Visualizing alpha and power Our example so far can be shown in one graph (with a bit of ggplot tinkering): alpha = 0.05 n = 10 d = 8/15 z.crit = abs(qnorm(alpha/2)) z = seq(-4,8,by=0.01) df = rbind(data.frame(z=z, p=dnorm(z), reject=ifelse(z&gt;=z.crit, &quot;H0 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H0 sig. low&quot;, &quot;H0 not sig&quot;)), distribution=&quot;H0&quot;), data.frame(z=z, p=dnorm(z,d*sqrt(n),1), reject=ifelse(z&gt;=z.crit, &quot;H1 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H1 sig. low&quot;, &quot;H1 not sig&quot;)), distribution=&quot;H1&quot;)) ggplot(subset(df, df$distribution==&quot;H0&quot;), aes(x=z, y=p, fill=reject, color=distribution))+ geom_area(alpha=0.3)+ geom_area(data = subset(df, df$distribution==&quot;H1&quot;), alpha=0.3)+ geom_vline(xintercept =z.crit)+ scale_fill_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;, &quot;orange&quot;, &quot;#008888&quot;, &quot;#008888&quot;))+ scale_color_manual(values=c(&quot;#880000&quot;, &quot;#008800&quot;)) Here, the distribution outlined in red is the sampling distribution of z scores from the null hypothesis; the distribution outlined in green is the sampling distribution of z scores (relative to the null sampling distribution) sampled from the alternate hypothesis. The red area corresponds to the probability of a type I error (alpha): rejecting a sample from the null hypothesis. The grey are corresponds to the probability of correctly failing to reject the null (for a sample from the null). The yellow area is the Type II error (beta): the probability of incorrectly failing to reject the null (for a sample from the alternate), and the teal area is the power – the probability of correctly rejecting the null (for a sample from the alternate). How power changes. Here we will considering how changes to effect size (\\(d\\)), sample size (\\(n\\)), and alpha (\\(\\alpha\\)) influence power. First, let’s define a few functions that will be helpful to us. getPower = function(alpha, n, d){ z.crit = abs(qnorm(alpha/2)) p.reject.H0.low = pnorm(-z.crit, d*sqrt(n),1) p.reject.H0.high = 1-pnorm(z.crit, d*sqrt(n), 1) return(p.reject.H0.low + p.reject.H0.high) } showAreas = function(alpha, n, d){ z.crit = abs(qnorm(alpha/2)) z = seq(-4,8,by=0.01) df = rbind(data.frame(z=z, p=dnorm(z), reject=ifelse(z&gt;=z.crit, &quot;H0 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H0 sig. low&quot;, &quot;H0 not sig&quot;)), distribution=&quot;H0&quot;), data.frame(z=z, p=dnorm(z,d*sqrt(n),1), reject=ifelse(z&gt;=z.crit, &quot;H1 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H1 sig. low&quot;, &quot;H1 not sig&quot;)), distribution=&quot;H1&quot;)) g = ggplot(subset(df, df$distribution==&quot;H0&quot;), aes(x=z, y=p, fill=reject, color=distribution))+ geom_area(alpha=0.3)+ geom_area(data = subset(df, df$distribution==&quot;H1&quot;), alpha=0.3)+ geom_vline(xintercept = z.crit)+ scale_fill_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;, &quot;orange&quot;, &quot;#008888&quot;, &quot;#008888&quot;))+ scale_color_manual(values=c(&quot;#880000&quot;, &quot;#008800&quot;))+ ggtitle(sprintf(&quot;alpha = %0.2f power = %0.2f&quot;, alpha, getPower(alpha,n,d)))+ theme(legend.position=&quot;none&quot;) return(g) } Changing alpha (\\(\\alpha\\)) library(gridExtra) n = 10 d = 0.5 alpha = c(0.01, 0.05, 0.1) g1 = showAreas(alpha[1], n, d) g2 = showAreas(alpha[2], n, d) g3 = showAreas(alpha[3], n, d) grid.arrange(g1,g2,g3,ncol=3) So, if we are willing to increase our Type I error rate, we can increase our power (by virtue of rejecting more of everything). This is not how we want to increase power, since our goal is not to trade one kind of error for another. Clearly, we don’t just want to move our cutoff, but want to further separate the distributions. Changing effect size (\\(d\\)) library(gridExtra) n = 10 d = c(0.25, 0.5, 0.75) alpha = 0.05 g1 = showAreas(alpha, n, d[1]) g2 = showAreas(alpha, n, d[2]) g3 = showAreas(alpha, n, d[3]) grid.arrange(g1,g2,g3,ncol=3) Let’s say we can somehow increase our effect size (perhaps by finding better, less noisy measurements?, or adopting a stronger manipulation?). If this happens, we effectively increase the separation between the null and alternate distributions, and increase power without lowering alpha or changing the sample size! In practice, it’s often tricky to increase the effect size though, so while we would like to do that, it’s usually not in our power. Changing sample size (\\(n\\)) library(gridExtra) n = c(4, 8, 16) d = 0.5 alpha = 0.05 g1 = showAreas(alpha, n[1], d) g2 = showAreas(alpha, n[2], d) g3 = showAreas(alpha, n[3], d) grid.arrange(g1,g2,g3,ncol=3) In practice, the easiest way to increase power is to increase the sample size. This effectively also separates the two distributions further, because the distance between the two sampling distributions of z-scores is \\(d*\\sqrt{n}\\). So we can get an effective separation that scales with the square root of the sample size. Calculating n for a desired level of power. If we have a particular effect size, we can calculate the sample size required to achieve a particular level of power. (Here, we are using the simplified, one-tail power, which works if our assumption of equal variance in null and alternate is correct, and the effect size is not zero.) power = 1-pnorm(abs(qnorm(alpha/2)), abs(d)*sqrt(n), 1)) With algebra, we get: pnorm(abs(qnorm(alpha/2)), abs(d)*sqrt(n), 1)) = 1-power Since the quantile function (qnorm) is the inverse of the cumulative distribution (pnorm)… abs(qnorm(alpha/2)) = qnorm(1-power, abs(d)*sqrt(n), 1) Since the normal is invariant to shifts in the mean… abs(qnorm(alpha/2)) = qnorm(1-power) + abs(d)*sqrt(n) We also know that the quantiles of the standard normal are symmetric around 0, so we can get rid of an absolute value… qnorm(1-alpha/2) - qnorm(1-power)= abs(d)*sqrt(n) ((qnorm(1-alpha/2)-qnorm(1-power))/abs(d))^2=n So, for a particular level of power that we might want, we can estimate the required sample size as: n.for.power = function(power,alpha,d){ return(((qnorm(1-alpha/2)-qnorm(1-power))/abs(d))^2) } alpha = 0.05 d = 0.3 power = seq(0.05,0.95,by=0.05) df = data.frame() for(i in 1:length(power)){ df = rbind(df, data.frame(power=power[i], d=d, alpha=alpha, required.n=n.for.power(power[i], alpha, d))) } df ## power d alpha required.n ## 1 0.05 0.3 0.05 1.103273 ## 2 0.10 0.3 0.05 5.113816 ## 3 0.15 0.3 0.05 9.476764 ## 4 0.20 0.3 0.05 13.896561 ## 5 0.25 0.3 0.05 18.360489 ## 6 0.30 0.3 0.05 22.898250 ## 7 0.35 0.3 0.05 27.550025 ## 8 0.40 0.3 0.05 32.361569 ## 9 0.45 0.3 0.05 37.385180 ## 10 0.50 0.3 0.05 42.682876 ## 11 0.55 0.3 0.05 48.331478 ## 12 0.60 0.3 0.05 54.430511 ## 13 0.65 0.3 0.05 61.115102 ## 14 0.70 0.3 0.05 68.578522 ## 15 0.75 0.3 0.05 77.114961 ## 16 0.80 0.3 0.05 87.209775 ## 17 0.85 0.3 0.05 99.759969 ## 18 0.90 0.3 0.05 116.749145 ## 19 0.95 0.3 0.05 144.385667 The noteworthy thing here is that achieving a level of power that folks conventionally recommend (0.8), requires a very large sample size for common, modest (d=0.3) effect sizes. In practice, when you want to calculate power, I recommend using the pwr package in R, rather than undertaking this manual calculation (especially because there will not be an easy analytical solution as most tests rely on distributions whose shape varies with sample size.) Sign and magnitude errors. Instead of dividing up errors into Type I/II (falsely rejecting, and falsely failing to reject the null), it is helpful instead to consider errors in sign and magnitude of the effect we report as significant. This philosophy makes a lot of sense if you consider that very few effects are truly zero (so rejecting the null isn’t that important), but are instead small (and variable), and we need to know their size and direction. A sign error amounts to getting the direction of the effect wrong. A magnitude error amounts to overestimating the effect size. Magnitude errors. Consider one of our earlier plots of the rejected and retained null hypotheses for samples from the null, and samples from the alternate with a particular effect size. alpha = 0.05 n = 10 d = 0.5 z.crit = abs(qnorm(alpha/2)) z = seq(-4,8,by=0.01) df = rbind(data.frame(z=z, p=dnorm(z), reject=ifelse(z&gt;=z.crit, &quot;H0 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H0 sig. low&quot;, &quot;H0 not sig&quot;)), distribution=&quot;H0&quot;), data.frame(z=z, p=dnorm(z,d*sqrt(n),1), reject=ifelse(z&gt;=z.crit, &quot;H1 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H1 sig. low&quot;, &quot;H1 not sig&quot;)), distribution=&quot;H1&quot;)) ggplot(subset(df, df$distribution==&quot;H0&quot;), aes(x=z, y=p, fill=reject, color=distribution))+ geom_area(alpha=0.3)+ geom_area(data = subset(df, df$distribution==&quot;H1&quot;), alpha=0.3)+ geom_vline(xintercept = z.crit)+ scale_fill_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;, &quot;orange&quot;, &quot;#008888&quot;, &quot;#008888&quot;))+ scale_color_manual(values=c(&quot;#880000&quot;, &quot;#008800&quot;)) We see that the bulk of the ‘rejected’ alternate hypothesis distribution falls on the wrong (do not reject) side of the critical z value. Thus, all the z scores we reject were abnormally high as far as samples from the alternate distribution go. Consequently, if we consider the effect size we might estimate from the sample (which we can get as \\(z/\\sqrt{n}\\)), we would expect an overestimate, on average. This is precisely what we see when we calculate the average estimated effect from samples from the alternate that were statistically significant. library(dplyr, quietly=TRUE) df$d.est = df$z/sqrt(n) df %&gt;% filter(distribution == &quot;H1&quot;, reject == &quot;H1 sig. high&quot;) %&gt;% summarize(true.d = d, avs.est.d = sum(d.est*p)/sum(p)) ## true.d avs.est.d ## 1 0.5 0.832083 This basic effect is sometimes called the “statistical significance filter”: findings that are significant, are likely to overestimate the true effect size in the population. Moreover, it’s easy to convince ourselves that the lower the power, the worse the overestimation: If power=100%, then we get 0 overestimation. Sign errors The other kind of error worth considering is the probability that we get the direction of the effect wrong. Although with a reasonable effect size, and no difference in variance between the null and alternate hypothesis, this probability is quite small, it might become intollerable with low power. alpha = 0.05 n = 10 d = 0.05 z.crit = abs(qnorm(alpha/2)) z = seq(-4,8,by=0.01) df = rbind(data.frame(z=z, p=dnorm(z), reject=ifelse(z&gt;=z.crit, &quot;H0 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H0 sig. low&quot;, &quot;H0 not sig&quot;)), distribution=&quot;H0&quot;), data.frame(z=z, p=dnorm(z,d*sqrt(n),1), reject=ifelse(z&gt;=z.crit, &quot;H1 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H1 sig. low&quot;, &quot;H1 not sig&quot;)), distribution=&quot;H1&quot;)) ggplot(subset(df, df$distribution==&quot;H0&quot;), aes(x=z, y=p, fill=reject, color=distribution))+ geom_area(alpha=0.3)+ geom_area(data = subset(df, df$distribution==&quot;H1&quot;), alpha=0.3)+ geom_vline(xintercept = z.crit)+ geom_vline(xintercept = -z.crit)+ scale_fill_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;, &quot;orange&quot;, &quot;#008888&quot;, &quot;#008888&quot;))+ scale_color_manual(values=c(&quot;#880000&quot;, &quot;#008800&quot;)) With low power (arising from small effect and sample sizes), we see that many of our rejections of the null hypothesis based on samples from the null are actually coming from the wrong side of the null! We can calculate their proportion as a function of power. (Note that here we are interested in power to reject on both the correct, and incorrect tail; and we don’t care about whether power comes from effect size or sample size, so we adopt a somewhat tricky equivalence, which you can ignore.) pow = function(m,z.crit){pnorm(-z.crit,m,1)+1-pnorm(+z.crit,m,1)} p.sign.error = function(des.pow,alpha){ z.crit = abs(qnorm(alpha/2)) mz = seq(0,4,by=0.01) m = mz[which.min((pow(mz,z.crit)-des.pow)^2)] return(pnorm(-z.crit,m,1)/(pnorm(-z.crit,m,1) + 1 - pnorm(+z.crit,m,1))) } alpha = 0.05 df = data.frame() power = seq(0.05,0.15,by=0.01) for(i in 1:length(power)){ df = rbind(df, data.frame(power=power[i], alpha=alpha, p.sign.err = p.sign.error(power[i],alpha))) } df ## power alpha p.sign.err ## 1 0.05 0.05 0.50000000 ## 2 0.06 0.05 0.20482240 ## 3 0.07 0.05 0.12289824 ## 4 0.08 0.05 0.08414848 ## 5 0.09 0.05 0.06204462 ## 6 0.10 0.05 0.04544329 ## 7 0.11 0.05 0.03465503 ## 8 0.12 0.05 0.02634886 ## 9 0.13 0.05 0.02093070 ## 10 0.14 0.05 0.01660337 ## 11 0.15 0.05 0.01378320 What this tells us is that a null hypothesis z-test, with the standard deviation correctly matched between the true alternate and the null, will declare a sample from the alternate as significant, but get the direction of the effect wrong frighteningly frequently when our power is very low. Hopefully, our power is rarely that low. However, one problem that is likely to arise when running z-tests is that the population might have a different standard deviation than assumed under the null hypothesis, we get quite a different phenomenon: alpha = 0.05 n = 10 d = 0 sd.ratio = 2 z.crit = abs(qnorm(alpha/2)) z = seq(-7,7,by=0.01) df = rbind(data.frame(z=z, p=dnorm(z), reject=ifelse(z&gt;=z.crit, &quot;H0 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H0 sig. low&quot;, &quot;H0 not sig&quot;)), distribution=&quot;H0&quot;), data.frame(z=z, p=dnorm(z,d*sqrt(n),1*sd.ratio), reject=ifelse(z&gt;=z.crit, &quot;H1 sig. high&quot;, ifelse(z&lt;=-z.crit, &quot;H1 sig. low&quot;, &quot;H1 not sig&quot;)), distribution=&quot;H1&quot;)) ggplot(subset(df, df$distribution==&quot;H0&quot;), aes(x=z, y=p, fill=reject, color=distribution))+ geom_area(alpha=0.3)+ geom_area(data = subset(df, df$distribution==&quot;H1&quot;), alpha=0.3)+ geom_vline(xintercept = z.crit)+ geom_vline(xintercept = -z.crit)+ scale_fill_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;, &quot;orange&quot;, &quot;#008888&quot;, &quot;#008888&quot;))+ scale_color_manual(values=c(&quot;#880000&quot;, &quot;#008800&quot;)) As you can see, even with no difference in means, but a difference in standard deviations, we might get considerable power (we will reject the null for lots of samples from the alternate). But not for the reason we think (not because there is a difference in means, but because there is a difference in standard deviations). This is the reason we generally don’t use z-tests (which postulate a null hypothesis with a specific mean and a specific standard deviation), and use t-tests instead (which postulate a null hypothesis with a specific mean, but are agnostic as to the standard deviation). From probability to statistics with the Binomial These notes are (a) very mathy, (b) very optional, and (c) cover a very broad range of material that we will not directly address this term. I hope that for especially advanced and ambitious folks, this kind of theoretical exposition of what is done, and why, in various statistical methods will be helpful. For folks who are less comfortable with math, this will be a whole lot of difficult material that is likely to increase, rather than reduce, confusion. Let’s say our data are the outcomes of 10 coin flips obtained by flipping a coin 10 times: \\(X = \\{x_1, x_2, ..., x_10\\} = \\{H,H,H,H,H,H,H,H,T,T\\}\\). Data description / summary The first thing we might do is summarize the data. Whenever we summarize data we inevitably make some assumptions about which aspects of our data are important, and which are not. Often, these assumptions reflect our hypotheses about how the data were generated, and which aspects of the data ought to be relatively stable across samples. “Descriptive statistics” is usually used as the opposite of “inferential statistics,” suggesting that when we choose some way to summarize the data, we are in no way drawing inferences or attempting to generalize from the sample. This is true only technically, but not practically, since our data descriptions often tend to coincide with estimators of model parameters. Hopefully this will become more clear as this section continues. Let’s start with our data: 10 outcomes of a coin flip: H,H,H,H,H,H,H,H,T,T. How should we summarize the data? Well, we can first tabulate the frequencies of different outcomes, and if we are so inclined, we can plot these frequencies as a histogram. outcome frequency H 8 T 2 By summarizing the frequencies of heads and tails, we are throwing out information about the order in which those data points came. This is a useful summary insofar as we believe that the order doesn’t matter. The order doesn’t matter if we believe that those particular H/T outcomes are independent (if the flip of a coin does not depend on the previous outcome of the coin flip, or any other coin flip at all). However, if we believed that our data came from a coin flipping process where the coin-flipper tended to flip the coin an even number of times, that means that the second outcome will tend to be like the first outcome, and the third like the second, etc. Thus, the individual coin flips are not independent, because they were generated by a process that tends to reproduce the same outcome in succession. If we thought that such a process was underlying out data, then instead of tabulating the raw frequencies of heads and tails we might choose to tabulate how often the outcomes were different in consecutive pairs of flips (note that while we have 10 flips, we only have 9 consecutive pairs). By choosing to tabulate repetitions/swaps, we throw out information about whether the coin was heads or tails, and which came up more often: outcome frequency same 8 different 1 Which of these histograms is a more useful summary depends on our beliefs about how the data were generated, and which features of the data are more or less important. It is important to note that both summaries throw out some information while highlighting some other information. In addition to tabulating frequencies, often data are summarized with some “statistics”. Any function of the data that returns some simple number (not always, but usually just one number), is called a statistic. Some of these statistics are more useful than others because they have useful properties and serve as estimators for parameters of common models. All of these summarize some aspect of the data, while ignoring other aspects; thus in choosing a particular summary statistic you effectively presume which aspects of your data are important, and which can be disregarded. For instance, for our data we might consider a number of statistics: \\(\\frac{\\text{# heads}}{\\text{total}} = 0.8\\), or \\(\\frac{\\text{# heads}}{\\text{# tails}} = 4.0\\), or “position of first occurrence of tails” \\(= 9\\), or \\(\\frac{\\text{# repeats}}{\\text{# consecutive pairs}} = 0.\\bar8\\), etc. In short, the choice of which aspects of the data to summarize necessarily reflects some implicit beliefs about the structure that ought to be present in the data. Similarly, when we decide to present some graphical summary of the data, we will also need to choose which aspects of the data to emphasize, and which can be obscured. Therefore the choice of which descriptive statistics to report and consider should be done with ample thought and care (like everything else in scientific research) Estimation Descriptive statistics are somewhat motivated by an (implicitly) assumed model of the data, if we make this process explicit, we will postulate a particular statistical model of the data. We usually call this the population model, which will have some unknown parameters. For instance, we might suppose that our sequence of coin flips reflect independent, identically distributed outcomes from the flip of a bent coin that comes up heads with probability \\(\\theta\\) (theta; note we are now switching to using greek letters for population parameters), and tails with probability \\(1-\\theta\\). Thus our model of the data is that the number of heads, \\(k\\), out of \\(n\\) flips is a sample from a Binomial: \\(k \\sim \\operatorname{Binomial}(k \\mid n, \\theta)\\), and we need to use our sample of coin flips to estimate the population parameter (obtaining the estimate \\(\\hat\\theta\\)). Point Estimates Point estimates are a single value, corresponding to our best guess about the latent parameter; these are contrasted with interval estimates which provide a range of plausible values that the parameter might be. We will consider two approaches to estimating parameters, in our case just \\(\\theta\\): Maximum Likelihood and Maximum A Posteriori. There are others methods for estimation, but explaining how they work, and the logic behind them, is rather convoluted. Moreover, there is rarely any reason to prefer them. Popular alternative approaches include “Method of Moments” and “Minimum squared error” (a.k.a. “Least squares”). Later in the course we will talk about least squares estimates, but this will be in the context of models where least squares error estimates are equivalent to the maximum likelihood estimates, so while we will use the least squares estimation procedure, we can think of it as a convenient way to obtain a maximum likelihood estimate. (One notable exception is estimators for the variance.) Classical (Maximum Likelihood) estimate The Maximum Likelihood estimate (ML estimate) aims to find the parameter value \\(\\theta\\) which makes the data most likely, thus maximizing the likelihood function. What is the likelihood function? Consider the conditional probability \\(P(X|\\theta)\\), this assigns probabilities to different data outcomes for a given \\(\\theta\\). For instance: \\(P(3 \\text{ heads out of } 8|\\theta=0.8) = \\operatorname{Binomial}(3 \\mid 8, 0.8) = {8 \\choose 3} 0.8^3(1-0.8)^7\\) = 0.009175 \\(P(7 \\text{ heads out of } 8|\\theta=0.8) = \\operatorname{Binomial}(7 \\mid 8, 0.8) = {8 \\choose 7} 0.8^7(1-0.8)^1\\) = 0.3355443 If we consider \\(P(X|\\theta,n)\\) as a function over possible \\(X\\)s, assigning probability to different data outcomes while \\(\\theta\\) remains fixed, then \\(P(X|\\theta,n)\\) is a conditional probability distribution over possible data sets (so if you sum \\(P(X|\\theta,n)\\) over all possible \\(X\\)s, you will get 1.) In contrast, the likelihood function, usually written as \\(\\mathcal{L}(X|\\theta)\\), is not a probability distribution. It is a collection of \\(P(X|\\theta)\\) values for different values of \\(\\theta\\) while \\(X\\) is fixed. The likelihood function does not sum to 1 when you sum over all \\(\\theta\\)s; however, it does indicate how likely the data are under each possible \\(\\theta\\): \\(\\mathcal{L}(3 \\text{ heads out of } 8|\\theta=0.2) = \\operatorname{Binomial}(3 \\mid 8, 0.2) = {8 \\choose 3} 0.2^3(1-0.2)^7\\) = 0.1468006 \\(\\mathcal{L}(3 \\text{ heads out of } 8|\\theta=0.4) = \\operatorname{Binomial}(3 \\mid 8, 0.4) = {8 \\choose 3} 0.4^3(1-0.4)^7\\) = 0.2786918 \\(\\mathcal{L}(3 \\text{ heads out of } 8|\\theta=0.6) = \\operatorname{Binomial}(3 \\mid 8, 0.6) = {8 \\choose 3} 0.6^3(1-0.6)^7\\) = 0.123863 \\(\\mathcal{L}(3 \\text{ heads out of } 8|\\theta=0.8) = \\operatorname{Binomial}(3 \\mid 8, 0.8) = {8 \\choose 3} 0.8^3(1-0.8)^7\\) = 0.009175 The likelihood function effectively describes how well different values of our parameter \\(\\theta\\) describe the data \\(X\\). If we choose the maximum point of this function, we choose the parameter value that maximizes the likelihood of the data. We can plot the likelihood function for our data (\\(X= \\{H,H,H,H,H,H,H,H,T,T\\}\\)) below: data = c(&#39;H&#39;,&#39;H&#39;,&#39;H&#39;,&#39;H&#39;,&#39;H&#39;,&#39;H&#39;,&#39;H&#39;,&#39;H&#39;,&#39;T&#39;,&#39;T&#39;) n.heads = sum(data==&#39;H&#39;) n.total = length(data) thetas = seq(0,1,by=0.001) lik.theta = dbinom(n.heads, n.total, thetas) library(ggplot2) ggplot(data.frame(theta=thetas, likelihood=lik.theta), aes(theta, likelihood))+geom_line() We can find the value of \\(\\theta\\) that maximizes the likelihood with: thetas[which(lik.theta==max(lik.theta))] ## [1] 0.8 The \\(\\theta\\) for which the likelihood is greatest is 0.8. Thus, our maximum likelihood estimate is \\(\\hat \\theta_{ML} = 0.8\\). The hat operator (\\(\\hat \\cdot\\)) is convention for indicating an estimate of a parameter value, the subscript usually denotes the type of estimator used to obtain the estimate (here “ML” for “maximum likelihood”). Note that for many statistical models, the maximum likelihood estimate need not be calculated numerically, but has an analytical solution; we will go over these when appropriate in the future, but for now, I want to convey what the maximum likelihood estimator means. Bayesian (Maximum a posteriori) estimate The maximum a posteriori estimate (MAP estimate) uses Bayes’ rule to invert the probability, to obtain \\(P(\\theta|X)\\) from \\(P(X|\\theta)\\). Of course, to do so one must specify some sort of prior distribution over the parameter \\(\\theta\\) (\\(P(\\theta)\\); this need to choose a prior is one reason why some are uneasy about Bayesian estimation). Let’s say we believe that a coin is unlikely to be biased too far away from 0.5, so our prior will be centered on 0.5. We can express such a prior with a beta distribution. \\(P(\\theta)= \\operatorname{Beta}(2,2)\\) This is the prior on \\(\\theta\\). So now, we can calculate the posterior as: \\(P(\\theta|X) = \\frac{\\mathcal{L}(X|\\theta)P(\\theta)}{\\int\\limits_0^1 \\mathcal{L}(X|\\theta)P(\\theta)d\\theta}\\) (Note that the denominator comes from the law of total probability.) In practice we could use an analytical solution for this posterior distribution (the Beta distribution, by virtue of it being a conjugate prior for the Binomial); however, here we can just do it via a crude numerical estimate. p.theta = dbeta(thetas, 2, 2) p.theta = p.theta/sum(p.theta) p.data = sum(lik.theta*p.theta) p.theta.data = lik.theta*p.theta / p.data p.theta.data.uni = lik.theta/sum(lik.theta) ggplot(rbind(data.frame(theta=thetas, probability=p.theta, whichone=&quot;beta prior&quot;), data.frame(theta=thetas, probability=p.theta.data, whichone=&quot;posterior (beta)&quot;), data.frame(theta=thetas, probability=p.theta.data.uni, whichone=&quot;posterior (uniform)&quot;)), aes(theta, probability, color=whichone))+geom_line() thetas[which(p.theta.data == max(p.theta.data))] # Maximum a posteriori ## [1] 0.75 Note that (a) the posterior distribution under a uniform prior (which is just the normalized likelihood), has the same shape and peak as just the likelihood function, (b) the posterior under a non-uniform prior gives us an estimate somewhere between the likelihood and the prior. Here the the maximum a posteriori (MAP) estimate is 0.75; we would write this as \\(\\hat \\theta_{MAP} = 0.75\\). (the MAP estimate under a uniform prior gives us the same estimate as the maximum likelihood). Estimators While the two approaches we have described are general, all-purpose methods to obtain ML and MAP estimates, typically, we will not work through all of this. Instead we will use special types of statistics (again, statistics meaning “functions of the data”) which someone else has proven to yield the appropriate estimates: these are called estimators. For instance, \\(k/n\\) is a maximum likelihood estimator for the Bernoulli/Binomial \\(\\theta\\), in the sense that it will always give the \\(\\theta\\) value which maximizes \\(\\mathcal{L}(D|\\theta)\\). The sample mean (which we will get to later) \\(\\bar x = \\sum_{i=1}^n x_i / n\\) is the maximum likelihood estimator for the mean (\\(\\mu\\)) when we assume the data come from a Gaussian, because, again, it will always maximize the Gaussian likelihood. So, estimators are functions of the data that yield useful estimates. Not all estimators map neatly on to the ML estimate, and sometimes this is quite desirable. For instance, the maximum likelihood estimate of the variance is biased because of the skewed shape of the likelihood function for variance. In this case, a different estimator is used (which can be thought of as a correction to the ML estimate). Interval Estimates Our point estimates, \\(\\hat \\theta_{ML}\\) and \\(\\hat \\theta_{MAP}\\), are not particularly useful on their own, because they are bound to have some error/uncertainty due to sampling variability. In other words: if we flip the same coin 10 more times, we will get different outcomes, and the number of heads will not be exactly 8; thus, the 8/10 estimate from our data arises from the idiosyncrasies of our specific sample. Consequently, instead of providing a single best guess (a point) as an estimate, it is usually preferable to provide a range (interval) of guesses around our best estimate, to indicate the expected precision of our estimate. Usually these are expressed as a percentage (e.g., a 95% interval). And for the general case we will often refer to a \\(100*q\\%\\) interval (where \\(q\\) is between 0 and 1). However, when providing such interval estimate, the difference between frequentist and Bayesian approaches to statistics becomes quite important. Bayesian Credible Intervals I will start by describing Bayesian interval estimates, called “credible intervals,” because their definition, interpretation, and procedure for calculation is intuitive – it is what most people think of when they read “95% interval estimate of men’s shoe size is 7 to 11,” or other such statements. \\([a,b]\\) is a \\(100*q\\%\\) credible interval for our parameter \\(\\theta\\) if \\(P(a \\leq \\theta \\leq b | X) = q\\). Which should be intuitive: \\([a,b]\\) is a \\(100q\\%\\) credible interval for our parameter \\(\\theta\\) if the posterior probability that \\(\\theta\\) is between \\(a\\) and \\(b\\) is \\(q\\). In the previous section we calculated the posterior distribution of \\(\\theta\\): \\(P(\\theta|X)\\). From this distribution we can compute the cumulative distribution function: \\(F_{\\theta}(\\theta&#39;) = P(\\theta \\leq \\theta&#39; | X)\\). This cdf has an intuitive interpretation: what is the probability that the parameter value \\(\\theta\\) is less than or equal to \\(\\theta&#39;\\)? Thus, if we aim to obtain a \\(100q\\%\\) interval, we need to find values \\(a\\) and \\(b\\) such that \\(F_{\\theta}(b) - F_{\\theta}(a) = q\\). There are many pairs of values for which this will be true, but some of them are more useful than another. It is generally most convenient to take an interval that leaves equal amounts of probability on either side (thus guaranteeing to include the median, so that \\(F_{\\theta}(a) = 0.5 - q/2\\) and \\(F_{\\theta}(b) = 0.5 + q/2\\). (Note that this procedure does not guarantee that the MAP estimate – the mode of the posterior distribution– will be included in the interval, but it is easiest to define an interval around the median; moreover, when dealing with wide intervals [when \\(q\\) is quite large], it doesn’t much matter.) For instance, if we use our numerical estimates from before, we can calculate an approximation of the cdf, and estimate quantiles from it numerically. cum.probability = cumsum(p.theta.data) ggplot(data.frame(theta=thetas, cumulative.prob=cum.probability), aes(theta, cumulative.prob))+geom_line() range(thetas[cum.probability&gt;=0.025 &amp; cum.probability&lt;=0.975]) ## [1] 0.462 0.908 Obviously, the validity of our estimates here is contingent on the granularity of our numerical approximation: if we consider only a small number of thetas, we cannot estimate an interval estimate very precisely. These credible intervals are constructed from the posterior distribution of the parameter given the data; thus we can make statements like “given the data, (and our prior), the probability that \\(\\theta\\) is between 0.46 and 0.91 is 95%.” Frequentist Confidence Intervals Frequentists are not as lucky as Bayesians when it comes to constructing interval estimates. According to the philosophy of frequentist statistics, there exists a true parameter value in the world. This value is fixed, and it is meaningless to assign probabilities to different possible parameter values (because probabilities are relative frequencies, and the parameter value will always be the same – it is fixed). Consequently, a given interval estimate either includes, or does not include, the true parameter value, and again, we cannot assign a probability to the statement that “the true parameter value is within this interval” (that statement is either true or false, and it will always be either true or false, so we can’t assign it a relative frequency). However, while we cannot make frequentist probability statements about a particular estimate, we can make probability statements about our procedure, so we might say, “the procedure that I used for constructing this 95% confidence interval, if repeated many many times in different experiments, will contain the respective true parameter values 95% of the time.” The Wikipedia confidence interval page provides the rather unwieldy formal definition of a confidence interval which I will simplify for our case. We can define two functions of the data (statistics) \\(L(X)\\) and \\(U(X)\\). If the interval \\([L(X), U(X)]\\) is a \\(100q\\%\\) confidence interval then if we generate many data sets from a Bernoulli distribution with a parameter \\(\\theta\\), and we construct a lower and upper bound using the functions \\(L(\\cdot)\\) and \\(U(\\cdot)\\) for each data set, then the parameter \\(\\theta\\) will be within those bounds \\(q\\) proportion of the time. Thus, the \\(100q\\%\\) associated with a confidence interval statement is not a statement about the parameter value, but a statement about the functions \\(L()\\) and \\(U()\\): those functions have the property that 95% of the intervals constructed using these functions will contain the true parameter value for that experiment. This is why frequentist confidence intervals are confusing. That’s all well and good, but how do we actually obtain a confidence interval in our case of 8 heads and 2 tails? A number of approaches to constructing intervals are available that have (approximately) the required property described above. The most common relies on a normal approximation (which works reasonably provided you have enough data and \\(\\theta\\) is not too close to 0 or 1). This confidence interval with \\(q=1-\\alpha\\), is defined as: \\(\\hat\\theta \\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat\\theta(1-\\hat\\theta)}{n}}\\), where \\(Z_{\\alpha/2}\\) is the z-score for \\((\\alpha/2)\\) – the \\((\\alpha/2)\\) quantile of the standard normal distribution (we will discuss this later). Since we haven’t yet talked about the normal distribution, we will not discus this approximate confidence interval further until we are really interested in categorical data; plus, that interval estimate will be inappropriate because of our small sample size). Confidence intervals from null hypothesis tests Instead of using a confidence interval from the normal approximation, we will consider the “exact” frequentist confidence interval, purely for pedagogical (not practical) purposes. Under this approach, we construct a confidence interval via significance testing. This yields the second interpretation of a frequentist confidence interval: “A \\(100q\\%\\) confidence interval for a parameter \\(\\theta\\) is the range of values that \\(\\theta_0\\) (the null hypothesis value of \\(\\theta\\)) could take on without you being able to reject the null hypothesis with your data at significance level \\(\\alpha = 1-q\\).” We will now construct a confidence interval for our Bernoulli \\(\\theta\\) in this manner (Although normally this could done analytically, here we will go through the exercise of calculating it numerically). We will use the binom.test function to get a p.value for the null hypothesis binomial test, and obtain this p-value for every theta we can consider, then keep the thetas we could not reject, and define our interval based on the range of the theta values we could not reject. p.val = sapply(thetas, function(theta){min(binom.test(n.heads, n.total, theta, alternative = &quot;greater&quot;)$p.value, binom.test(n.heads, n.total, theta, alternative = &quot;less&quot;)$p.value)}) not.rejected.thetas = thetas[p.val&gt;(1-0.95)/2] range(not.rejected.thetas) ## [1] 0.444 0.974 So the interval we get out is [0.45 0.96]. Null Hypothesis Significance testing (NHST) Hypothesis testing typically follows these steps: First, you define a null hypothesis, usually written as \\(\\mathcal{H}_0\\). This null hypothesis should capture some belief about the population (or data distribution), which is the default/standard, or the dull “no discovery here” current belief. For instance, if comparing salaries between men and women, you might want the null hypothesis to be “they have the same average.” Or if you are Kepler measuring planetary orbits, you might pose the null hypothesis that orbits are circular (rather than elliptical). Or if you are measuring coin flips, as we are, your null hypothesis could sensibly be “they are independent, identically distributed samples from a fair coin (\\(X\\sim \\text{Bernoulli}(\\theta=0.5)\\)).” Note that our null hypothesis here is “parametric” in the sense that it makes a claim about the distribution of the data by specifying a parametric distribution of the data. Second, you define a “test statistic.” This is like like any other statistic (a function of the data which returns a value), but it will serve as the basis for your significance test. Let’s call the value you obtain for your test statistic \\(v_x\\) (for “value (from the data \\(x\\))”). For us, a sensible choice of test statistic would be “the number of heads” observed: \\(v_x = 8\\). Third, you obtain (look up somewhere, compute numerically, or calculate analytically) the “sampling distribution” for this test statistic under the null hypothesis. In other words, if the null hypothesis were true, what values of the test statistic would you expect to see, with what frequency. We can refer to this distribution as \\(P_0(V)\\). This is the “sampling distribution of V” where V is the test statistic of interest under the null hypothesis. This is often simply called the “null hypothesis distribution” (beware that this creates some ambiguity, since the null hypothesis distribution also refers to the distribution of possible data under the null hypothesis.) Fortunately, we have already calculated the sampling distribution for our “number of heads” test statistic: the Binomial distribution. Fourth, you compare where your observed test statistic (\\(v_x\\)) falls with respect to the null-hypothesis sampling distribution of the test statistic (\\(P_0(V)\\)). Your goal is to ascertain the probability under the null hypothesis of observing a test-statistic value at least as “extreme” as the one you calculated of your data. \\(P_0(V \\geq v_x) = \\text{p-value}\\). (We are doing a one-tailed test here – not because it is the right thing to do, but because it is easier to describe as a start.) This is the “p value” – the probability that a test statistic at least as extreme as the one you observed would arise from the null hypothesis distribution. You will then reject the null hypothesis if this probability is small enough, and if the probability is not sufficiently small, you will “fail to reject” the null hypothesis. (You will never “accept” the null hypothesis, since NHST follows Karl Popper’s “falsificationist” approach to the philosophy of science: meaning that science can only falsify theories, but never validate them.) There are a number of variations on hypothesis testing, but the essence remains the same. The traditional procedure is to choose a “significance level” some time before step 4. A significance level, usually denoted as \\(\\alpha\\) (“alpha”), indicates the rate at which you are willing to falsely reject the null hypothesis. The traditional procedure suggests that you use \\(\\alpha\\) to compute a critical value \\(v_{crit}\\), defined (for our one tailed test) as \\(P_0(V \\geq v_{crit})=\\alpha\\). The null hypothesis is rejected if your test statistic is more extreme than the critical value (\\(v_x \\geq v_{crit}\\)). Note that if \\(v_x \\geq v_{crit}\\), then, necessarily \\(\\text{p-value} \\leq \\alpha\\), these are exactly the same condition for rejecting the null hypothesis. In the past the typical approach was to choose one of a few standard \\(\\alpha\\) values, then find the critical statistic; this was the case because one could not calculate a p-values due to lack of computers, and folks relied on pre-calculated tables to find specific critical values. Nowadays, it is much more common to simply report the calculated p-value, because people can. Choosing \\(\\alpha\\) is quite arbitrary, but according to tradition in psychology and social science, \\(\\alpha=0.05\\). This tradition arose because Ronald Fisher said (paraphrasing), “Being wrong 1 out of 20 times seems fine.” Note that the standards in other fields are considerably higher, for instance, in physics, the standard for a discovery is “six-sigma” – an observation that is 6 standard deviations away from the average of the noise – which amounts to \\(\\alpha=10^{-9}=0.000000001\\). There is hardly ever any reason to prefer an \\(\\alpha\\) value that is higher than 0.05 (that is, to decide that you are content with a higher than 1 in 20 chance of false positives), but there is often reason to choose a lower alpha value. When p-values are between 0.1 and 0.05, they are often called “marginally significant” and a number of other awkward phrases. There are other variations (involving the directionality of the test, but we will cover those later). Ok, so let’s say we want to test the null hypothesis \\(\\mathcal{H}_0: D \\sim \\text{Bernoulli}(\\theta_0 = 0.5)\\), via the test statistic “number of heads” (\\(v\\)). We have previously introduced the Binomial distribution, which, conveniently, gives us the sampling distribution of our test statistic under our null hypothesis. Recall that the \\(\\text{Binomial}(k|n,\\theta)\\) distribution distributes probability over the number of successes \\(k\\) out of \\(n\\) attempts, with a probability of success \\(\\theta\\). So in our case: \\(P_0(V=v) = P(K=k|n=10,\\theta_0=0.5)=\\text{Binomial}(k|n=10,\\theta_0=0.5)\\). n.total = length(data) n.heads = sum(data==&#39;H&#39;) ks = seq(0,n.total) ggplot(data.frame(n.heads = ks, p.H0=dbinom(ks, n.total, 0.5), geq=ks&gt;=n.heads), aes(x=n.heads, y=p.H0, fill=geq))+geom_bar(stat=&quot;identity&quot;) Let’s work through this graph. The x axis shows \\(k\\): the number of heads one might get from \\(n\\) flips. The y axis shows the the probability of obtaining \\(k\\) heads out of \\(n=10\\) flips where each flip is heads with probability \\(\\theta_0=0.5\\); in other words, this is our null hypothesis distribution for the test statistic \\(k\\). The colors of the bars indicate whether or not this value of \\(k\\) is at least as large as the number we saw in our sample. The total probability of the bars where \\(k\\) is at least as large as n.heads is the p-value for the one (upper) tailed null hypothesis test. p.value = sum(dbinom(ks[ks&gt;=n.heads],n.total,0.5)) p.value ## [1] 0.0546875 We can then compare the p.value to \\(\\alpha=0.05\\) to see if it is significant. p.value &lt;= 0.05 ## [1] FALSE Note that we did a one-tailed test, meaning we only looked for extreme outcomes on one side of the distribution (8 or more). A two-tailed test would also consider equally extreme outcomes on the other side (2 or fewer heads out of 10). A two tail test distributes the probability \\(\\alpha\\) to both tails (but this is somewhat tricky to do with potentially asymmetric binomial distributions, so let’s skip it for now). The statistical test we just described is often called the Binomial test (obviously because it is based on the binomial distribution), which might be run with binomial.test() in R. While it is useful for testing null hypotheses about the probability underlying a bunch of data that comes from Bernoulli trials, it is used more often to test null hypotheses about the median or sign. Model selection Thus far every individual analysis we have done has assumed a particular model for our data: for instance, we assumed that the coin flips are independent, but perhaps biased; or the converse: the coin flips are not independent, but unbiased with respect to heads/tails). How do we decide between these two models? This is the domain of model selection, and all model selection approaches have a similar structure: they find a tradeoff between how well a given model can fit the data and how complicated the model is. At the start of the estimation section we proposed two different models of our data (\\(\\{H,H,H,H,H,H,H,H,T,T\\}\\)); we now add a third: \\(M_0\\): The data are independent samples from a Bernoulli distribution with some unknown parameter \\(\\theta\\) (this is the intuitive model of coin-flips, so each one is independent of the others) \\(M_1\\): The data are sequentially dependent samples, the first one is H or T with probability 0.5, and \\(x_i\\) is the same as \\(x_{i-1}\\) with probability \\(\\theta\\), and different with probability \\((1-\\theta)\\). (This model makes sense if you suppose that the person flipping the coin is able to flip the coin an even number of times more often than an odd number of times – or vice versa). \\(M_2\\): The data are sequentially dependent samples, the first one is H or with probability \\(\\theta_1\\), and \\(x_i\\) is the same as \\(x_{i-1}\\) with probability \\(\\theta_2\\), and different with probability \\((1-\\theta_2)\\). Which model is a better description of our data? Penalized (maximum) likelihood selection criteria First, let’s consider the Akaike information criterion (AIC) to choose a model. AIC is defined as: \\(\\operatorname{AIC} = 2m - 2 \\log\\left({\\mathcal{L}(D \\mid \\hat \\theta_{ML})}\\right)\\) Where \\(m\\) is the number of parameters (1 in \\(M_0\\) and \\(M_1\\), and 2 in \\(M_2\\)), \\(\\hat \\theta_{ML}\\) is the maximum likelihood estimate of those parameters; therefore, \\(\\mathcal{L}(D \\mid \\hat \\theta_{ML})\\) is the maximized likelihood of the data \\(D\\) under the given model. The better model is the one with the lowest AIC value. This model selection “criterion” has all the usual properties of model selection methods: it trades off model fit to the data (\\(- 2 \\log\\left({\\mathcal{L}(D \\mid \\hat \\theta_{ML})}\\right)\\)) while penalizing models with more parameters (\\(2m\\)). We will write out the likelihood functions for the three models. For \\(M_0\\): \\(\\mathcal{L}_0(X \\mid \\theta) = \\prod\\limits_{i=1}^n P(x_i \\mid \\theta) = \\theta^k(1-\\theta)^{n-k}\\), where \\(k\\) is the number of heads, and \\(n\\) is the length of the sequence. For \\(M_1\\): \\(\\mathcal{L}_1(X \\mid \\theta) = 0.5\\prod\\limits_{i=2}^n P(x_i \\mid \\theta,x_{i-1}) = 0.5\\theta^k(1-\\theta)^{n-k-1}\\), where \\(k\\) is the number of repetitions, and \\(n\\) is the length of the sequence. For \\(M_2\\): \\(\\mathcal{L}_2(X \\mid \\theta_1,\\theta_2) = P(x_1|\\theta_1)\\prod\\limits_{i=2}^n P(x_i \\mid \\theta_2,x_{i-1}) = \\theta_1^a(1-\\theta_1)^{1-a}\\theta_2^k(1-\\theta_2)^{n-k-1}\\), where \\(a=1\\) is the first coin of the sequence is heads, and 0 otherwise; \\(k\\) is the number of repetitions; and \\(n\\) is the length of the sequence. In the interest of space, I will not write out the exercise of finding the maximum likelihood parameters for these models, but they are: \\(M_0: \\hat \\theta_{ML} = 0.8\\); \\(M_1: \\hat \\theta_{ML} = 8/9 = 0.\\bar8\\); \\(M_2: \\hat \\theta_{ML} = [1.0, 0.\\bar8]\\) We can compute the AIC values for each model:\\ \\(\\operatorname{AIC}_0 = 2(1)-2\\log \\mathcal{L}_0(D \\mid \\hat \\theta_{ML}) = 2-2 \\log (0.00671) = 12\\) \\(\\operatorname{AIC}_1 = 2(1)-2\\log \\mathcal{L}_1(D \\mid \\hat \\theta_{ML}) = 2-2 \\log (0.02165) = 9.7\\) \\(\\operatorname{AIC}_2 = 2(2)-2\\log \\mathcal{L}_2(D \\mid \\hat \\theta_{ML}) = 4-2 \\log (0.0433) = 10.3\\) So of the three models proposed, \\(M_1\\) wins, in the sense that it has the lowest AIC value. So, our data are best described as sequentially dependent; however, the increase in maximized likelihood obtained from the more complicated \\(M_2\\) does not sufficiently offset the extra parameter. This is generally how penalized maximum likelihood model selection works (e.g., BIC differs only in that it has a slightly different magnitude of penalty for each additional parameter). Bayesian model selection (and Bayes Factors) Bayesian model selection works a bit differently than penalized maximum likelihood criteria. Instead of adopting an ad-hoc penalty for extra parameters, we will calculate the marginal likelihood of the model, by marginalizing over parameters. This requires specifying a prior about the parameters for each model. Specifically, we wish to calculate: \\(P(M_i \\mid D) = \\frac{P(D \\mid M_i)P(M_i)}{P(D)}\\) To do so, we need to calculate the marginal likelihood of the data, marginalizing over parameters: \\(P(D \\mid M_i) = \\int \\mathcal{L}_i(D \\mid \\theta,M_i) P(\\theta \\mid M_i) d\\theta\\) And to calculate this, we need to specify the prior distribution over model parameters: \\(P(\\theta \\mid M_i)\\) So let’s specify some very vague priors (uniform on all \\(\\theta\\)s): \\(P(\\theta \\mid M_i) = \\operatorname{Uniform}(0,1)\\) While we can calculate the marginal likelihood analytically, let’s do it numerically instead: p.theta = function(theta){dunif(theta,0,1)} d.theta = 0.001 theta = seq(0,1,by=d.theta) p.M = c(&quot;M0&quot;=1/3, &quot;M1&quot;=1/3, &quot;M2&quot;=1/3) p.theta.M0 = p.theta(theta) likelihood.M0 = function(theta){theta^8*(1-theta)^2} p.data.theta_M0 = vapply(theta, likelihood.M0, c(&quot;lik&quot; = 0)) p.data.M0 = sum(p.theta.M0*p.data.theta_M0*d.theta) p.theta.M1 = p.theta(theta) likelihood.M1 = function(theta){0.5*theta^8*(1-theta)^1} p.data.theta_M1 = vapply(theta, likelihood.M1, c(&quot;lik&quot; = 0)) p.data.M1 = sum(p.theta.M1*p.data.theta_M1*d.theta) p.theta_1.M2 = p.theta(theta) p.theta_2.M2 = p.theta(theta) p.theta.M2 = outer(p.theta_1.M2, p.theta_2.M2, function(a,b){a*b}) likelihood.M2 = function(theta_1, theta_2){theta_1^1*theta_2^8*(1-theta_2)^1} p.data.theta_M2 = outer(theta, theta, likelihood.M2) p.data.M2 = sum(p.theta.M2*p.data.theta_M2*d.theta*d.theta) p.data.M = c(&quot;M0&quot;=p.data.M0, &quot;M1&quot;=p.data.M1, &quot;M2&quot;=p.data.M2) p.data = sum(p.M*p.data.M) (p.M.data = p.M*p.data.M/p.data) ## M0 M1 M2 ## 0.1537821 0.4228975 0.4233204 This kinds of Bayesian model selection is hard because calculating \\(P(D|M)\\) – the likelihood of the data while marginalizing over possible parameter values under that model – is hard. Bayes Factors Often Bayesian model selection is used to compare just two hypotheses: some null \\(H_0\\), which has an assumption of an effect of 0, and some alternative \\(H_1\\) in which the effect size is a parameter. For instance, we might say that the two models are: \\(H_0\\): The data are independent samples from a Bernoulli distribution with \\(\\theta=0.5\\) (this is the null hypothesis postulating that the coin has zero bias). \\(H_1\\): The data are independent samples from a Bernoulli distribution with \\(\\theta\\) free to vary (this is the alternate hypothesis in which the coin can have some bias). The logic of Bayes factors is that if we consider the ratio of the posterior probability of \\(H_1\\) and \\(H_0\\), the calculation can be broken up into the ratio of the priors, and the ratio of the (marginal) likelihoods: \\[\\frac{P(H_1 \\mid D)}{P(H_0 \\mid D)} = \\frac{P(D \\mid H_1)}{P(D \\mid H_0)} \\frac{P(H_1)}{P(H_0)}\\] Since we want to know what kind of evidence in favor of \\(H_1\\) (or \\(H_0\\)) our data offer, we are only interested in the ratio of the marginal likelihoods. This ratio is called the Bayes factor: \\[\\operatorname{BF} = \\frac{P(D \\mid H_1)}{P(D \\mid H_0)} = \\frac{\\int \\mathcal{L}(D \\mid \\theta,H_1) P(\\theta \\mid H_1)d\\theta}{\\int \\mathcal{L}(D \\mid \\theta,H_0) P(\\theta \\mid H_0)d\\theta}\\] Again, this is generally hard to calculate, because it is hard to calculate the marginal likelihood. However, here we can do so numerically: d.theta = 0.001 theta = seq(0,1,by=d.theta) p.data.H0 = 0.5^8*0.5^2 p.theta.H1 = dunif(theta,0,1) likelihood.H1 = function(theta){theta^8*(1-theta)^2} p.data.theta_H1 = vapply(theta, likelihood.H1, c(&quot;lik&quot; = 0)) p.data.H1 = sum(p.theta.H1*p.data.theta_H1*d.theta) (bayesfactor = p.data.H1/p.data.H0) ## [1] 2.068687 This bayes factor (about 2), is generally not considered to be sufficiently strong evidence in favor of the alternate hypothesis (the prescribed cutoffs can be found here) t-distribution TL; DR. When using the sample standard deviation to define a test statistic, the sampling variability of the sample sd. will influence the distribution of the test statistic under the null. Consequently, we can’t use the normal distribution if we use the sample sd. Instead we use the t distribution, (dt, pt, qt, with the degrees of freedom used to estimate the sample sd) Sampling distribution of sample variance, and t-statistic Ok, so suppose we no longer know what the population standard deviation ought to be under the null hypothesis. This is nearly always the case in practice. For instance, we might measure the math GRE scores of folks in our class, and aim to test whether or not those GRE scores are distributed with a mean different from 500. We don’t know what the standard deviation ought to be, so we cannot use a Z test. Instead we must use a T-test. Sample variance Recall that the sample variance is defined as: \\(s^2_x = \\frac{1}{n-1}\\sum\\limits_{i=1}^n{(x_i-\\bar x)^2}\\) You would reasonably ask: why are we dividing by \\((n-1)\\)? The short answer: because if you used \\(n\\), your sample variance would tend to underestimate the population variance; however, with the \\((n-1)\\) correction, ensures that the sample variance is not biased. The longer answer: The sample variance is defined as the variance around the sample mean; however, the sample mean is calculated so as to minimize the sum of squared deviations of the data points from it; in other words, to minimize the sample variance. Therefore, without the \\((n-1)\\) correction, the sample variance would be biased by our calculation of sample mean. Since we compute the sample mean based on the data, and then use the same data to calculate the sample variance, our calculation of sample variance has fewer degrees of freedom than our calculation of the sample mean. The degrees of freedom refers to how much we have constrained our calculation. When we calculate the sample mean, we have not constrained any of the parameters of our calculation based on the data, therefore all the data points are “free to vary.” So for the calculation of sample mean, our degrees of freedom is the number of data points \\(df = n\\). However, when we calculate the sample variance, we have already calculated the sample mean, and the sample variance calculation is conditioned on the value of the sample mean (since we calculate squared deviations from the sample mean). Consequently, we have constrained the calculation by one parameter. This means that conditioned on our estimated sample mean, all but one of the data points are free to vary (in other words, the nth data point can be calculated from the first n-1 data points and the sample mean, so it is not free to vary) – thus we have \\(n-1\\) degrees of freedom, and using that in the calculation of the sample variance appropriately corrects the bias. The mathematical answer: If this text-based explanation seems convoluted and confusing, that is because it is. For those that are comfortable with math, the wikipedia page on Variance works through the calculation of the expected bias of the sample variance calculated using \\(n\\), and shows that using \\(n-1\\) yields an unbiased estimate of variance. Sampling distribution of the sample variance If we take multiple samples of size \\(n\\) from some population, and compute the sample mean of each, the sample mean will vary from one sample to the next. The variation of the sample mean across samples (when they all come from the same distribution) is known as the sampling distribution of the sample mean, and it follows a normal distribution with a standard deviation known as the “standard error of the mean” (see notes on normal statistics ). Similarly, the sample variance will vary from one sample to another. We need not worry now about the details of the sampling distribution of the sample variance, but it suffices to say: it will vary around the population variance. We can simulate the sampling distribution of the sample standard deviation: sample.n = function(n){rnorm(n, 100, 15)} df.sample.var = rbind(data.frame(sample.variance=replicate(10000,sd(sample.n(10))), n=&quot;10&quot;), data.frame(sample.variance=replicate(10000,sd(sample.n(50))), n=&quot;50&quot;), data.frame(sample.variance=replicate(10000,sd(sample.n(250))), n=&quot;250&quot;)) ggplot(df.sample.var, aes(x=sample.variance))+facet_grid(.~n)+ geom_histogram(fill=&quot;darkblue&quot;)+ geom_vline(xintercept=15, size=1, color=&quot;red&quot;)+ # true population variance. my_theme # consult visualization notes on how to make a plot theme. In other words, sometimes the sample standard deviation (\\(s_x\\)) will be bigger than the population standard deviation (\\(\\sigma_x\\)), and sometimes smaller. The smaller our sample size, the more variable the sample standard deviation will be around the population standard deviation. Sampling distribution of t-statistic Because the sample standard deviation is subject to sampling variability, if we calculate the equivalent of a Z-statistic, but using the sample standard deviation instead of the population standard deviation: \\(t_{\\bar x} = \\frac{(\\bar x-mu)}{s_x / \\sqrt n}\\) this statistic will not follow the standard normal (Z) distribution. This is entirely because our sample standard deviation also tends to vary from sample to sample. The sampling distribution of this “t” statistic reflects the variation of both the sample mean as well as the sample variance. The sampling distribution of the t statistic is effectively a weighted mixture of many gaussian distributions, each with a different standard deviation (reflecting the sampling distribution of the sample variance). This is known as the (Student’s) T distribution. And this is the distribution we will be using to calculate null hypothesis tests and confidence intervals in situations when we must estimated the population standard deviation from the sample. We can simulate the distribution of a t-statistic, and compare it to the standard normal (Z) distribution. mu.x = 100 # population value sd.x = 15 # population value n = 3 # small sample size to highlight t distribution tails sample.n = function(n){rnorm(n, mu.x, sd.x)} calculate.t = function(x){(mean(x) - mu.x)/(sd(x)/sqrt(length(x)))} # uses sample sd calculate.z = function(x){(mean(x) - mu.x)/(sd.x /sqrt(length(x)))} # uses population sd sample.ts = replicate(10000, calculate.t(sample.n(n))) sample.zs = replicate(10000, calculate.z(sample.n(n))) df = rbind(data.frame(value=sample.ts, statistic=&quot;T&quot;), data.frame(value=sample.zs, statistic=&quot;Z&quot;)) ggplot(df, aes(x=value, fill=statistic))+ geom_histogram(position=&quot;identity&quot;, binwidth=0.2, alpha=0.5)+ scale_x_continuous(limits=c(-5,5))+ my_theme # consult visualization notes on how to make a plot theme. Notice that with our small sample size (3), the tails of the t-distribution are considerably “heavier” than those of the standard normal distribution. With small ns, the standard deviation of the t distribution is larger than 1, and it has a kurtosis much greater than that of a normal. We can see the deviation of the T-distribution from a standard normal distribution much more clearly using QQ plots: qs = seq(0.01, 0.99, by=0.01) df.qs = data.frame(quantile.P = qs, q.val.Normal = qnorm(qs), q.val.t = quantile(sample.ts,qs), q.val.z = quantile(sample.zs,qs)) ggplot(df.qs, aes(q.val.Normal, q.val.z))+ geom_point(col=&quot;blue&quot;, cex=2)+ geom_line(col=&#39;blue&#39;, size=0.75)+ geom_point(aes(y=q.val.t), col=&quot;red&quot;, cex=2)+ geom_line(aes(y=q.val.t), col=&#39;red&#39;, size=0.75)+ geom_abline(position=&quot;identity&quot;)+ xlab(&quot;Standard normal quantile&quot;)+ ylab(&quot;Sampling dist of statistic quantile&quot;)+ my_theme The blue line corresponds to the qq-plot for sample z statistics, and the red line for sampled t statistics, compared to the standard normal distribution. This display highlights that the heavy tails of the sampling distribution of the t statistic have a huge impact on extreme quantiles: the z statistic corresponding to the 98th percentile is about qnorm(0.98)=2, but the corresponding 0.98 quantile for a t distribution with n=3 is about qt(0.98,2)=5. Likewise, while a statistic of 2 corresponds to the pnorm(2)=98th percentile of the z distribution, it is the pt(2,2)=91st percentile of the t distribution. Consequently, if we define a 95% confidence interval as \\(\\pm 1.96 s_{\\bar x}\\) (as we would using the z distribution), we only get a 1-2*pt(-1.96,2)=81% confidence interval out. Similarly, if we reject a t-statistic based on a critical z value chosen for \\(\\alpha=0.05\\), our false-alarm rate will actually be 19%. T-distribution Consequently, the sampling distribution of the t-statistic is not a standard normal, but is a T distribution with a “degrees of freedom” parameter: \\(df = \\nu = n-1\\) (this is pronounced “nu,” but normally we will just refer to it as degrees of freedom) We can show that the sampled t statistics follow this t-distribution by looking at the qq-plot comparing sampled t statistics to the theoretical t distribution: df = rbind(data.frame(value=sample.ts, what=&quot;sample t stats&quot;), data.frame(value=rt(length(sample.ts), n-1), what=&quot;t distribution samples&quot;)) g1 = ggplot(df, aes(x=value, fill=what))+ geom_histogram(position=&quot;identity&quot;, binwidth=0.2, alpha=0.5)+ scale_x_continuous(limits=c(-5,5))+ my_theme # consult visualization notes on how to make a plot theme. qs = seq(0.01, 0.99, by=0.01) df.qs = data.frame(quantile.P = qs, q.val.t.theoretical = qt(qs,n-1), q.val.t.samples = quantile(sample.ts,qs)) g2 = ggplot(df.qs, aes(q.val.t.theoretical, q.val.t.samples))+ geom_point(col=&quot;red&quot;, cex=2)+ geom_line(col=&#39;red&#39;, size=0.75)+ geom_abline(position=&quot;identity&quot;)+ my_theme library(gridExtra) grid.arrange(g1,g2,ncol=2) Degrees of freedom. As our sample size increases, our degrees of freedom increase. This means that the sampling distribution of the sample variance has less variability, and as a consequence, the t-distribution looks more normal. t = seq(-5,5,by=0.05) df = rbind(data.frame(t=t, pt=dnorm(t), df=&quot;Normal&quot;), data.frame(t=t, pt=dt(t,100), df=&quot;100&quot;), data.frame(t=t, pt=dt(t,30), df=&quot;30&quot;), data.frame(t=t, pt=dt(t,10), df=&quot;10&quot;), data.frame(t=t, pt=dt(t,3), df=&quot;5&quot;), data.frame(t=t, pt=dt(t,1), df=&quot;1&quot;)) ggplot(df, aes(x=t,y=pt,color=as.factor(df)))+ geom_line(position=&quot;identity&quot;, size=1)+ scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;magenta&quot;))+ my_theme As we see, when n is larger than about 30, the t distribution is pretty close to the normal distribution, so its deviation won’t matter for all but the most extreme quantiles. Summary. In short, when we use the sample standard deviation to calculate a statistic, (as we usually will), we use the t distribution with \\(n-1\\) degrees of freedom rather than the standard normal distribution. In R, we can get the T distribution’s density, cumulative probability, quantile, and random samples with dt(x,df), pt(x,df), qt(q,df), and rt(n,df), respectively. "],["t-tests.html", "(Student’s) t-tests 1-sample t-test Paired / repeated-measures t-test 2-sample, presumed equal variance, t-test 2-sample, unequal variance, t-test Power calculations. Summary of tests for the mean and effect sizes Math.", " (Student’s) t-tests We use t-tests when we calculate the standard error based on the sample standard deviation. The logic of why we must use the t-distribution in these cases is described when we consider the sampling distribution of the sample variance, and the t statistic. Read that first, and then we can quickly cover the slightly different t-test varieties we can use to compare sample means. 1-sample t-test So if we have a sample, which we assume to be normally distributed with unknown population mean and variance, and want to test whether that sample has a mean different from some null hypothesis mean (\\(\\mu_0\\)). We would proceed as follows: estimate the sample mean and sample variance, and then compute a t-statistic: \\(t_x = (\\bar x - \\mu_0) / (s_x / \\sqrt n)\\) Under the null hypothesis this t statistic will follow a t distribution with one parameter, the degrees of freedom: \\(df = n-1\\). The degrees of freedom parameter changes the t-distribution depending on our certainty in our estimated sample variance. If \\(n\\) is very large, then we should be very certain that our estimated sample variance will not be very different from the population variance. Consequently, the t-statistic when \\(n\\) is large will be distributed no differently than a z-statistic: it will follow a normal distribution. However, when \\(n\\) is small, we have substantial uncertainty about the estimated sample variance, so the distribution of the t-statistic will take on a different shape. Specifically, it will have heavier tails (it will have greater kurtosis), this should be intuitive because if we underestimate the population standard deviation, our t-statistic (which has estimated standard deviation in the denominator) might be quite large by chance – hence the heavier tails. Anyway, so to compute a p-value for a t-test, you would look up your t-statistic in the cumulative t-distribution (just as p-values for a z-test are obtained from a cumulative standard normal distribution), with \\(n-1\\) degrees of freedom: For a conventional two-tailed test, we would calculate the p-value as: 2*pt(-abs(tx),n-1) (notice that we are merely substituting pt for pnorm) If the degrees of freedom are small, the p value from the t distribution may be considerably larger than for a normal distribution: 2*pt(-abs(3),2)=0.095466 compare to 2*pnorm(-abs(3))=0.0026998 However, in R we can do this quickly using the t.test command. df = data.frame(our.iqs = rnorm(15, 103,15)) (test.results = t.test(df$our.iqs, mu = 100)) ## ## One Sample t-test ## ## data: df$our.iqs ## t = 0.69038, df = 14, p-value = 0.5012 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 92.81427 114.00755 ## sample estimates: ## mean of x ## 103.4109 We can access the various aspects of the test result as: test.results$statistic # the t statistic ## t ## 0.6903765 test.results$parameter # the degrees of freedom ## df ## 14 test.results$p.value # the p-value ## [1] 0.5012433 Paired / repeated-measures t-test In other cases we might have two measurements from a single source. The canonical example of this would be “before” and “after” measurements. e.g., for each person we measure their weight before they go on our new-fangled diet, and after 3 months of being on the diet, so for each person we have two measurements: before and after the 3 months of dieting. We might then want to see whether people lost weight, or gained weight, or didn’t change between the two tests. This scenario offers us an opportunity to reduce person-to-person variability by calculating the difference of before and after scores, and then calculating a 1-sample t-test on the difference, comparing them to a mean of 0: x = rnorm(10, 170, 10) df = data.frame(before = x+rnorm(10, 5, 2), after = x+rnorm(10, 0, 2)) df$difference = df$after - df$before (test.results = t.test(df$difference, mu=0)) ## ## One Sample t-test ## ## data: df$difference ## t = -6.029, df = 9, p-value = 0.0001954 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -6.967487 -3.165466 ## sample estimates: ## mean of x ## -5.066477 By calculating the before-after difference, we can effectively get rid of the across-subject variation, which will be manugest in both the before and after weights. This is the virtue of repeated-measures designs in general. For t-tests, such a design only makes sense if we have a “paired” design (such that our data come in pairs; here: before/after). 2-sample, presumed equal variance, t-test Another setting is that we have two independent samples (\\(x\\) and \\(y\\)) and want to know if they differ in their means, under the assumption that they have the same standard deviation. This kind of setting may arise if we have a sample of males and a sample of females, and want to see if their heights are systematically different: In R this can be run quickly as: df = rbind(data.frame(height = rnorm(10, 70, 5), sex=&quot;male&quot;), data.frame(height = rnorm(18, 65, 5), sex=&quot;female&quot;)) (test.results = t.test(df$height[df$sex==&quot;male&quot;], df$height[df$sex==&quot;female&quot;], var.equal = T)) ## ## Two Sample t-test ## ## data: df$height[df$sex == &quot;male&quot;] and df$height[df$sex == &quot;female&quot;] ## t = 3.0701, df = 26, p-value = 0.004961 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.12556 10.73818 ## sample estimates: ## mean of x mean of y ## 72.29764 65.86577 Note that: we must specify that we assume the variances are equal (the var.equal=T portion) the degrees of freedom are equal to \\(n_x + n_y -2\\). the standard error is calculated by pooling variance around the mean of x and the mean of y. 2-sample, unequal variance, t-test The final model we might have of our 2 samples of data is that they come from normal distributions with different means and (potentially) different variances, and we are interested in testing whether their means are equal. If we do not assume equal variances, we can run the “Welch’s t-test.” This is the default behavior of R’s t.test function: df = rbind(data.frame(val = rnorm(10, 105, 20), variable=&quot;x&quot;), data.frame(val = rnorm(18, 98, 10), variable=&quot;y&quot;)) (test.results = t.test(df$val[df$variable==&quot;x&quot;], df$val[df$variable==&quot;y&quot;])) ## ## Welch Two Sample t-test ## ## data: df$val[df$variable == &quot;x&quot;] and df$val[df$variable == &quot;y&quot;] ## t = 1.8044, df = 13.851, p-value = 0.09296 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.754139 20.230843 ## sample estimates: ## mean of x mean of y ## 109.08624 99.84789 Note that: we must specify that the default is to not assume the variances are equal (default is var.equal=F; this is the deault behavior in R) the degrees of freedom is some weird non-integer value It is worth getting an intuition for what the degrees of freedom are doing in a Welch’s t-test: The unequal variance degrees of freedom are somewhere between \\(n - 1\\) and \\(n_x + n_y -2\\), where \\(n\\) is the size of the sample with the larger standard error of the mean). What this does is return roughly the two-sample equal-variance degrees of freedom (\\(df = n_x + n_y -2\\)) when the standard errors are approximately equal (since in that case you are essentially doing an equal-variance t-test), and returns the one-sample degrees of freedom (\\(n-1\\)), when one variance is much larger than the other. The latter should make sense: one sample is much more variable than another, there is no sampling variability in the precise sample, so we are effectively comparing the noisy sample to a determinate value – a one-sample t-test. Power calculations. Instead of doing algebra to calculate power for a t-test, as we did for a normal, we will just use the pwr:: library. library(pwr) # get power for a particular sample size... # for a 1 sample (or repeated measures) t-test pwr::pwr.t.test(n = 20, d=0.5, sig.level=0.05, type = &#39;one&#39;) ## ## One-sample t test power calculation ## ## n = 20 ## d = 0.5 ## sig.level = 0.05 ## power = 0.5645044 ## alternative = two.sided # for a 2 sample t-test (equal sample sizes, each group has sample size n) pwr::pwr.t.test(d=0.5, n=10, sig.level=0.05, type=&#39;two&#39;) ## ## Two-sample t test power calculation ## ## n = 10 ## d = 0.5 ## sig.level = 0.05 ## power = 0.1850957 ## alternative = two.sided ## ## NOTE: n is number in *each* group # for a 2 sample t-test (unequal sample sizes) pwr::pwr.t2n.test(d=0.5, n1=10, n2=10, sig.level=0.05) ## ## t test power calculation ## ## n1 = 10 ## n2 = 10 ## d = 0.5 ## sig.level = 0.05 ## power = 0.1850957 ## alternative = two.sided # get sample size for a particular level of power # fir a 1 sample (or repeated measures) t-test pwr::pwr.t.test(d = 0.5, power = 0.8, sig.level=0.05, type = &#39;one&#39;) ## ## One-sample t test power calculation ## ## n = 33.36713 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided # for a 2 sample t-test (find one sample size for both groups) pwr::pwr.t.test(d=0.5, power=0.8, sig.level=0.05, type=&#39;two&#39;) ## ## Two-sample t test power calculation ## ## n = 63.76561 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Summary of tests for the mean and effect sizes Test test statistic d.f. effect size Z-test \\(z = \\frac{\\bar x - \\mu_0}{\\sigma_0 / \\sqrt n}\\) na \\(\\hat d = \\frac{\\bar x - \\mu_0}{\\sigma_0}\\) t-test \\(t = \\frac{\\bar x - \\mu_0}{s_x / \\sqrt n}\\) \\(n-1\\) \\(\\hat d = \\frac{\\bar x - \\mu_0}{s_x}\\) paired t-test \\(t = \\frac{\\bar d - \\mu_0}{s_d / \\sqrt n}\\) n-1 \\(\\hat d = \\frac{\\bar d - \\mu_0}{s_d}\\) eq. var t-test \\(t = \\frac{\\bar y - \\bar x}{s_p \\sqrt{1/n_x + 1/n_y}}\\) \\(n_x+n_y-2\\) \\(\\hat d = \\frac{\\bar y - \\bar x}{s_p}\\) uneq. var t-test \\(t = \\frac{\\bar y - \\bar x}{\\sqrt{s_x^2/n_x + s_y^2/n_y}}\\) ** \\(\\hat d = \\frac{\\bar y - \\bar x}{\\sqrt{s_x^2 + s_y^2}}\\) ** The degrees of freedom for an unequal variance t-test are given by the following long expression (which you should not try to learn): \\(df = \\frac{(s_x^2/n_x + s_y^2 / n_y)^2}{sx^4/(n_x^2(n_x-1)) + s_y^4/(n_y^2(n_y-1))}\\) Math. This section is here for thoroughness, but there is no particularly good reason to commit this stuff to memory. Math behind 2-sample equal variance t-test Specifically, our model is: \\(x \\sim \\text{Normal}(\\mu_x, \\sigma)\\) \\(y \\sim \\text{Normal}(\\mu_y, \\sigma)\\) Note that here we have assumed that x and y have the same standard deviation. For reasons that become clear later, this assumption is usually made when running a t-test, especially when calculating one by hand. Under this assumption, our expected distributions for the sample means will be: \\(\\bar x \\sim \\text{Normal}(\\mu_x, \\sigma / (\\sqrt n_x))\\) \\(\\bar y \\sim \\text{Normal}(\\mu_y, \\sigma / (\\sqrt n_y))\\) And the distribution of the difference between the means (\\(\\bar y - \\bar x\\)), will be: \\((\\bar y - \\bar x) \\sim \\text{Normal}(\\mu_y - \\mu_x, \\sqrt{\\frac{\\sigma^2}{n_x} + \\frac{\\sigma^2}{n_y}})\\) (the random variable corresponding to the difference of \\(\\bar x\\) and \\(\\bar y\\) will have a variance equal to the sum of the variances \\(\\sigma^2_{\\bar x} = \\sigma^2/n_x\\) and \\(\\sigma^2_{\\bar y} = \\sigma^2/n_x\\)) The standard deviation of the sampling distribution of the difference of the means (this is the “standard error” of the difference between two means), \\(\\sqrt{\\frac{\\sigma^2}{n_x} + \\frac{\\sigma^2}{n_y}}\\), can be simplified to: \\(\\sigma \\sqrt{1/n_x + 1/n_y}\\). In this case, as usual, we want to test a null hypothesis, and here the null hypothesis is specified as the difference between the two means \\(H_0 = (\\mu_{y_0} - \\mu_{x_0})\\). This is usually 0: our null hypothesis is usually that the two means do not differ. Since we do not know the shared standard deviation, \\(\\sigma\\), we have to estimate it. To do so, we have to “pool” information from both \\(x\\) and \\(y\\), since they should both inform our estimate of the shared \\(\\sigma\\). We can compute \\(\\bar x\\) and \\(\\bar y\\), as well as \\(s_x\\) and \\(s_y\\), however, our estimate of the common \\(\\sigma\\) must pool \\(s_x\\) and \\(s_y\\). This pooled estimate is usually written as \\(s_p\\), and is a weighted average of sample variances: \\(s_p^2 = \\frac{\\sum_i (n_i - 1) s_i^2}{\\sum_i (n_i -1)}\\) Since we only have two groups here, this general statement about how to pool variances boils down to: \\(s_p^2 = \\frac{(n_x-1) s^2_x + (n_y-1) s^2_y}{n_x + n_y - 2}\\) (Note that these equations refer to pooled variance, so the pooled standard deviation would be the square root of these!) Since this is our estimated variance, we can use the standard error formula we derived before to compute the estimated standard error of the difference between two means: \\(s_{\\bar y - \\bar x} = s_p \\sqrt{1/n_x + 1/n_y}\\). Once we have obtained a standard error, we can compute a t-statistic. \\(t_{\\bar y - \\bar x} = \\frac{(\\bar y - \\bar x) - (\\mu_{y0} - \\mu_{x0})}{s_{\\bar y - \\bar x}}\\) Sometimes, for clarity, this is written out as the equivalent expression: \\(t_{\\bar y - \\bar x} = \\frac{(\\bar y - \\bar x) - (\\mu_{y0} - \\mu_{x0})}{s_p \\sqrt{1/n_x + 1/n_y}}\\) Usually, the null hypothesis \\((\\mu_{y0} - \\mu_{x0})\\) is 0: meaning that the difference between the means ought to be 0, so the t-statistic ends up being shortened to: \\(t_{\\bar y - \\bar x} = \\frac{(\\bar y - \\bar x)}{s_p \\sqrt{1/n_x + 1/n_y}}\\) This t-statistic will follow the t-distribution with degrees of freedom matching the denominator in the pooled sample variance calculation: \\(df = n_x + n_y -2\\) Special case of equal sample sizes Sometimes textbooks present special formulas for cases where \\(n=n_x=n_y\\) (where the sample sizes are equal), but those formulas can be calculated directly from the more general formulas that assume unequal sample sizes by plugging \\(n\\) in for both \\(n_x\\) and \\(n_y\\): \\(df = 2n-2\\) \\(s_p^2 = (s_x^2 + s_y^2)/2\\) \\(s_{\\bar y - \\bar x} = s_p \\sqrt{2/n}\\) Math behind unequal variance t-test Specifically, the model we consider is: \\(x \\sim \\text{Normal}(\\mu_x, \\sigma_x)\\) \\(y \\sim \\text{Normal}(\\mu_y, \\sigma_y)\\) Under this assumption, our expected distributions for the sample means will be: \\(\\bar x \\sim \\text{Normal}(\\mu_x, \\sigma_x / (\\sqrt n_x))\\) \\(\\bar y \\sim \\text{Normal}(\\mu_y, \\sigma_y / (\\sqrt n_y))\\) And the distribution of the difference between the means (\\(\\bar y - \\bar x\\)), will be: \\((\\bar y - \\bar x) \\sim \\text{Normal}(\\mu_y - \\mu_x, \\sqrt{\\frac{\\sigma_x^2}{n_x} + \\frac{\\sigma_y^2}{n_y}})\\) So we can proceed to calculate \\(\\bar x\\), \\(\\bar y\\), \\(s_x\\), \\(s_y\\), and plug these in to get a t-statistic: \\(t_{\\bar y - \\bar x} = \\frac{(\\bar y - \\bar x)}{\\sqrt{s_x^2/n_x + s_y^2/n_y}}\\). (Again we have dropped the $ - (\\(\\mu_{y0} - \\mu_{x0}\\))$ from the numerator because in most cases in which this test is applied \\((\\mu_{y0} - \\mu_{x0})=0\\).) However, the trickiness comes in defining the degrees of freedom for this t-test. The trickiness arises from the fact that we have two estimated standard deviations; how do we represent the sampling variability associated with both? This was a hairy problem when folks were first dealing with t-tests, until the reasonably accurate approximation via ``Welch’s t-test\", which defines the degrees of freedom for this test statistic as: \\(df = \\frac{(s_x^2/n_x + s_y^2 / n_y)^2}{sx^4/(n_x^2(n_x-1)) + s_y^4/(n_y^2(n_y-1))}\\) This unwieldy definition of the degrees of freedom generates all the desirable properties for the t-test. Intuitively, what this formula does is return \\(n -1 \\leq df \\leq n_x + n_y -2\\) (where \\(n\\) is the size of the sample with the larger sample variance). Intuitively, what this does is return roughly \\(df = n_x + n_y -2\\) when \\(s_x\\) and \\(s_y\\) are approximately equal (since in that case you are essentially doing an equal-variance t-test), and this formula returns \\(n_x-1\\) when \\(s_x\\) is much larger than \\(s_y\\) (and vice versa; this should make sense: when \\(s_x\\) is really big compared to \\(s_y\\), this t-test is equivalent to doing a 1-sample t-test of \\(x\\) vs the point-null hypothesis of \\(\\mu_0 = \\mu_y\\)). "],["binomial-test.html", "Binomial test. Estimating proportions. Sign test, test for percentiles", " Binomial test. If we have two possible categories (e.g., “pass”/“fail,” “female”/“male,” etc.), we can use the Binomial test to see if their proportions deviate from some null hypothesis proportion. We talked about this via problem via sampling, but that served to explain the logic, rather than provide practical advice. We also talked about this in the advanced notes going from probability to statistics via the binomial, but that section was broad, mathy and complicated. Here let’s just be practical. If you have binary categorical data: df = data.frame(sex = sample(c(&#39;male&#39;, &#39;female&#39;), size = 50, replace = T, prob = c(0.6, 0.4))) table(df$sex) ## ## female male ## 22 28 And we want to know if the rates of one outcome deviate from some null hypothesis probability H0.p.female = 0.52 We can run a binomial test: binom.test(x=sum(df$sex==&quot;female&quot;), # number of one of two outcomes. nrow(df), # total number of observations p = H0.p.female) # probability of that outcome under the null ## ## Exact binomial test ## ## data: sum(df$sex == &quot;female&quot;) and nrow(df) ## number of successes = 22, number of trials = 50, p-value = 0.2618 ## alternative hypothesis: true probability of success is not equal to 0.52 ## 95 percent confidence interval: ## 0.2999072 0.5874559 ## sample estimates: ## probability of success ## 0.44 Thus we test the null hypothesis that our counts deviate from some null probability, using the Binomial distribution. Estimating proportions. If we have some categorical observations, generally we will want to report the proportion of different outcomes. How do we estimate this proportion and put confidence intervals on it? I think it’s useful to learn the confidence interval defined by the Normal approximation to the binomial proportion (although there are many other alternatives). If I flip \\(n\\) coins and get \\(k\\) heads, my estimated probability that we will get heads is \\(\\hat p = k/n\\). The standard error of \\(\\hat p\\) will be \\(s_{\\hat p}=\\sqrt{\\hat p (1-\\hat p) / n}\\) If our sample size (\\(n\\)) is large, and \\(p\\) is not too close to 0 or 1, \\(\\hat p\\) will follow a normal distribution with standard deviation equal to the standard error of \\(\\hat p\\). Consequently, we can use z-scores to put confidence intervals on \\(p\\): \\[\\hat p \\pm z_{\\alpha/2} \\sqrt{\\frac{1}{n} \\hat p (1-\\hat p)}\\] This Z-score based confidence interval on a proportion is sufficient for most purposes. It is fairly easy to calculate in R: k.women = 14 n.people = 23 p.hat = k.women/n.people q = 0.95 # 95% confidence interval se.p.hat = sqrt(p.hat*(1-p.hat)/n.people) z.crit = abs(qnorm((1-q)/2)) (CI.p.hat = p.hat + c(-1,1)*z.crit*se.p.hat) ## [1] 0.4092422 0.8081491 There are many suggested intervals for the binomial proportion, and the binom.test gives you a different interval than the one calculated via the normal approximation (it uses Clopper &amp; Pearson). The interval based on the Normal distribution seems the easiest to use quickly for rough calculations, so it seems more useful to know exactly. Sign test, test for percentiles The binomial test is also useful to test for a specific quantile (usually the median), in numerical data. Let’s say we have some weird looking data on changes in performance (delta): df = data.frame(delta = c(-1000, rnorm(20,1.5,1))) ggplot(df, aes(x=delta))+geom_histogram()+my_theme Note that the crazy outlier makes us unable to detect that improvement is actually positive: t.test(df$delta, mu=0) ## ## One Sample t-test ## ## data: df$delta ## t = -0.96728, df = 20, p-value = 0.345 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -145.62094 53.35413 ## sample estimates: ## mean of x ## -46.1334 Testing for the mean, using the standard deviation is very sensitive to outliers. Testing for the median, on the other hand, is not. We can test to see if the median is 0 by calculating the proportion of improvement scores that are greater than 0. If 0 is really the median, this should be about half. Thus we have a binomial test situation: what proportion of scores are greater than zero, compared to the null distribution of them being binomially distributed with probability 0.5 binom.test(x=sum(df$delta &gt; 0), # number greater than null median n=sum(df$delta != 0), # number that are not equal to zero -- should be all p=0.5) # null distribution of fraction of data that should be above the mean. ## ## Exact binomial test ## ## data: sum(df$delta &gt; 0) and sum(df$delta != 0) ## number of successes = 18, number of trials = 21, p-value = 0.00149 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.636576 0.969511 ## sample estimates: ## probability of success ## 0.8571429 Note that we count as the possible total number all values that are not equal to the hypothetical median. Generally, with continuous data, you will never get numbers exactly equal to the null median, but if you do, you should not count them as being either above or below the median (thus they get dropped from the binomial test analysis). This use of a binomial test for a particular percentile is often called a sign test, and can be generalized to test for percentiles other than the median (e.g., the 75th percentile), although I never see them used that way. "],["chi-squared.html", "Pearson’s Chi-squared test “Goodness of fit” test Test for independence More than 2-way contingency table Mathematical rationale Limitations Fisher’s “exact” test", " Pearson’s Chi-squared test If we have counts in two categories (e.g. heads or tails; women or men; democrat or republican; etc.) we can test whether the distribution of observations deviates from a null hypothesis distribution via a Binomial test (where the null hypothesis specifies the probability/proportion of “success” (e.g., heads/women/democrats/whatever): the parameter \\(\\theta\\) or \\(p\\)). But what if we have counts in more than two categories? For instance, if we don’t have a bipartisan system, but have, say “Tory,” “Labour,” and “Liberal democrats” (as in the UK some time ago), and want to test the distribution of these political parties to some null hypothesis about the proportion of each that we expect to see (e.g., p=1/3 for all three). I this case, we can no longer do a Binomial test, and we turn instead to (Karl) Pearson’s Chi-squared test (which I will often write as \\(\\chi^2\\)). There are two ways that the Chi-squared test is used: comparing the observed distribution to some theoretical distribution pre-specified ahead of time: to test the “Goodness of fit” of the theoretical distribution to the observations; testing for “independence” between different factors (which, technically, is just a specific theoretical distribution, with some extra parameters that must be estimated from the data). We will first talk about the practicalities of running a chi-squared test, and then we will talk about the probabilistic rationale for the test, and when that rationale breaks down (and what to do about it). “Goodness of fit” test The simplest use of Chi-squared tests is to compare some observed counts to an expected distribution of proportions to assess the “goodness of fit” of the theoretical distribution with the observations. Let’s say we survey students about their political affiliation. We find: 30 Democrats, 8 Republicans, and 15 Independents. We want to compare to a theoretical (null hypothesis) distribution; according to this null hypothesis, 50% should be Democrats, 30% should be republicans, and 20% should be independents. Implementation in R In R we can do this via the chisq.test function, although interfacing with this function can be a bit awkward. df = data.frame(party = c(replicate(30, &quot;democrat&quot;), replicate(8, &quot;republican&quot;), replicate(15, &quot;independent&quot;))) table(df$party) ## ## democrat independent republican ## 30 15 8 # the null hypothesis distribution of categories. (H0.p = c(&quot;democrat&quot;=0.5, &quot;republican&quot;=0.3, &quot;independent&quot;=0.2)) ## democrat republican independent ## 0.5 0.3 0.2 What we want to do is provide the table summary of the observed categories, and the vector of null hypothesis probabilities to chisq.test; however, we must make sure that their order is the same (which they are not now!). We can do this by indexing with a sorted name vector: sort(names(H0.p)) ## [1] &quot;democrat&quot; &quot;independent&quot; &quot;republican&quot; (sorted.data.table = table(df$party)[sort(names(H0.p))]) ## ## democrat independent republican ## 30 15 8 (sorted.H0.p = H0.p[sort(names(H0.p))]) ## democrat independent republican ## 0.5 0.2 0.3 chisq.test(sorted.data.table, p=sorted.H0.p) ## ## Chi-squared test for given probabilities ## ## data: sorted.data.table ## X-squared = 6.2138, df = 2, p-value = 0.04474 What we get out here is a \\(\\chi^2\\) statistic, a degrees of freedom, and a p-value. To understand what these things are, we need to work through the calculations behind this test. Chi-squared test calculations The \\(\\chi^2\\) statistic is defined as: \\[\\chi^2 = \\sum\\limits_{i=1}^c \\frac{(O_i-E_i)^2}{E_i}\\] where \\(c\\) is the number of columns (here 3: democrats, republicans, independents); \\(O_i\\) is the observed count in a given cell; \\(E_i\\) is the expected count: \\(E_i = p_i * n\\), where \\(n\\) is the total count from all cells, and \\(p_i\\) is the theoretical expected probability in a given cell. We can reproduce this calculation in R: o = sorted.data.table p = sorted.H0.p e = p*sum(o) (chi.sq.statistic = sum((o-e)^2/e)) ## [1] 6.213836 This chi-squared statistic corresponds to the sum of the squared deviations of the obesrved cell counts from the expected (under the null hypothesis) cell counts. Bigger deviations mean that our data are further from the null hypothesis prediction. Below we show this calculation in tabular form, to clarify how it all works. Democrats Republicans Independents cell (\\(i\\)) 1 2 3 Observations (\\(O_i\\)) \\(O_1 = 30\\) \\(O_2 = 8\\) \\(O_3 = 15\\) Null (\\(H_0\\) \\(p_1 = 0.5\\) \\(p_2 = 0.3\\) \\(p_3 = 0.2\\) n = 53 Expectation \\(E_1 = n*p_1 = 26.5\\) \\(E_2 = n*p_2 = 15.9\\) \\(E_3 = n*p_3 = 10.6\\) \\((O_i-E_i)\\) 3.5 -7.9 4.4 \\((O_i-E_i)^2\\) 12.25 62.41 19.36 \\(\\frac{(O_i-E_i)^2}{E_i}\\) 0.462264151 3.925157233 1.826415094 \\(\\chi^2 = \\sum_i \\frac{(O_i-E_i)^2}{E_i}\\) \\(\\chi^2 = 6.21383648\\) df = c-1 df=3-1=2 The degrees of freedom (df) is given by the number of observed counts (3 here), minus the number of parameters estimated from the data to calculate the expected counts, here just one parameter: n, the total count. Consequently, the degrees of freedom will be \\(df=c-1=2\\) Now that we have our \\(\\chi^2\\) statistic, we can compare it to the null hypothesis \\(\\chi^2\\) distribution with those degrees of freedom, to see if we have sufficiently large deviations from the null hypothesis. Specifically, we do a 1-tail test: looking only at the right tail. We can calculate the p-value in R as: df = length(sorted.data.table)-1 (p.value = 1-pchisq(chi.sq.statistic,df)) ## [1] 0.04473862 Alternatively, we could do the same chi-squared test by finding the critical chi-squared value to reach a particular level of \\(\\alpha\\): alpha = 0.05 (chi.sq.crit = qchisq(1-alpha,df)) ## [1] 5.991465 We can look at a plot of what all of this is doing by looking at the corresponding chi-squared distribution: chi.vals = seq(0,10,by=0.01) df = data.frame(chi=chi.vals, p.chi = dchisq(chi.vals, df), more.extreme = ifelse(chi.vals&gt;=chi.sq.statistic, &quot;more extreme than observed&quot;, &quot;less extreme than observed&quot;)) ggplot(df, aes(x=chi, y=p.chi, fill=more.extreme))+ geom_area()+ geom_vline(xintercept=chi.sq.statistic, color=&quot;blue&quot;, size=2)+ geom_vline(xintercept=chi.sq.crit, color=&quot;black&quot;)+ my_theme We see that our test statistic (6.21; blue line) is greater than the critical value (5.99; black line); or equivalently, the p-value (area under curve beyond our statistic) is smaller than \\(\\alpha\\), meaning that seeing a chi-squared statistic at least as large as ours from samples from the null hypothesis is sufficiently small for us to reject the null hypothesis. So we do. In other words, we reject the null hypothesis if our observed counts deviate more from the theoretically expected proportions than could be expected by chance. So, we would interpret this result as the distribution of political affiliations in the students we sampled does not follow our theoretical distribution. The chi-squared test when used to assess whether observed counts deviate from the null is always one tail – the upper tail. However, the lower tail can be used to test whether there is too little variation around the theoretical distribution (this is rarely done in practice, but may be a useful test to see if data were faked, or if there is voter fraud). Test for independence A very common extension of the “goodness of fit” test is to assess independence in a contingency table. For instance, lets say we gathered something like the following data (counting how many men and how many women reported a particular color as their favorite – out of a small set): Red Green Blue Black Men 4 2 9 2 Women 3 9 3 4 A chi-squared independence test asks whether the rate at which different colors are selected varies between men and women (or equivalently, whether the population of people who prefer different colors constitutes different proportions of men and women). Implementation in R First let’s make a data frame of the sort that we would have for these data: df = rbind(data.frame(sex=&quot;male&quot;, color=&quot;red&quot;)[replicate(4, 1),], data.frame(sex=&quot;male&quot;, color=&quot;green&quot;)[replicate(2, 1),], data.frame(sex=&quot;male&quot;, color=&quot;blue&quot;)[replicate(9, 1),], data.frame(sex=&quot;male&quot;, color=&quot;black&quot;)[replicate(2, 1),], data.frame(sex=&quot;female&quot;, color=&quot;red&quot;)[replicate(3, 1),], data.frame(sex=&quot;female&quot;, color=&quot;green&quot;)[replicate(9, 1),], data.frame(sex=&quot;female&quot;, color=&quot;blue&quot;)[replicate(3, 1),], data.frame(sex=&quot;female&quot;, color=&quot;black&quot;)[replicate(4, 1),]) str(df) ## &#39;data.frame&#39;: 36 obs. of 2 variables: ## $ sex : chr &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; ... ## $ color: chr &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; ... table(df$sex, df$color) ## ## black blue green red ## female 4 3 9 3 ## male 2 9 2 4 The independence test takes the table of counts, and returns a chi-squared value to measure how much the observed counts deviate from those expected if the rates were independent. chisq.test(table(df$sex, df$color)) ## ## Pearson&#39;s Chi-squared test ## ## data: table(df$sex, df$color) ## X-squared = 8.1782, df = 3, p-value = 0.04247 Understanding how this test is carried out requires that we talk about the calculations that go into it: Independence test calculations Red Green Blue Black Men 4 2 9 2 Women 3 9 3 4 We will refer to these observations as \\(O_{i,j}\\) where \\(i\\) indexes the row, and \\(j\\) indexes the column. So the count of women who reported blue as their favorite color would be \\(O_{2,3}=3\\). The total count is the sum of the counts in all cells: \\(n = \\sum_{i=1}^r \\sum_{j=1}^c O_{i,j}\\) here \\(n=36\\). (\\(r\\) refers to the number of rows – here 2; \\(c\\) refers to the number of columns – here 4). Independence is a theoretical claim about the contingency table. Using the language of probability, it says that the probability of a conjunction should be equal to the product of the marginal probabilities. In other words: \\(P(A=a, B=b) = P(A=a)P(B=b)\\). With respect to our contingency table: \\(P(G=\\text{men }, C=\\text{red}) = P(G=\\text{men})P(C=\\text{red})\\) To calculate the expected counts under the null hypothesis of independence, we first estimate the marginal probabilities (e.g., P(men), P(women), etc.). We calculate two sets of marginal probabilities, one for the rows, another for the columns (in theory, we could have a higher dimensional contingency table; but in practice they are usually limited to 2D row x column arrangements). We can calculate the marginal probabilities by summing over rows or columns, respectively: \\(p_{i,\\cdot} = \\sum_{j=1}^c O_{i,j} / n\\) (e.g., \\(P(men) = p_{1,\\cdot} = (4+2+9+2)/36)\\)) \\(p_{\\cdot,j} = \\sum_{i=1}^r O_{i,j} / n\\) (e.g., \\(P(green) = p_{\\cdot,2} = (2+9)/36)\\)) The “expected” count in each cell will be \\(E_{i,j} = n p_{i,\\cdot} p_{\\cdot,j}\\). The \\(p_{i,\\cdot} p_{\\cdot,j}\\) is the theoretical prediction of independence, and we multiply by \\(n\\) to get the expected count. Shown in the table below. \\(E_{i,j}\\) Red Green Blue Black \\(P_{i,\\cdot}\\) Men 3.3 5.2 5.7 2.8 0.472 Women 3.7 5.8 6.3 3.2 0.528 \\(P_{\\cdot,j}\\) 0.19444 0.30555 0.333 0.1667 Most books skip the step of calculating marginal probabilities for chi-squared tests for independence, and instead expected counts are calculated directly. Note that the expressions are algebraically equivalent. I prefer to expose the step of calculating marginal probabilities for the sake of displaying the logic of the test. \\(E_{i,j} = \\frac{\\sum_{a=1}^r O_{a,j} \\sum_{b=1}^c O_{i,b}}{n}\\) This can be read as (number of observations in the column)*(number of observations in the row)/(number of observations in the total table). We can now calculate the deviation of each cell: \\(\\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\) \\(\\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\) Red Green Blue Black Men 0.15 1.96 1.96 0.25 Women 0.13 1.76 1.75 0.22 The test statistic is the sum of all of these deviations: \\(\\chi^2 = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\) In our case the test statistic will be: \\(\\chi^2 = 8.18\\). The degrees of freedom are again, the number of cells (here 8) minus the number of parameters we estimated from the data: 5 (\\(n\\), \\(p_{\\cdot,1}\\), \\(p_{\\cdot,2}\\), \\(p_{\\cdot,3}\\), and \\(p_{1,\\cdot}\\)). (note \\(p_{\\cdot,4}=1-p_{\\cdot,1}-p_{\\cdot,2}-p_{\\cdot,3}\\), and \\(p_{2,\\cdot}=1-p_{1,\\cdot}\\), so they do not count as extra estimated parameters.) So our \\(df = 8-5 = 3\\). The shortcut to estimating the degrees of freedom for a test for independence is: \\(df = (r-1)(c-1)\\); which will always give us the same answer (here 3). We can calculate the p-value in R as: 1-pchisq(w,df) where \\(w=\\chi^2\\) – our test statistic, and \\(df\\) are our degrees of freedom. In our case: p-value = 1-pchisq(8.18,3) = 0.0441 We can also calculate a critical \\(\\chi^2\\) value, as we did before. In short, the chi-squared independence test is the same thing as a goodness of fit test, except the null hypothesis does not specify all the probabilities, but instead specifies the relationship between the marginal and joint probabilities. Consequently, to get expected counts, we have to estimate not only the total count (\\(n\\)) from the data, but also the marginal probabilities. This changes how we calculate both the expected counts as well as the degrees of freedom. More than 2-way contingency table Chi-squared tests of independence for 2-way contingency tables are by far the most common, but in principle, we can scale the same logic up to contingency tables with more than 2 factors. In practice, I have never seen this done, because (a) it is a test for complete independence of all factors, and a rejection of such a claim of independence is hard to interpret, so people are more interested in the independence of two factors in isolation, and (b) this functionality is rarely built into statistics software. Mathematical rationale The Chi-squared test is based on the \\(\\chi^2\\) distribution, and the Normal approximation to the binomial distribution. The binomial distribution of the number of successes \\(k\\) out of \\(n\\) attempts (where each attempt \\(w_i\\) is independent and has probability of success \\(p\\)), is written as \\(k \\sim \\text{Binomial}(n,p)\\). If we label each success as 1, and each failure as 0, then \\(k = \\sum_{i=1}^n w_i\\). The central limit theorem says that when \\(n\\) is sufficiently large, we can approximate the distribution of \\(k\\) as a Normal distribution with mean \\(np\\) and variance \\(np(1-p)\\) (provided some continuity correction). If \\(x_i \\sim \\text{Normal}(0,1)\\), then the sum of \\(m\\) squared x values is distributed as a chi-squared distribution with \\(n\\) degrees of freedom: \\(\\sum_{i=1}^m x_i^2 \\sim \\chi^2(m)\\). In other words, the chi-squared distribution is the distribution of \\(m\\) squared z-scores. The Chi-squared test relies on the normal approximation to the binomial to calculate the z-score of the deviation of the count in each cell compared to that expected under the null hypothesis; then sums these z-scores (this isn’t quite right, but it conveys the intuition). If the null hypothesis is true, then this sum of z-scored deviations ought to follow a Chi-squared distribution, if the observed statistic is very large compared to the null hypothesis Chi-squared distribution then we can reject the null hypothesis. (for a more detailed mathematical description of how these notions combine, take a look at the wikipedia page. Limitations The chi-squared test relies on the normal approximation to the Binomial distribution of counts within each cell; the normal approximation holds when the observed and expected counts are large, but breaks down when they are small. I have seen a number of rules of thumb for “how large should the counts be to use a chi-squared test?” In practice, it seems that folks use the chi-squared test regardless of those rules of thumb. You can look at some of these rules of thumb on wikipedia. Wikipedia suggests using the Yates correction to alter the statistic (when cell counts are too small) to: \\(\\chi^2_{\\text{Yates}} = \\sum_{i=1}^c \\frac{(|O_{i} - E_{i}|-0.5)^2}{E_{i}}\\) Using Yate’s correction is always more conservative, meaning that the probability of Type I error is lower for a given \\(\\alpha\\); this is desirable when counts are low because without the correction the Type I error may end up higher than \\(\\alpha\\). (as a consequence, Yate’s correction makes Type II error higher, and thus lowers power). The statement “the probability of Type I error is lower for a given \\(\\alpha\\)” may be puzzling, because we usually say that \\(\\alpha\\) is the probability of Type I error. However, this is only true in theory: when all the assumptions of a given test/model are met. When the assumptions of the test/model are not met, the test statistic does not follow the exact hypothetical distribution, and therefore the Type I error rate ends up different from \\(\\alpha\\) – sometimes lower, sometimes higher. Obviously, the deviations from assumptions that increase Type I error rates compared to \\(\\alpha\\) are those that we must be more careful of. In practice, if you have particularly small counts and need to test against some theoretical distribution, I suggest doing resampling to calculate the exact null hypothesis distribution, and compute a p-value without relying on the questionable normal approximation under low cell counts (we will cover variations of this technique in the second semester). Fortunately, R’s chisq-test function warns you when cell counts are small, and suggests doing exactly this. To carry out this Monte Carlo simulation to get a p-value without assuming asymptotically normal behavior from small cell counts, set simulate.p.value=TRUE in chisq.test. chisq.test(table(df$sex, df$color), simulate.p.value = T) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 2000 replicates) ## ## data: table(df$sex, df$color) ## X-squared = 8.1782, df = NA, p-value = 0.04698 Fisher’s “exact” test Another variant of the simulation-based chi-squared test for independence that does not rely on a normal approximation is Fisher’s exact test. In R, for a 2x2 contingency table, fisher.test() runs the exact test for independence (based on the hypergeometric distribution), for larger NxM tables it calculates a simulation based probability that these data could have been generated if the categorical variables are indeed independent. Generally, fisher.test is only used if you have a 2x2 table, and even then, rarely. fisher.test(table(df$sex, df$color)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(df$sex, df$color) ## p-value = 0.04316 ## alternative hypothesis: two.sided (Note that the chi-squared distribution based test, the chi-squared simulated p value, and the simulated p value from the fisher test function all give us slightly different p-values! None of them is more correct than another; I prefer to use the simulated chi-squared test by default.) "],["bivariate.html", "Bivariate linear relationships Linear relationships Covariance and correlation: Measuring the linear dependence. (OLS) Regression: Predicting the mean of y for a given x Partitioning variance Significance of a linear relationship. Prediction from regression. Anscombe’s quartet Covariance Estimating covariance. Correlation Ordinary Least-Squares (OLS) Regression y~x vs x~y vs principle component line Partitioning variance and the coefficient of determination. Significance of linear relationship. Regression prediction. Regression Diagnostics", " Bivariate linear relationships When we have two variables measured per “unit” (e.g., measure height and weight for each person), we can refer to this as “bivariate” data. We already discussed how to analyze contingency tables when dealing with bivariate categorical data, here we are concerned with bivariate numerical data that may have a linear relationship. To play with these measures, we will consider Karl Pearson’s data on the heights of fathers and their sons. (In 1895 Pearson worked out the formula for calculating what we now call the correlation coefficient.) Generally, we will look at a bivariate numerical relationship in a scatterplot, like the one below. library(tidyverse, quietly=TRUE) heights &lt;- read_csv(&#39;http://vulstats.ucsd.edu/data/Pearson.csv&#39;) heights %&gt;% ggplot(aes(x=Father, y=Son))+ geom_point() Linear relationships Everything we discuss in this section is specific to measuring a linear relationship, meaning (informally) that the scatterplot looks like it would be meaningful to draw a line through it. This has two implications. Lots of patterns of bivariate data have a relationship between x and y (meaning that we can learn something about y by knowing x, or vice versa), but that relationship is not linear. The bottom row of the graph below (from wikipedia) does a great job showing many such cases. Many different patterns of data might yield exactly the same line, and line statistics, and our simple measures of a linear relationship will not be able to distinguish among them. The canonical example of this is Anscombe’s quartet (below; graph it yourself), which shows 4 sets of data with obviously very different relationships, but which have the same correlation, covariance, linear slope, as well as marginal means and standard deviations of x and y. What this means, is that you should always look at a scatterplot of the data, don’t just blindly rely on the numerical summaries. Covariance and correlation: Measuring the linear dependence. Covariance The co-variance measures whether, when x is bigger than the mean of x, is y also bigger than the mean of y, and what is the overall scale of this co-variation? \\[\\operatorname{covariance}(x,y) = s_{xy} = \\frac{1}{n-1}\\sum\\limits_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)\\] To get an intuition for what this is doing, consider when the expression being summed (\\((x_i - \\bar x)(y_i - \\bar y)\\)) will be positive, and when it will be negative. When x and y both deviate from their means in the same direction (either both larger, or both smaller than their mean), this product will be positive. When they deviate in different directions (one smaller and the other larger than their mean), this product will be negative. Thus the whole sum will be very positive if x and y tend to deviate in the same direction as their respective means, and will be very negative if they tend to deviate in opposite directions. Note that this expression (\\((x_i - \\bar x)(y_i - \\bar y)\\)) will give the same answer regardless of which variable we call x, and which we call y. So the covariance of x and y is the same as the covariance of y and x. Also consider how this expression (\\((x_i - \\bar x)(y_i - \\bar y)\\)) will change if we (a) add a constant to x, or y, or both, and (b) multiply x or y or both by a constant. If we add a constant, it will just end up being subtracted out, since we consider only differences of a given value from its mean, thus the covariance will be the same regardless of where we “center” x and y. However, if we multiply x (or y) by something, we will end up scaling the distance between x and its mean. Thus, if we scale by a factor larger than 1, we will get a larger magnitude of this expression, and thus the covariance; and if we scale by a factor between 0 and 1, we will get a smaller covariance. So while the covariance doesn’t depend on the location it does depend on the scale of our variables. In R we can calculate the covariance with the cov function: cov(heights$Father, heights$Son) ## [1] 3.875382 Here are a few more detailed notes on the covariance. Correlation coefficient The correlation coefficient re-scales the covariance by the standard deviations of x and y, so it yields a measure of the linear relationship that is scale invariant, and is always between -1 and +1. \\[\\operatorname{correlation}(x,y) = r_{xy} = \\frac{\\operatorname{covariance}(x,y)}{s_x s_y} = \\frac{s_{xy}}{s_x s_y}\\] There are a bunch of ways to think about what the correlation means (this paper lists 13), but there are a few that I consider to be particularly useful: The correlation is a scale-invariant covariance. It is equal to the covariance of the z-score of x and the z-score of y. Thus, it disregards both the location and the scale of the variables, so we will get the same correlation regardless of how we linearly transform (multiply by a constant and/or add a constant) x and y (provided we don’t multiply by a negative number – that will change the sign, but not the magnitude, of the correlation). The correlation is the slope of the z-scores: meaning that if an x is 2 standard deviations above the mean of x, we would expect the corresponding y to be \\(r_{xy}*2\\) standard deviations above the mean of y (and vice versa). The correlation squared is the “coefficient of determination”. We will talk more about this when we consider the partitioning of variance, but this basically means: the proportion of the variance of y that we can explain by its relationship with x (and vice versa) is \\(r_{xy}^2\\). In R, we can calculate the correlation via cor, or calculate the correlation and test it’s significance via cor.test: cor(heights$Father, heights$Son) ## [1] 0.5011627 cor.test(heights$Father, heights$Son) ## ## Pearson&#39;s product-moment correlation ## ## data: heights$Father and heights$Son ## t = 18.997, df = 1076, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4550726 0.5445746 ## sample estimates: ## cor ## 0.5011627 For more details on the correlation, here are some detailed notes. (OLS) Regression: Predicting the mean of y for a given x A linear regression of y~x (“~” here is read “as a function of”) finds the “best fitting” line of the form \\(a \\cdot x + b\\) to estimate the “conditional mean” of y at a given x. heights %&gt;% ggplot(aes(x=Father, y=Son))+ geom_point()+ geom_smooth(method=&quot;lm&quot;) In the detailed notes we go through the mathematical definitions, but here, let’s just sort out what the different words in the sentence above mean. Regression line of y~x estimates the conditional mean of y for a given x. Therefore, the y value of the line at a given x is the estimated mean of the y values with that paired x. For instance, the Son ~ Father line shown above has y=72.4 at x=75. This indicates that we estimate the mean height of Son, whose fathers are 75\" tall, to be 72.4\" tall. A line (\\(\\hat y = a \\cdot x + b\\)) is characterized by its slope (\\(a\\)) and intercept (\\(b\\)). The slope indicates how many units \\(\\hat y\\) will increase (or decrease in the case of a negative slope) every time x goes up by one unit, thus it is in units of \\(y/x\\). For instance if the slope of a line predicting weight (kg) as a function of height (cm) is 0.44 (kg/cm), that means that we expect someone who is 1 cm taller to weight 0.44 kg more. The intercept tells us the value of the line (\\(\\hat y\\)), and thus our predicted mean y, when \\(x=0\\). So an intercept of of 2.46 on a line predicting weight (kg) as a function of height (cm) says that we predict people who are 0 cm tall to weigh 2.46 kg. For some parameters (in our case, a slope and intercept of a line) to be “best fitting,” that means that of all the values the parameters could have taken on, the “best fitting” values optimize some function that evaluates the overall fit. In the simple regression case, we evaluate the overall fit as the sum of squared errors, the smaller the better: \\(\\operatorname{SSE} = \\sum_{i=1}^n (y_i - \\hat y_i)^2 = \\sum_{i=1}^n (y_i - (a\\cdot x_i + b))^2\\). So the “best fitting” line is the line with a slope (\\(a\\)) and intercept (\\(b\\)) that yields the smallest sum of squared errors. These are also the “maximum likelihood” slope/intercept because we usually specify a probability model for the data which says that the y values are Normally distributed around a mean which varies linearly with x: \\(y_i \\sim \\operatorname{Normal}(a\\cdot x_i + b, \\sigma_e)\\). Thus, although ‘least squares’ regression can be motivated simply by asserting that we want to minimize squared error, it also happens to be the correct procedure for estimating the maximum likelihood parameters under the standard regression model. In R, we can do all this (and a whole lot more) with a single command lm, which stands for “linear model.” It takes as an argument a formula, and a data frame. A formula is written with the syntax respons.variable ~ explanatory.variable, so if we want to estimate the regression line predicting Son’ height as a function of fathers’ height, we would invoke the following incantation: lm(data=heights, Son ~ Father) ## ## Call: ## lm(formula = Son ~ Father, data = heights) ## ## Coefficients: ## (Intercept) Father ## 33.893 0.514 Difference between y~x, x~y, and the principle component line The “best fitting” line shown in the above section, doesn’t really look like the best fitting line. What gives? Consider the three lines below… I bet the black line above looks like the best line to you, and both the red and blue lines seem off, right? So what are these lines? Blue: the y~x line. Red: the x~y line. Black: the principle component line. Why is the “best looking” line not the “best fitting” y~x line? While we’re at it, why is the y~x line different from the x~y line? The answer to all of these is that when we do a regression predicting y from x, we only care about error in y. We minimize the squared deviations of each data point from the line in y, while keeping x constant. Consider this a teaser explanation, and go read the more detailed explanation. Partitioning variance As we move toward more complicated data, and away from simply comparing means between groups, it is useful to consider our analysis goals as partitioning variance. We want to separate variability in some response variable into different sources. In the simple linear regression case, we are just going to separate out the variability of y into the linear ‘signal’ (variability in y which we can explain via a linear relationship with x), and ‘noise’ (all other variability). When we move on to more complicated regression setups, we will be using more explanatory variables, and thus will be partitioning variability into more sources. It is important to keep in mind that ‘noise’ depends very much on the model we are considering. Perhaps all the variability in sons’ heights we can’t explain with fathers’ heights might be explained by ‘volume of breast milk consumed in the first year of life,’ or some other variable we don’t have access to. Thus, the ‘noise,’ is just unexplained/unmodeled variance. We have some detailed notes on partitioning variance, but briefly. When we partition variance, we mostly just consider the partitioning of the “sums of squares,” which we can get in R via the anova (analysis of variance) command, which tells us the variability in y attributable to our explanatory variable, and that which is left unexplained (residuals). lm.son.father &lt;- lm(data=heights, Son~Father) anova(lm.son.father) ## Analysis of Variance Table ## ## Response: Son ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Father 1 2145.4 2145.35 360.9 &lt; 2.2e-16 *** ## Residuals 1076 6396.3 5.94 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The proportion of variability in y that we can explain by taking into account the linear relationship with x, is the correlation squared (\\(r_{xy}^2\\)). However, R will just give us this number if we look at the detailed summary of the linear model we fit (via the summary function). summary(lm.son.father) ## ## Call: ## lm(formula = Son ~ Father, data = heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.8910 -1.5361 -0.0092 1.6359 8.9894 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.89280 1.83289 18.49 &lt;2e-16 *** ## Father 0.51401 0.02706 19.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.438 on 1076 degrees of freedom ## Multiple R-squared: 0.2512, Adjusted R-squared: 0.2505 ## F-statistic: 360.9 on 1 and 1076 DF, p-value: &lt; 2.2e-16 That ‘Multiple R-squared’ number is “the coefficient of determination,” which is the proportion of variance we can explain with the linear model, and in this simple one-variable regression case, it is also just the correlation squared. Significance of a linear relationship. We have seen a few p values above. One in cor.test, that told us the the correlation between fathers’ and sons’ heights is larger than expected by chance. One in anova(lm(...)) which told us that a linear model of sons~fathers explained more variance in sons heights than we would expect by chance. Another in summary(lm(...)) which told us that the father coefficient in the linear model we fit (\\(\\mbox{son} = \\hat \\beta_{\\mbox{father}} \\cdot \\mbox{father} + \\hat \\beta_0\\)) is significantly greater than zero. For the simple linear regression case, these are all the same by definition! They are all asking “is there more of a linear relationship than we expect from chance?” In the case of multiple regression these will all be different, and will all be asking different questions. (In this father-son data, all these p-values are so small, that they are past the limit of our computer’s ability to represent tiny numbers. Thus, even if they were different, we wouldn’t be able to tell. To see a case in which they are less tiny, look here. Prediction from regression. A regression line is our estimate of the mean of y for a given x (given the assumption that these conditional means fall on a line). Since there is uncertainty inherent in all estimation, the slope and intercept are uncertain, and thus the estimated mean for a given x is also uncertain. We can translate our standard errors of the slope and intercept into a standard error of the conditional mean of y for a given x. This is our uncertainty about the conditional mean of y, and we can extract it from a given linear model using the predict.lm function, with interval='coonfidence' (Note that the syntax for the predict function is to provide the fitted model object (m) and a data frame of new observations that we want to make new predictions on): m &lt;- lm(m &lt;- lm(data=heights, Son ~ Father)) predict.lm(m, newdata = data.frame(Father=72), interval=&#39;confidence&#39;) ## fit lwr upr ## 1 70.90123 70.62981 71.17264 By default, this returns the estimated conditional mean of y, at the x values provided in newdata; here, the estimated mean height of sons whose fathers were 72\" tall, and a 95% confidence interval on that mean. However, the confidence interval on the mean doesn’t give us a good idea of what heights we might expect of these sons – it tells us that on average they will be about 70.9\" tall, but any given son will vary a lot from the mean. To put a confidence interval on the height of a particular son we might see, we need not only take into account out uncertainty in estimating the mean height, but also how much variability there is in heights around the mean. The confidence interval for that is given with the interval = 'prediction' option: predict.lm(m, newdata = data.frame(Father=72), interval=&#39;prediction&#39;) ## fit lwr upr ## 1 70.90123 66.1095 75.69296 This interval is much broader because, although we can estimate the best fitting line (and thus the mean) very well, the individual heights have a lot of variability around that line. For more on these intervals, read here Anscombe’s quartet The data for Anscombe’s quartet are built into R, and it is a somewhat useful exercise to reshape the data frame and produce the plot. str(anscombe) ## &#39;data.frame&#39;: 11 obs. of 8 variables: ## $ x1: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x2: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x3: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x4: num 8 8 8 8 8 8 8 19 8 8 ... ## $ y1: num 8.04 6.95 7.58 8.81 8.33 ... ## $ y2: num 9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ... ## $ y3: num 7.46 6.77 12.74 7.11 7.81 ... ## $ y4: num 6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ... Note that these data are in “wide” format, we have 4 x columns and 4 y columns, rather than having an x column, a y column, and a column indicating whether the values correspond to set 1, 2, 3, or 4. We will reshape these data. # let&#39;s use tidyr and dplyr to reshape this into long format. library(tidyr) library(dplyr) library(ggplot2) # first, we add a new variable (observation number) anscombe.tidy = mutate(anscombe, observation=seq_len(n())) # now we reshape into (too) long format (all x and y values in one column) anscombe.tidy = gather(anscombe.tidy, key, value, -observation) # now we separate the key variable (&quot;x1&quot;, &quot;x2&quot;, .. &quot;y1&quot;) into two variables: # &quot;variable&quot; (&quot;x&quot; or &quot;y&quot;) and &quot;set&quot; (1, 2, 3, 4) anscombe.tidy = separate(anscombe.tidy, key, c(&quot;variable&quot;, &quot;set&quot;), 1, convert = TRUE) # Now we change the &quot;set&quot; to be a character variable anscombe.tidy = mutate(anscombe.tidy, set = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;, &quot;IV&quot;)[set]) # Finally, we spread into a wide format x and y values: anscombe.tidy = spread(anscombe.tidy, variable, value) str(anscombe.tidy) ## &#39;data.frame&#39;: 44 obs. of 4 variables: ## $ observation: int 1 1 1 1 2 2 2 2 3 3 ... ## $ set : chr &quot;I&quot; &quot;II&quot; &quot;III&quot; &quot;IV&quot; ... ## $ x : num 10 10 10 8 8 8 8 8 13 13 ... ## $ y : num 8.04 9.14 7.46 6.58 6.95 8.14 6.77 5.76 7.58 8.74 ... Great, now these data are in “long” format, and we can easily handle them in ggplot to produce a multi-panel plot. ggplot(anscombe.tidy, aes(x=x,y=y))+facet_grid(.~set)+ geom_smooth(method=&#39;lm&#39;, formula=y~x)+ geom_point(color=&quot;black&quot;, fill=&quot;orange&quot;, shape=21, size=3.5)+theme_minimal()+theme(text=element_text(size=16)) What’s notable about Anscombe’s quartet is that their summary statistics are so similar. anscombe.tidy %&gt;% group_by(set) %&gt;% summarize( n=n(), mean.x=round(mean(x),2), mean.y=round(mean(y),2), sd.x=round(sd(x),2), sd.y=round(sd(y),2), correlation=round(cor(x,y),2)) ## # A tibble: 4 x 7 ## set n mean.x mean.y sd.x sd.y correlation ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 I 11 9 7.5 3.32 2.03 0.82 ## 2 II 11 9 7.5 3.32 2.03 0.82 ## 3 III 11 9 7.5 3.32 2.03 0.82 ## 4 IV 11 9 7.5 3.32 2.03 0.82 These matching ns, means, standard deviations, and correlation, means that the slope and intercept, and corresponding statistical tests, are all equivalent; yet the data look clearly different. The point here is: summary statistics of marginal distributions and linear relationships necessarily overlook some (potentially important) aspects of the data. Covariance Generally, bivariate numerical data are often summarized in terms of their mean and covariance matrix. Since we are avoiding dealing with linear algebra in this class, we will not deal with this matrix directly. Instead we will consider the different components of a covariance matrix for a bivariate distribution. The elements of a covariance matrix (usually denoted with a capital sigma: \\(\\Sigma\\)) for two variables \\(x\\) and \\(y\\) are \\(\\sigma_x^2\\) (the variance of \\(x\\)), \\(\\sigma_y^2\\) (the variance of \\(y\\)), and \\(\\sigma_{xy}\\) the covariance of \\(x\\) and \\(y\\). \\(\\Sigma_{xy} = \\begin{bmatrix}\\sigma_x^2 &amp; \\sigma_{xy} \\\\ \\sigma_{xy} &amp; \\sigma_y^2\\end{bmatrix}\\) The covariance of two random variables (\\(\\sigma_{xy}\\)) is defined as the expectation of the products of deviations from the mean: \\(\\sigma_{XY} = \\operatorname{Cov}[X,Y] = \\mathbb{E}\\left[{(X-\\mu_X)(Y-\\mu_Y)}\\right] = \\int\\limits_X \\int\\limits_Y (X-\\mu_X)(Y-\\mu_Y) P(X,Y) \\, dX \\, dY\\) It is worth noting the similarity between the definition of the variance and the definition of the covariance: \\(\\operatorname{Var}[X] = \\mathbb{E}\\left[{(X-\\mu_X)^2}\\right]= \\mathbb{E}\\left[{(X-\\mu_X)(X-\\mu_X)}\\right]\\) The similarity here is that the variance of \\(X\\) can be thought of as the covariance of \\(X\\) with itself: \\(\\operatorname{Var}[X] = \\operatorname{Cov}[X,X]\\) What will the definition of \\(\\operatorname{Cov}[X,Y]\\) do? \\((X-\\mu_X)(Y-\\mu_Y)\\) will be positive for combinations of X and Y that both deviate in the same direction from their respective means (either both higher, or both lower, than their means) \\((X-\\mu_X)(Y-\\mu_Y)\\) will be negative for combinations of X and Y that deviate in different directions from their means (X is higher than its mean, Y is lower) Consequently, if combinations of X and Y that deviate in the same direction are more common than those that deviate in different directions, the covariance will be positive; vice versa for negative; and same-direction and different-direction deviation pairs are equally common, the covariance will be zero. cvs = c(-0.8, 0, 0.8) df = data.frame() for(cv in cvs){ df = rbind(df, data.frame(MASS::mvrnorm(n=500, c(0,0), matrix(c(1, cv, cv, 1), 2, 2)), Covariance=as.character(cv))) } ggplot(df, aes(X1,X2))+facet_grid(.~Covariance)+geom_point() If the overall variance of X or Y changes, the scale of the scared deviations of them from their means will change, and consequently, so will the covariance. This means that the covariance measures both the strength of the linear relationship between X and Y, as well as the overall spread of X and Y. Covariance decomposed into shared variance. One helpful way to think about the “covariance” is as arising from some shared variable. \\(W \\sim \\operatorname{Normal}(0,\\sigma_W)\\) \\(X = \\mu_X + aW + U \\text{, where } U \\sim \\operatorname{Normal}(0,\\sigma_U)\\) \\(Y = \\mu_Y + bW + V \\text{, where } V \\sim \\operatorname{Normal}(0,\\sigma_V)\\) In this case, the variance of \\(X\\) will be \\(\\sigma_X^2 = a^2 \\sigma_W^2 + \\sigma_U^2\\), the variance of \\(Y\\) will be \\(\\sigma_Y^2 = b^2 \\sigma_W^2 + \\sigma_V^2\\), and the covariance of \\(X\\) and \\(Y\\) will be \\(\\sigma_{XY} = a\\,b\\,\\sigma_W^2\\). In short, the covariance arises from some shared variance. Estimating covariance. We estimate the variance from a sample by summing up the squared deviations to yield a “sum of squares,” which we divide by n-1 to obtain an estimator for the variance (\\(s_X^2\\)): \\(s_X^2 = \\frac{\\sum\\limits_{i=1}^n (x_i - \\bar x)(x_i - \\bar x)}{n-1}\\) n = nrow(df.cm) (SS.h = sum((df.cm$height - mean(df.cm$height))^2)) ## [1] 86457 c(SS.h/(n-1), var(df.cm$height)) ## [1] 173.2605 173.2605 Similarly, we calculate the sample covariance by calculating the “sum of products” of the deviations of X and Y from their respective means, then dividing by n-1. n = nrow(df.cm) (SP.hf = sum((df.cm$height - mean(df.cm$height))* (df.cm$foot - mean(df.cm$foot)))) ## [1] 10308.97 c(SP.hf/(n-1), cov(df.cm$height, df.cm$foot)) ## [1] 20.65926 20.65926 You might wonder: why are we dividing by n-1, when we estimate two parameters (mean of x and mean of y). The answer comes from thinking about each observation as a two element vector (x,y), we have n of these vectors, and we estimate the mean the mean of those vectors – a two element mean. As discussed earlier, the covariance scales with the variance of X and Y. Generally, though, we want a measure of association that is scaled to the spread of the two variables, scaling the covariance yields the correlation. Correlation The correlation is a scaled/normalized covariance, which we calculate by dividing the covariance of x and y by their standard deviations. The population correlation (usually referred to as the greek letter “rho” \\(\\rho_{XY}\\)) can be calculated from the population covariance (\\(\\sigma_{XY}\\)), and population standard deviations of X and Y (\\(\\sigma_X\\), \\(\\sigma_Y\\)): \\(\\rho_{XY} = \\frac{\\sigma_{XY}}{\\sigma_X\\,\\sigma_Y}\\) And we can calculate the sample correlation (\\(r_{XY}\\)) – which we use as the estimator of population correlation – the same way, using the sample covariance and sample standard deviations: \\(\\hat \\rho_{XY} = r_{XY} = \\frac{s_{XY}}{s_X\\,s_Y}\\) In R, we just use the cor function to calculate the sample correlation. (r.hf = cor(df.cm$height, df.cm$foot)) ## [1] 0.7291268 The correlation is a scale-free measure of covariance – since we calculate it by dividing by the standard deviations of X and Y, it has lost any units X and Y had. This has some benefits and costs. The benefits are: The correlation will not care what units X was measured in (centimeters? inches? parsecs?), so we get a measure of the strength and direction of the linear relationship that abstracts away from these units. The cost is that we got rid of the physical units and thus our measure is further removed from reality. Correlation as the slope of z-scores One helpful way to think about the correlation is as the slope of the z-scores, or the slope in standard deviations. If the correlation between x and y is 0.8, that means that for an x that is 1 sd above the mean of x, we expect the y to be 0.8 sds above the mean of y. (more on this in OLS regression Coefficient of determination If we square the correlation, we get “R-squared,” or the “coefficient of determination,” which roughly describes what proportion of the overall variance is shared (more on this when we get to regression). Ordinary Least-Squares (OLS) Regression In OLS regression we have one numerical response variable (\\(y\\)), and one numerical explanatory variable (\\(x\\)), and we model the relationship between the two as a line plus some error in (\\(y\\)): \\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\], where \\[\\varepsilon_i \\sim \\operatorname{Normal}(0, \\sigma_\\varepsilon)\\] library(ggplot2) x = seq(-5,5,by=0.1) b0 = -2 b1 = 1/3 y.p = b0 + b1*x e = rnorm(length(x), 0, 0.4) df = data.frame(x = x, y = y.p, e = e) ggplot(df, aes(x,y))+ geom_line(size=1.5, color=&quot;blue&quot;)+ geom_point(aes(y=y+e), size=3)+ geom_linerange(aes(x=x, ymin=y, ymax=y+e), color=&quot;red&quot;) Here we show the individual \\(x,y\\) points as black dots, the underlying linear relationship in blue, and the error/residual (deviations in of \\(y\\) from the exact line) in red. A few things are noteworthy about the relationship this model assumes: The relationship between \\(y\\) and \\(x\\) is a line. We can always fit a line to some data, but the fact that we can do so does not mean that the relationship really is linear, or that a line fit to the relationship tells you anything meaningful at all. This is a strong structural assumption made in regression modeling, and you should always check (at least by looking at a scatterplot), that this is an adequate description of the data. There is error only in \\(y\\); there is no error in the \\(x\\) values. This assumption is violated often, without particularly adverse consequences; however, one should be thoughtful when assigning \\(x\\) and \\(y\\) variables. As a consequence, the regression line for \\(y\\) as a function of \\(x\\) is different than the regression line for \\(x\\) as a function of \\(y\\) (for more, see the difference between y~x and x~y). The correlation, on the other hand, is symmetric: \\(r_{xy} = r_{yx}\\). \\(y\\) value errors for different points are independent and identically distributed. This means that errors should not be correlated (as usually happens in timeseries data, or otherwise structured data), and that the distribution of errors does not change with \\(x\\) (errors are “homoscedastic”). Small violations of these assumptions often do not have much of an effect, but sometimes they do. We will talk about this at length later. Errors are normally distributed. While this assumption is not necessary for calculating the least squares \\(B\\) values, it is necessary for some of the null hypothesis tests we will be using to assess statistical significance of estimated model parameters. Regression terminology Let’s start with some terminology: \\(y_i = B_0 + B_1 x_i + \\varepsilon_i\\) \\(B_0\\) is the y intercept: the value of \\(y\\) when \\(x = 0\\). It shifts the line up and down along the \\(y\\) axis. \\(B_1\\) is the slope: how many units of \\(y\\) you gain/lose per unit change in \\(x\\). \\(B_1\\) is in the units \\(y/x\\). E.g., if we are predicting women’s height from the height of her mother – both in inches – and we find a slope of 0.8, that means that (all else equal) for every inch taller that a mother is, we expect the daughter to be 0.8 inches taller. The identity line is \\(y=x\\) (in other words: \\(B_0=0\\), \\(B_1=1\\)). \\(\\varepsilon_i\\) is the residual, or error, of the \\(i\\)th point: how far the real \\(y\\) value differs from the \\(y\\) value we predict using our linear function of \\(x\\). In all regression, the goal is to estimate \\(B\\) so as to minimize the sum of the squares of these residuals – the sum of squared errors. Estimating the regression line. Let’s generate some fake data, and then fit a line to them. IQ = rnorm(50, 100, 15) GPA = pmax(1, pmin(4.3, round((IQ-100)/15*0.5+2.8+rnorm(length(IQ),0,0.7), 1))) iq.gpa = data.frame(iq=IQ, gpa=GPA) g1 = ggplot(iq.gpa, aes(iq,gpa))+ geom_point(size=2) ( lm.fit = lm(data = iq.gpa, gpa~iq) ) ## ## Call: ## lm(formula = gpa ~ iq, data = iq.gpa) ## ## Coefficients: ## (Intercept) iq ## -1.60190 0.04367 This fit gives us an intercept (\\(B_0\\)) and a slope (\\(B_1\\)) for the line that minimizes the sum of squared errors. Math behind estimating the regression line When estimating the regression line we are interested in finding the slope (\\(B_1\\)) and intercept (\\(B_0\\)) values that will make the predicted y values \\(\\hat y_i = B_0 + B_1 x_i\\) as close to actual \\(y_i\\) values as possible. Formally, we want to find the \\(B\\) values that minimize the sum of squared errors: \\(\\sum (y_i - \\hat y_i)^2\\). It is useful to work through the algebra that allows us to obtain least squares estimates of the slope and intercept from the summary statistics of x and y and their correlation. Mean of x and y: \\(\\bar x = \\sum\\limits_{i=1}^n x_i\\), and \\(\\bar y = \\sum\\limits_{i=1}^n y_i\\). Standard deviations of x and y (by way of the sum of squares of x and y): \\(\\operatorname{SS}[x] = \\sum\\limits_{i=1}^n (x_i - \\bar x)^2\\) \\(s_x = \\sqrt{\\frac{1}{n-1} \\operatorname{SS}[x]}\\) \\(\\operatorname{SS}[y] = \\sum\\limits_{i=1}^n (y_i - \\bar y)^2\\) \\(s_y = \\sqrt{\\frac{1}{n-1} \\operatorname{SS}[y]}\\) The correlation of x and y by way of their sum of products and their covariance: \\(\\operatorname{SP}[x,y] = \\sum\\limits_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)\\) \\(s_{xy} = \\frac{1}{n-1} \\operatorname{SP}[x,y]\\) \\(r_{xy} = \\frac{s_{xy}}{s_x s_y}\\) n = nrow(iq.gpa) m.x = mean(iq.gpa$iq) m.y = mean(iq.gpa$gpa) SS.x = sum((iq.gpa$iq-m.x)^2) s.x = sd(iq.gpa$iq) SS.y = sum((iq.gpa$gpa-m.y)^2) s.y = sd(iq.gpa$gpa) SP.xy = sum((iq.gpa$iq-m.x)*(iq.gpa$gpa-m.y)) s.xy = cov(iq.gpa$iq, iq.gpa$gpa) r.xy = cor(iq.gpa$iq, iq.gpa$gpa) The least squares estimate of the slope is obtained by rescaling the correlation (the slope of the z-scores), to the standard deviations of y and x: \\(B_1 = r_{xy}\\frac{s_y}{s_x}\\) b1 = r.xy*s.y/s.x The least squares estimate of the intercept is obtained by knowing that the least-squares regression line has to pass through the mean of x and y. Consequently, \\(B_1 \\bar x + B_0 = \\bar y\\), and we can solve for the intercept as: \\(B_0 = \\bar y - B_1 \\bar x\\) b0 = m.y - b1*m.x With these estimates we can obtain the predicted y values for each observed x: \\(\\hat y_i = B_0 + B_1 x_i\\) iq.gpa$gpa.hat = iq.gpa$iq*b1 + b0 And from these we can calculate the individual residuals, the deviation of each y value from the y value predicted by the regression line: \\(e_i = y_i - \\hat y_i\\) iq.gpa$residual = iq.gpa$gpa - iq.gpa$gpa.hat From these residuals we can calculate the sum of squared error – that is, the sum of squared residuals: \\(\\operatorname{SS}[e] = \\sum\\limits_{i=1}^n e_i^2\\) SS.error = sum(iq.gpa$residual^2) And thus, we can estimate the standard deviation of the residuals by dividing the sum of squared error by \\(n-2\\) to get the variance, and then taking the square root. We use \\(n-2\\) because those are the degrees of freedom that are left after we estimate two parameters (the slope and intercept): \\(s_e = \\sqrt{\\frac{1}{n-2} \\operatorname{SS}[e]}\\) s.e = sqrt(SS.error/(n-2)) Standard errors of regression coefficients We can get the (marginal) standard errors of the slope and intercept using the summary function to get further details of the model fit: summary(lm.fit) ## ## Call: ## lm(formula = gpa ~ iq, data = iq.gpa) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.98118 -0.41757 0.03593 0.48339 1.22903 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.60190 0.71547 -2.239 0.0298 * ## iq 0.04367 0.00702 6.221 1.15e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6518 on 48 degrees of freedom ## Multiple R-squared: 0.4464, Adjusted R-squared: 0.4348 ## F-statistic: 38.7 on 1 and 48 DF, p-value: 1.151e-07 Math behind regression line errors It is worth looking at the equations used to calculate the marginal standard errors for the slope and intercept. Both standard errors increase with greater standard deviations of the residuals, and decrease with sample size; however, they also change in interesting ways with the standard deviation of \\(x\\), and the mean of \\(x\\). The standard error of the slope is proportional to the standard deviation of the residuals, inversely proportional to the square root of sample size, and also inversely proportion to the standard deviation of \\(x\\). This last fact is perhaps most intriguing, but should make sense: the more spread-out the x values are, the greater the change in y due to changes in x (rather than error), thus the better our estimate of the slope. \\(s\\{B_1\\} = s_e \\frac{1}{s_x \\sqrt{n-1}}\\) s.b1 = s.e/(s.x*sqrt(n-1)) The standard error of the intercept is more interesting. Remember that we calculate the intercept by relying on the fact that the least squares regression line must go through the mean of y and the mean of x. Consequently, we calculate the standard error of the intercept by summing the variance due to error in estimating the mean y value (which is inversely proportional to n), and the variance due to extrapolating the line with our uncertainty in the slope to x=0 (which is proportional to the squared standard error of the slope and the squared distance of the mean of x from 0). \\(s\\{B_0\\} = \\sqrt{\\left({\\frac{s_e}{\\sqrt(n)}}\\right)^2 + \\left({\\bar x s_e \\frac{1}{s_x \\sqrt{n-1}}}\\right)^2 }\\) s.b0 = s.e*sqrt(1/n + m.x^2/s.x^2/(n-1)) Confidence intervals and tests for regression coefficients The summary function by default returns the t-test statistic and p-values for comparing parameter values to 0, which we can extract via the coef function: (s.lm.fit = summary(lm.fit)) ## ## Call: ## lm(formula = gpa ~ iq, data = iq.gpa) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.98118 -0.41757 0.03593 0.48339 1.22903 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.60190 0.71547 -2.239 0.0298 * ## iq 0.04367 0.00702 6.221 1.15e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6518 on 48 degrees of freedom ## Multiple R-squared: 0.4464, Adjusted R-squared: 0.4348 ## F-statistic: 38.7 on 1 and 48 DF, p-value: 1.151e-07 coef(s.lm.fit) # returns the matrix of coefficients, errors, t-stats, and p-values ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.60190496 0.715466039 -2.238967 2.982907e-02 ## iq 0.04367101 0.007019801 6.221118 1.150963e-07 # we can index that matrix to get specific rows and columns out: coef(s.lm.fit)[c(&quot;(Intercept)&quot;), c(&quot;t value&quot;, &quot;Pr(&gt;|t|)&quot;)] ## t value Pr(&gt;|t|) ## -2.23896715 0.02982907 coef(s.lm.fit)[c(&quot;iq&quot;), c(&quot;t value&quot;, &quot;Pr(&gt;|t|)&quot;)] ## t value Pr(&gt;|t|) ## 6.221118e+00 1.150963e-07 Similarly, we can get confidence intervals on these coefficients using confint, which calculates the standard t-distribution confidence intervals using the standard errors. confint(lm.fit, &quot;(Intercept)&quot;, 0.95) ## 2.5 % 97.5 % ## (Intercept) -3.040446 -0.1633641 confint(lm.fit, &quot;iq&quot;, 0.95) ## 2.5 % 97.5 % ## iq 0.02955675 0.05778527 Calculating t-tests and intervals Just as in the case of all of our t-tests for various mean comparisons, we are going to use the t-distribution to obtain p-values and get confidence intervals. T-test for slope being non-zero: t.b1 = (b1-0)/s.b1 df = n-2 # n-2 because we lose two degrees of freedom for the slope and intercept when calculating the sample standard deviation of the residuals 2*pt(-abs(t.b1), df) #two-tail p value ## [1] 1.150963e-07 Confidence interval on the slope: q = 0.95 t.crit = qt((1-q)/2,df) b1 + c(1,-1)*t.crit*s.b1 ## [1] 0.02955675 0.05778527 T-test for intercept being different from some null (0 by default). Note that it is quite rare to test for some null intercept value; we do something analogous in ANCOVA, but for a simple regression this is rarely a useful question to ask. t.b0 = (b0-0)/s.b0 df = n-2 2*pt(-abs(t.b0), df) #two-tail p value ## [1] 0.02982907 Confidence interval on the intercept: q = 0.95 t.crit = qt((1-q)/2,df) b0 + c(1,-1)*t.crit*s.b0 ## [1] -3.0404458 -0.1633641 y~x vs x~y vs principle component line Let’s consider the fake IQ-GPA from our overview of OLS regression again. library(ggplot2) IQ = rnorm(50, 100, 15) GPA = pmax(1, pmin(4.3, round((IQ-100)/15*0.5+2.8+rnorm(length(IQ),0,0.7), 1))) iq.gpa = data.frame(iq=IQ, gpa=GPA) g1 = ggplot(iq.gpa, aes(iq,gpa))+ geom_point(size=2) g1 We can do two sorts of regression here: GPA as a function of IQ, or IQ as a function of GPA: ( lm.g.i = lm(data=iq.gpa, gpa~iq) ) ## ## Call: ## lm(formula = gpa ~ iq, data = iq.gpa) ## ## Coefficients: ## (Intercept) iq ## -1.4756 0.0414 ( lm.i.g = lm(data=iq.gpa, iq~gpa) ) ## ## Call: ## lm(formula = iq ~ gpa, data = iq.gpa) ## ## Coefficients: ## (Intercept) gpa ## 69.01 11.73 Note that the slopes are different, and critically, they are not inverses of each other. You might imagine that the IQ/GPA slope will be the inverse of the GPA/IQ slope, but they are not: c(1/coef(lm.g.i)[&#39;iq&#39;], coef(lm.i.g)[&#39;gpa&#39;]) ## iq gpa ## 24.15338 11.73068 Why are they different? \\(B\\{\\frac{y}{x}\\} = r_{xy}\\frac{s_y}{s_x}\\) \\(B\\{\\frac{x}{y}\\} = r_{xy}\\frac{s_x}{s_y}\\) Consequently, the slopes will be inverses of each other only if the correlation is 1. But why is this so? Remember, the y~x regression line is the line that minimizes squared error in y, and considers there to be no error in x. Similarly, the x~y regression line minimizes squared error in x and considers there to be no error in y. This discrepancy yields potentially very different slopes. Let’s look at the two lines, along with the “principle component” line that minimizes squared error orthogonal to the line (which is the intuitive line of best fit for most people). m.y = mean(iq.gpa$gpa) m.x = mean(iq.gpa$iq) s.y = sd(iq.gpa$gpa) s.x = sd(iq.gpa$iq) b1.y.x = coef(lm.g.i)[&#39;iq&#39;] b0.y.x = coef(lm.g.i)[&#39;(Intercept)&#39;] b1.x.y = 1/coef(lm.i.g)[&#39;gpa&#39;] b0.x.y = m.y - b1.x.y*m.x pc.load = prcomp(iq.gpa, scale=T, retx=T) b1.yx = pc.load$rotation[2,1]/pc.load$rotation[1,1]*s.y/s.x b0.yx = m.y - b1.yx*m.x iq.gpa$yh.y.x = b1.y.x*iq.gpa$iq + b0.y.x iq.gpa$xh.x.y = coef(lm.i.g)[&#39;gpa&#39;]*iq.gpa$gpa + coef(lm.i.g)[&#39;(Intercept)&#39;] iq.gpa$xh.yx = pc.load$x[,1]*pc.load$rotation[2,1]*s.x+m.x iq.gpa$yh.yx = pc.load$x[,1]*pc.load$rotation[1,1]*s.y+m.y ggplot(iq.gpa, aes(x=iq, y=gpa))+ geom_abline(intercept = b0.y.x, slope=b1.y.x, color=&quot;blue&quot;, size=1.5)+ geom_abline(intercept = b0.x.y, slope=b1.x.y, color=&quot;red&quot;, size=1.5)+ geom_abline(intercept = b0.yx, slope=b1.yx, color=&quot;gray&quot;, size=1.5)+ geom_point(size=2.5) Here the blue line shows the gpa~iq fit, the red line shows iq~gpa, and the gray line shows the principle component line (which is the intuitive line most people would draw to describe the best fit). Why are they different? They are minimizing different squared errors. The gpa~iq line minimizes error in gpa, the iq~gpa line minimizes error in iq, and the principle component line minimizes error orthogonal to the line (counting both iq and gpa deviations as error). g2 &lt;- ggplot(iq.gpa, aes(x=iq, y=gpa))+ geom_abline(intercept = b0.y.x, slope=b1.y.x, color=&quot;blue&quot;, size=1.5)+ geom_segment(aes(x=iq, y=gpa, xend=iq, yend=yh.y.x), color=&quot;blue&quot;)+ geom_point(size=2.5) g3 &lt;- ggplot(iq.gpa, aes(x=iq, y=gpa))+ geom_abline(intercept = b0.x.y, slope=b1.x.y, color=&quot;red&quot;, size=1.5)+ geom_segment(aes(x=iq, y=gpa, yend=gpa, xend=xh.x.y), color=&quot;red&quot;)+ geom_point(size=2.5) g4 &lt;- ggplot(iq.gpa, aes(x=iq, y=gpa))+ geom_abline(intercept = b0.yx, slope=b1.yx, color=&quot;gray&quot;, size=1.5)+ geom_segment(aes(x=iq, y=gpa, yend=yh.yx, xend=xh.yx), color=&quot;gray&quot;)+ geom_point(size=2.5) library(gridExtra) grid.arrange(g2,g3,g4,ncol=3) Partitioning variance and the coefficient of determination. Let’s start with the fake IQ-GPA data from our discussion of ordinary least squared regression. library(ggplot2) IQ = rnorm(50, 100, 15) GPA = pmax(1, pmin(4.3, round((IQ-100)/15*0.5+2.8+rnorm(length(IQ),0,0.7), 1))) iq.gpa = data.frame(iq=IQ, gpa=GPA) g1 = ggplot(iq.gpa, aes(iq,gpa))+ geom_point(size=2) ( lm.fit = lm(data = iq.gpa, gpa~iq) ) ## ## Call: ## lm(formula = gpa ~ iq, data = iq.gpa) ## ## Coefficients: ## (Intercept) iq ## -0.34751 0.03134 Linear regression brings to the forefront a running theme in classical statistics: partitioning the overall variance of measurements into the variance that may be attributed to different sources. This kind of partitioning is most commonly associated with an “analysis of variance,” which is a particular way of analyzing the results of a linear regression. In R, we can get the ANOVA results for a given linear model fit via the anova command: anova(lm.fit) ## Analysis of Variance Table ## ## Response: gpa ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iq 1 11.297 11.2973 32.323 7.555e-07 *** ## Residuals 48 16.776 0.3495 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Tha ANOVA table shows the degrees of freedom, sums of squares, and other values for different sources of variance in y. The two sources of variance in play in our simple regression are (1) variance in GPA that can be attributed to variance in IQ, and its impact on GPA, and (2) variance in GPA that we cannot account for with our predictors (here just IQ) – the left over variance of the residuals. These sums of squares are not quite actual variance estimates, they are variance estimates unnormalized by the number of data points that went into those estimates (the sums of squares, rather than the sums of squares divided by the number of things that go into that sum). However, the number of elements that goes into each of these sums is the same, so we can compare them. For instance we can divide the sum of squares attributed to IQ, by the sum of IQ and residual sums of squares, to calculate the proportion of the variance in GPA that can be explained by IQ: ss.iq = anova(lm.fit)[&#39;iq&#39;,&#39;Sum Sq&#39;] ss.error = anova(lm.fit)[&#39;Residuals&#39;,&#39;Sum Sq&#39;] (r.sq = ss.iq/(ss.iq+ss.error)) ## [1] 0.4024137 This “proportion of variance explained,” or “coefficient of determination” can also be calculated by squaring the sample correlation (for this simple case of one response and one explanatory variable): cor(iq.gpa$iq, iq.gpa$gpa)^2 ## [1] 0.4024137 So what are these sums of squares? Calculating sums of squares. The basic partitioning of sums of squares follows the logic that the “total sum of squares” is equal to the sum of all the sums of squares of different candidate sources. In our case, the “total” sum of squares is the total variation of \\(y\\) (GPA) around its mean: \\(\\text{SST} = \\operatorname{SS}[y] = \\sum\\limits_{i=1}^n (y_i - \\bar y)^2\\) m.y = mean(iq.gpa$gpa) iq.gpa$gpa.hat = predict(lm.fit, newdata = iq.gpa) ( SS.y = sum((iq.gpa$gpa-m.y)^2) ) ## [1] 28.0738 The sum of squares of the “regression,” that is – the sum of squares that can be explained by the linear model we fit, can be calculated as the sum of the squared deviations of the predicted y values, from the mean of y: \\(\\text{SSR} = \\operatorname{SS}[\\hat y] = \\sum\\limits_{i=1}^n (\\hat y_i - \\bar y)^2\\) ( SS.yhat = sum((iq.gpa$gpa.hat - m.y)^2) ) ## [1] 11.29728 The sum of squares of the residuals (or the error), is the sum of squared deviations of the actual y values from those predicted by the linear regression. \\(\\text{SSE} = \\operatorname{SS}[e] = \\sum\\limits_{i=1}^n (y_i - \\hat y_i)^2\\) ( SS.error = sum((iq.gpa$gpa - iq.gpa$gpa.hat)^2) ) ## [1] 16.77652 We can generate a plot of these. The black line segments indicate the deviation of y values from the mean y value (black horizontal line); the blue line segments indicates the deviation of the predicted y value (blue slope) from the mean y value; and the red segments indicate the error – the deviation of the actual y value from the predicted y value. ggplot(iq.gpa, aes(x=iq, y=gpa))+ geom_point(size=5)+ geom_hline(yintercept = m.y)+ geom_abline(intercept=coef(lm.fit)[&quot;(Intercept)&quot;], slope=coef(lm.fit)[&quot;iq&quot;], color=&quot;blue&quot;, size=1.5)+ geom_linerange(ymin = m.y, mapping = aes(x=iq, ymax=gpa), color=&quot;black&quot;)+ geom_linerange(ymin=m.y, aes(x=iq-0.35, ymax=gpa.hat), color=&quot;blue&quot;)+ geom_linerange(aes(x=iq+0.35, ymin=gpa, ymax=gpa.hat), color=&quot;red&quot;) The partitioning of the deviation of a given y value from the mean into the deviation of the corresponding regression prediction from the mean, and the deviation of the y value from the regression prediction maps onto the partition of sums of squares: SST = SSR + SSE c(SS.y, SS.yhat+SS.error) ## [1] 28.0738 28.0738 Significance of linear relationship. Here we will work with the fake IQ-GPA data we generated when talking about Ordinary Least-Squares (OLS) Regression. library(ggplot2) IQ = rnorm(50, 100, 15) GPA = pmax(1, pmin(4.3, round((IQ-100)/15*0.5+2.8+rnorm(length(IQ),0,0.7), 1))) iq.gpa = data.frame(iq=IQ, gpa=GPA) g1 = ggplot(iq.gpa, aes(iq,gpa))+ geom_point(size=2) print(g1) ( lm.fit = lm(data = iq.gpa, gpa~iq) ) ## ## Call: ## lm(formula = gpa ~ iq, data = iq.gpa) ## ## Coefficients: ## (Intercept) iq ## 0.19080 0.02676 There are many equivalent ways to ascertain whether a single-variable ordinary least-squares regression is “significant.” Significance of slope. We already saw that we can test whether the slope is significantly different from 0 by calculating a t statistic by using the estimated slope and its standard error: \\(t_{n-2} = \\frac{B_1}{s\\{B_1\\}}\\). This is the statistic R calculates when estimating the significance of the coefficient from the linear model summary: coef(summary(lm.fit))[&#39;iq&#39;,] ## Estimate Std. Error t value Pr(&gt;|t|) ## 2.676271e-02 5.404687e-03 4.951759e+00 9.499429e-06 Significance of pairwise correlation Similarly, we can obtain this same t statistic via the pairwise correlation via: \\(t_{n-2} = r_{xy}\\sqrt{\\frac{n-2}{1-r_{xy}^2}}\\) We can obtain this test of the significance of a pairwise correlation via the cor.test function: cor.test(iq.gpa$iq, iq.gpa$gpa) ## ## Pearson&#39;s product-moment correlation ## ## data: iq.gpa$iq and iq.gpa$gpa ## t = 4.9518, df = 48, p-value = 9.499e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3616630 0.7400446 ## sample estimates: ## cor ## 0.5814747 Significance of variance partition. Finally, we can calculate an F statistic based on the partition of the variance attributable to the regression as: \\(F_{(1,n-2)} = \\frac{\\operatorname{SS}[\\hat y]}{\\operatorname{SS}[error]/(n-2)}\\) This is the statistic we get from the anova command: anova(lm.fit)[&#39;iq&#39;,] ## Analysis of Variance Table ## ## Response: gpa ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iq 1 9.0766 9.0766 24.52 9.499e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Which we can calculate manually as: SS.yhat = anova(lm.fit)[&#39;iq&#39;, &#39;Sum Sq&#39;] SS.error = anova(lm.fit)[&#39;Residuals&#39;, &#39;Sum Sq&#39;] df.1 = 1 # anova(lm.fit)[&#39;iq&#39;, &#39;Df&#39;] df.2 = nrow(iq.gpa)-2 # anova(lm.fit)[&#39;Residuals&#39;, &#39;Df&#39;] ( F = (SS.yhat/df.1) / (SS.error/df.2) ) ## [1] 24.51992 ( p = 1-pf(F, df.1, df.2) ) ## [1] 9.499429e-06 Isomorphism with one response and one predictor In this special case of a linear regression with just one explanatory variable, all of these are equivalent. The t statistic for the correlation is the t-statistic for the coefficient, and that t-value squared is the F value from the analysis of variance. The resulting p-values are also the same. # t statistics from slope and correlation (ts = c(coef(summary(lm.fit))[&#39;iq&#39;, &#39;t value&#39;], cor.test(iq.gpa$iq, iq.gpa$gpa)$statistic) ) ## t ## 4.951759 4.951759 # squared t statistics and anova F value c(ts^2, anova(lm.fit)[&#39;iq&#39;, &#39;F value&#39;]) ## t ## 24.51992 24.51992 24.51992 # p values from slope, correlation, and anova c(coef(summary(lm.fit))[&#39;iq&#39;, &#39;Pr(&gt;|t|)&#39;], cor.test(iq.gpa$iq, iq.gpa$gpa)$p.value, anova(lm.fit)[&#39;iq&#39;, &#39;Pr(&gt;F)&#39;]) ## [1] 9.499429e-06 9.499429e-06 9.499429e-06 This isomorphism between the test for the pairwise correlation (cor.test(x,y)), the test of the estimated regression slope (summary(lm(y~x))), and the test for the variance in y attributable to x (anova(lm(y~x))), is specific to the simple case of one response and one explanatory variable. With multiple explanatory variables (multiple regression) these will all yield different results (as they all ask subtly different questions – more on this when we get to multiple regression). Regression prediction. We will start our discussion of prediction intervals with the same fake IQ-GPA data that we covered in OLS regression. library(ggplot2) IQ = rnorm(50, 100, 15) GPA = pmax(1, pmin(4.3, round((IQ-100)/15*0.5+2.8+rnorm(length(IQ),0,0.7), 1))) iq.gpa = data.frame(iq=IQ, gpa=GPA) g1 = ggplot(iq.gpa, aes(iq,gpa))+ geom_point(size=2) ( lm.fit = lm(data = iq.gpa, gpa~iq) ) ## ## Call: ## lm(formula = gpa ~ iq, data = iq.gpa) ## ## Coefficients: ## (Intercept) iq ## -0.36814 0.03185 Now that we have estimated the best fitting intercept (\\(B_0\\)) and slope (\\(B_1\\)), we can ask what y values we predict for an arbitrary x. There are two kinds of predictions we might make: a prediction for the mean y value at a given x, and a prediction for a new data point at a given x. Predicting mean y given x. Our uncertainty about the slope and our uncertainty about the mean of y will combine to give us uncertainty about the y value that the line will pass through at a given x. This line describes the mean of y at each x; consequently, our uncertainty about the line, is our uncertainty about mean y at a given x. Two sources of uncertainty contribute to our error in estimating the y value at a given x: (1) uncertainty about the mean of y, which contributes a constant amount of error regardless of which x we are talking about, and (2) extrapolation uncertainty, due to our uncertainty about the slope – this source of error grows the further from the mean of x we try to predict a new y value. These combine into the net error in predicted y values as: \\(s\\{\\hat y \\mid x\\} = s_e \\sqrt{\\frac{1}{n} + \\frac{(x-\\bar x)^2}{s_x^2 (n-1)}}\\) To calculate this error in R, and get a corresponding confidence interval, we use the predict function, which yields this standard error, the confidence interval on the y value of the line at a given x, which is defined by: \\((\\hat y \\mid x) \\pm t^*_{alpha} s\\{\\hat y \\mid x\\}\\) predict(lm.fit, newdata = data.frame(iq=160), se.fit = T, interval = &quot;confidence&quot;, level=0.95) ## $fit ## fit lwr upr ## 1 4.727137 4.126364 5.32791 ## ## $se.fit ## [1] 0.2987977 ## ## $df ## [1] 48 ## ## $residual.scale ## [1] 0.6105194 We can do this calculation manually: n = nrow(iq.gpa) m.x = mean(iq.gpa$iq) m.y = mean(iq.gpa$gpa) s.x = sd(iq.gpa$iq) s.y = sd(iq.gpa$gpa) r.xy = cor(iq.gpa$iq, iq.gpa$gpa) b1 = r.xy*s.y/s.x b0 = m.y - b1*m.x s.e = sqrt(sum((iq.gpa$gpa - (iq.gpa$iq*b1 + b0))^2)/(n-2)) new.x = 160 s.yhat.x = function(new.x){s.e*sqrt(1/n + (new.x - m.x)^2/(s.x^2*(n-1)))} ( s.yhat.160 = s.yhat.x(new.x) ) ## [1] 0.2987977 t.crit = qt((1-0.95)/2,n-2) y.hat = b0 + b1*new.x ( y.hat + c(1,-1)*t.crit*s.yhat.x(new.x) ) ## [1] 4.126364 5.327910 Predicting new y given x. If instead of putting confidence intervals on mean y at a given x, we want confidence intervals on a new data point, we have to add to our uncertainty about the mean, our uncertainty about where data points are relative to the mean. Since our data do not fall exactly on the line, there is some spread of data around the line, and we have to take that into account when predicting a new data point. We do so by adding the variance of the residuals (the spread of data around the mean), to the variance of the line position (described in the previous section). \\(s\\{y \\mid x\\} = \\sqrt{s_e^2 + s\\{\\hat y \\mid x\\}^2}\\) In R this is called a “prediction” interval, and we can get it with the predict function as well. Note that the “standard error” predict returns is still just the standard error of the line (same as the previous section), but the confidence intervals are defined by further incorporating the standard deviation of data points around the line (the standard deviation of the residuals). predict(lm.fit, newdata = data.frame(iq=160), se.fit = T, interval = &quot;predict&quot;, level=0.95) ## $fit ## fit lwr upr ## 1 4.727137 3.360476 6.093797 ## ## $se.fit ## [1] 0.2987977 ## ## $df ## [1] 48 ## ## $residual.scale ## [1] 0.6105194 To do this manually, we just need to add the variance of the residuals to the variance of the line: new.x = 160 s.y.x = function(new.x){sqrt(s.e^2 + s.yhat.x(new.x)^2)} ( s.y.160 = s.y.x(new.x) ) ## [1] 0.6797161 t.crit = qt((1-0.95)/2,n-2) y.hat = b0 + b1*new.x ( y.hat + c(1,-1)*t.crit*s.y.x(new.x) ) ## [1] 3.360476 6.093797 Visualizing the difference It is useful to see how these two kinds of prediction confidence intervals change as a function of x. We see that the interval for predicting a new data point (gray) is much wider, due to the considerable amount of variability of data points around the line. The interval on the line is narrower and is also very saliently inhomogenous – it grows the further from the mean we are. Technically, the prediction (gray) interval also grows, but often this is not easily detectable by eye because so much of that variability is swamped by the variance of the data points around the line. iq = seq(40,160) y.x = function(x){b0+b1*x} t.crit = qt(0.05,n-2) pred.df = data.frame(iq=iq, gpa = y.x(iq), s.yhat.x = s.yhat.x(iq), s.y.x = s.y.x(iq)) ggplot(pred.df, aes(x=iq, y=gpa))+ geom_ribbon(aes(ymax=gpa+s.y.x, ymin=gpa-s.y.x), fill=&quot;gray&quot;)+ geom_ribbon(aes(ymax=gpa+s.yhat.x, ymin=gpa-s.yhat.x), fill=&quot;blue&quot;)+ geom_line(aes(x=iq, y=gpa), color=&quot;red&quot;, size=1.5) As we look at this plot, another salient feature should jump out: for very large or very small iq values, our predicted GPA is not contained in a reasonable range of GPAs. This is a problem of relying too much on a linear fit to a fundamentally non-linear relationship: the IQ-GPA relationship cannot be linear, because GPA has a lower and an upper bound. Consequently, if we extrapolate too far outside the range we studied, we will get predictions outside the reasonable bound (and we will also get very wide confidence intervals). Regression Diagnostics When we fit a line to some data using ordinary least squares regression and then interpret the coefficients, corresponding t-tests, etc. we are making quite a few assumptions. When some of these are violated, we will git a misleading answer from the regression. In this section, we will conjure up various plots and tests to check some of these assumptions. Some of the most important assumptions we make cannot be tested by looking at the data, instead they require domain knowledge. The most important assumption is that we are measuring the things we want to be. Usually we use some observable proxy variables for unobservable latent properties we are interested in. For instance, we might use GDP/capita of a country as a proxy for income of its citizens, but, this measure does not consider cost of living, income distributions, etc. Domain knowledge and critical thought is requires to guess whether the difference between the measurable proxy and the latent variable is a big deal with respect to our question of interest. Assumption: relationship between y and x is well described by a line. We can always fit a line, that doesn’t mean it’s a good idea. The simplest, and most important way to check if it’s a good idea to fit a line is to look at the scatterplot. If it looks clearly non-linear, don’t fit a line to it! suppressMessages(library(dplyr)) suppressMessages(library(ggplot2)) n = 200 quadratic.data &lt;- data.frame(x = seq(-100, 100, length.out = n), y=seq(-3, 4, length.out = n)^2+rnorm(n)) ggplot(quadratic.data, aes(x,y))+geom_point()+geom_smooth(method=&#39;lm&#39;) When we have a simple regression with just one predictor, it’s really easy to look at the scatterplot, so that’s usually enough, but we can also look at the residuals (difference between y and the y we predict from the regression) as a function of x: quadratic.data$residuals = residuals(lm(data=quadratic.data, y~x)) ggplot(quadratic.data, aes(x,residuals))+geom_point()+geom_hline(yintercept = 0, color=&#39;red&#39;) As we move to multiple regression, we can only look at one variable at a time in plots like the one above, so they tend to not be so useful. Instead, we will look at the residuals as a function of the fitted y value. quadratic.data$fitted.y = fitted(lm(data=quadratic.data, y~x)) ggplot(quadratic.data, aes(fitted.y,residuals))+geom_point()+geom_hline(yintercept = 0, color=&#39;red&#39;) This plot is also returned as the first diagnostic plot of a linear model, from plot(lm(..), 1). plot(lm(data=quadratic.data, y~x), which = 1) All of these plots are basically showing us what was obvious from the get-go: the line misses the really big quadratic component we built into our data. So we probably shouldn’t be fitting a line. Assumption: out estimates are not driven by a few huge outliers n = 99 outlier.data &lt;- data.frame(x = c(rnorm(n),-15), y=c(rnorm(n), 15)) ggplot(outlier.data, aes(x,y))+geom_point()+geom_smooth(method=&#39;lm&#39;) These fake data make it patently obvious that the one outlier at (-15,15) is driving the regression line, but often it might be more subtle. What allows an outlier to greatly influence a regression? It needs to have a lot of leverage – meaning that it is an outlier with respect to the predictors (here, just x), which we can measure with hat(). Second, it needs to use that leverage by also being an outlier in y. We can check if such a thing happens by looking at the residuals as a function of the leverage. outlier.data$leverage = hat(outlier.data$x) outlier.data$residuals = residuals(lm(data=outlier.data, y~x)) ggplot(outlier.data, aes(leverage, residuals))+geom_point() An more useful version of this plot is generated via plot(lm(), 5): plot(lm(data=outlier.data, y~x), which = 5) This shows residuals (here they are standardized, meaning, scaled by their sd) as a function of leverage, just as in the plot we made above. However, it also shows contours corresponding to a particular “Cook’s distance.” Cook’s distance is high when a data point has a lot of influence on the regression, meaning that it has a lot of leverage and uses it (by also being an outlier in y). It basically tells us how much this data point is changing our regression line. Proposed cutoffs for a Cook’s distance being too large are 1, or 4/n. We can look at the Cook’s distance for each data point with plot(lm(), 4) plot(lm(data=outlier.data, y~x), which = 4) If you have data points with very large Cook’s distances, it means that the regression line you have estimated is largely reflecting the influence of a few data points, rather than the bulk of your data. Assumption: errors are independent, identically distributed, normal. There is also a set of assumptions about the behavior of the errors/residuals. They should be normal (so that we are justified in using t and F statistics), they should be identically distributed, and they should be independent. Errors are independent: independent of x, fitted y, order, each other. Probably the most important of these assumptions is that the errors are independent, but not being independent can mean a few different things. Errors can be autocorrelated in order (e.g., if I measure things over time, errors tend to be correlated over time) – this is a major issue in time-series analysis, but for the kind of data we usually deal with, we can mostly ignore it. Errors might depend on x, or fitted y; this is the kind of thing we saw when we had obviously non-linear data. This is important, but we’ve already considered it. Errors might correlate with each other – this is what happens when we incorrectly specify a nested / hierarchical model. For instance, if I measure the weight and height of 10 people, each 5 times, I will get 50 measurements; however, I only really have 10 independent units (people) – all measurements of the same person will have a correlated error with repsect to the overall weight~height relationship. We really need to avoid this kind of non-independence in errors, as it will lead to really wrong conclusions; but this is not something we can easily check for; we just need to understand the structure of our data, and specify the appropriate model for the error correlation (with lmer or aov, etc). Errors are identically distributed Typically, ‘identically distributed’ for errors refers to them being ‘homoscedastic,’ or ‘equal variance’ – meaning that the magnitude of the residuals is roughly consistent across our regression (rather than ‘heteroscedastic,’ in which there is more variability in y for some values of x than others). heteroscedastic.data &lt;- data.frame(x=seq(0,100,length.out=100), y=seq(0,100,length.out=100)*0.1+ rnorm(100,0,seq(0,100,length.out=100)/10)) ggplot(heteroscedastic.data, aes(x,y))+geom_point() We can check for this using plot(lm, 3) plot(lm(data=heteroscedastic.data, y~x),3) This shows the absolute, standardized residuals as a function of the fitted y value (not as a function of x, so that this plot will also work for multiple regression with lots of explanatory variables). Clearly, we have bigger errors at bigger y values. Sometimes such heteroscedasticity arises from our model being wrong. For instance, if we consider how much net worth fluctuates over time, we will see that wealthy people have much larger fluctuations in absolute terms. This is because fluctuations are constant not in dollars, but in percent. Thus, the heteroscedasticity in this case reflects that we are applying a linear model to dollars, but we should instead be considering log(dollars), or percent fluctuations. So, if we see very large, blatant, heteroscedasticity, we should carefully evaluate if we are measuring something that we really believe should have linear, additive errors. If not, we should use an appropriate non-linear transform (like a logarithm), to get a measure of something that we do believe is linear and additive. However, if we have a variable that seems to us should be linear, and otherwise seems to behave linearly, a slight amount of heteroscedasticity is not that big of a deal, in the grand scheme of things. Errors are normal The last technical assumption is that the errors (residuals) are normally distributed. Just as in the case of heteroscedasticity, if they clearly are not, we should think hard if we are specifying the correct model; however, slight deviations from normality are practically guaranteed, and are largely irrelevant. We can check for this using a qq-plot of the residuals with plot(lm, 2). x = rnorm(100) non.normal &lt;- data.frame(x=x, y=x*1+rexp(100, 0.5)) ggplot(non.normal, aes(x,y))+geom_point() plot(lm(data=non.normal, y~x),2) Insofar as the stadardized residual quantiles don’t fall in line with the theoretical quantiles, we have non-normal residuals. I think a better way to look at weirdness of residuals would be to look at their histogram: non.normal$residuals = residuals(lm(data=non.normal, y~x)) ggplot(non.normal, aes(x=residuals))+geom_density(fill=&#39;gray&#39;) If the histogram of the residuals looks really skewed, then it’s worth trying to figure out how to transform the y variable to get the data to behave more sensibly. Slight deviations from normality won’t matter much. Testing assumptions There are assorted null hypothesis tests to see if these assumptions are violated. With enough (real) data, the answer will almost certainly be yes. With too little data, glaringly obvious violations might not reach statistical significance. So, in practice, I find the null hypothesis tests for assumption violations to not be especially useful, and the diagnostic plots to be far more practical. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
