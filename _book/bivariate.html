<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Bivariate linear relationships | UCSD Psyc 201ab / CSS 205 / Psyc 193</title>
  <meta name="description" content="This is the class website and lecture notes for psyc 201ab and CSS 205" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Bivariate linear relationships | UCSD Psyc 201ab / CSS 205 / Psyc 193" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the class website and lecture notes for psyc 201ab and CSS 205" />
  <meta name="github-repo" content="UCSD-PSYC201/website" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bivariate linear relationships | UCSD Psyc 201ab / CSS 205 / Psyc 193" />
  
  <meta name="twitter:description" content="This is the class website and lecture notes for psyc 201ab and CSS 205" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chi-squared.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#instructors"><i class="fa fa-check"></i>Instructors</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#class-meetings"><i class="fa fa-check"></i>Class meetings</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#grading"><i class="fa fa-check"></i>Grading</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#class-resources"><i class="fa fa-check"></i>Class Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html"><i class="fa fa-check"></i>201a Schedule</a>
<ul>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-0-introduction"><i class="fa fa-check"></i>Week 0: Introduction</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-1-data"><i class="fa fa-check"></i>Week 1: Data</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-2-visualization"><i class="fa fa-check"></i>Week 2: Visualization</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-3-theoretical-foundations"><i class="fa fa-check"></i>Week 3: Theoretical foundations</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-4-linear-model-regression"><i class="fa fa-check"></i>Week 4: Linear model: Regression</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-5-linear-model-miderm"><i class="fa fa-check"></i>Week 5: Linear model, miderm</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-6-linear-model-categorical-predictors"><i class="fa fa-check"></i>Week 6: Linear model: Categorical predictors</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-7-linear-model-ancova-diagnostics"><i class="fa fa-check"></i>Week 7: Linear model: ANCOVA, diagnostics</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-8-linear-model-linearizing-transforms"><i class="fa fa-check"></i>Week 8: Linear model: Linearizing transforms</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-9-covarying-errors-repeated-measures-random-effects"><i class="fa fa-check"></i>Week 9: Covarying errors (repeated measures / random effects)</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-10-review-and-preview"><i class="fa fa-check"></i>Week 10: Review and preview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html"><i class="fa fa-check"></i>Projects</a>
<ul>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#examples-of-this-sort-of-thing"><i class="fa fa-check"></i>Examples of this sort of thing</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a"><i class="fa fa-check"></i>201a</a>
<ul>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-timeline"><i class="fa fa-check"></i>201a Timeline</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#groups"><i class="fa fa-check"></i>Groups</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-project-plan"><i class="fa fa-check"></i>201a Project plan</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-preliminary-data-summaries"><i class="fa fa-check"></i>201a Preliminary data summaries</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-write-ups"><i class="fa fa-check"></i>201a Write-ups</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-presentation"><i class="fa fa-check"></i>201a Presentation</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-group-evaluation"><i class="fa fa-check"></i>201a Group-evaluation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#b"><i class="fa fa-check"></i>201b</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#data-sources"><i class="fa fa-check"></i>Data sources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html"><i class="fa fa-check"></i>R homework</a>
<ul>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#grading-1"><i class="fa fa-check"></i>Grading</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#collaboration"><i class="fa fa-check"></i>Collaboration</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#submitting-r-assignments"><i class="fa fa-check"></i>Submitting R Assignments</a>
<ul>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#writing-r-scripts"><i class="fa fa-check"></i>Writing R scripts</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#submitting-your-assignment"><i class="fa fa-check"></i>Submitting your assignment</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#additional-resources."><i class="fa fa-check"></i>Additional resources.</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Notes</b></span></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html"><i class="fa fa-check"></i>Getting started with R</a>
<ul>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#R-install"><i class="fa fa-check"></i>Installing R</a>
<ul>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#packages"><i class="fa fa-check"></i>Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#R-intro"><i class="fa fa-check"></i>Introduction to R</a>
<ul>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#getting-started"><i class="fa fa-check"></i>Getting started</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#better-data-analysis-code."><i class="fa fa-check"></i>Better data analysis code.</a></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#using-r-markdown"><i class="fa fa-check"></i>Using R-markdown</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i>Visualizations</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#general-rules-for-scientific-data-visualization."><i class="fa fa-check"></i>General rules for scientific data visualization.</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#picking-a-plot-whats-convention"><i class="fa fa-check"></i>Picking a plot (what’s convention)</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#categorical-0"><i class="fa fa-check"></i>Categorical ~ 0</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#histogram"><i class="fa fa-check"></i>Histogram</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#pie-chart"><i class="fa fa-check"></i>Pie chart</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#stacked-area"><i class="fa fa-check"></i>Stacked area</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-0"><i class="fa fa-check"></i>numerical ~ 0</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#histogram-density"><i class="fa fa-check"></i>Histogram &amp; density</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-categorical"><i class="fa fa-check"></i>numerical ~ categorical</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#bar-plot-with-error-bars"><i class="fa fa-check"></i>Bar plot with error bars</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#jittered-data-points."><i class="fa fa-check"></i>Jittered data points.</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#violaviolin-plot"><i class="fa fa-check"></i>Viola/Violin plot</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#box-and-whiskers-plot"><i class="fa fa-check"></i>Box and whiskers plot</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#overlayed-densities"><i class="fa fa-check"></i>Overlayed densities</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#empirical-cumulative-distribution"><i class="fa fa-check"></i>Empirical cumulative distribution</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#comparisons"><i class="fa fa-check"></i>Comparisons</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#recommendations"><i class="fa fa-check"></i>Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-numerical-2-x-numerical-0"><i class="fa fa-check"></i>numerical ~ numerical (2 x numerical ~ 0)</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#scatter-and-heatmap"><i class="fa fa-check"></i>Scatter and heatmap</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#conditional-means"><i class="fa fa-check"></i>Conditional means</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-numerical-categorical"><i class="fa fa-check"></i>numerical ~ numerical + categorical</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#categorical-numerical"><i class="fa fa-check"></i>categorical ~ numerical</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#x-categorical-and-categorical-categorical"><i class="fa fa-check"></i>2 x categorical and categorical ~ categorical</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#heatmap"><i class="fa fa-check"></i>Heatmap</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#categorical-categorical"><i class="fa fa-check"></i>categorical ~ categorical</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#extra-plot-notes."><i class="fa fa-check"></i>Extra plot notes.</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-2-x-categorical"><i class="fa fa-check"></i>numerical ~ 2 x categorical</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#bin-width-and-bandwidths"><i class="fa fa-check"></i>bin width and bandwidths</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i>Probability</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-terms"><i class="fa fa-check"></i>Probability terms</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#absolute-probability-statements-prob"><i class="fa fa-check"></i>Absolute probability statements {prob}</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-comparisons"><i class="fa fa-check"></i>Probability comparisons</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#proportional-magnitudes-and-confusion"><i class="fa fa-check"></i>Proportional magnitudes and confusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-foundations"><i class="fa fa-check"></i>Foundations of probability</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#set-notation-for-combinations-of-outcomes."><i class="fa fa-check"></i>Set notation for combinations of outcomes.</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#basic-probability-definition-and-axioms"><i class="fa fa-check"></i>Basic probability definition and axioms</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#events-and-the-rules-of-probability."><i class="fa fa-check"></i>Events and the rules of probability.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-conditional"><i class="fa fa-check"></i>Conditional probability and Bayes</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#chain-rule"><i class="fa fa-check"></i>Chain rule</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#partitions-and-total-probability"><i class="fa fa-check"></i>Partitions and total probability</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#bayes-rule"><i class="fa fa-check"></i>Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-simulations"><i class="fa fa-check"></i>Simulation, Sampling and Monte Carlo.</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-long-run-frequency-and-the-law-of-large-numbers."><i class="fa fa-check"></i>Sampling, long-run frequency, and the law of large numbers.</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-to-estimate-event-probabilities."><i class="fa fa-check"></i>Sampling to estimate event probabilities.</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-of-a-conjunction-of-events"><i class="fa fa-check"></i>Probability of a conjunction of events</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-to-get-probability-of-disjunction"><i class="fa fa-check"></i>Sampling to get probability of disjunction</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-to-calculate-conditional-probability"><i class="fa fa-check"></i>Sampling to calculate conditional probability</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-rv-functions"><i class="fa fa-check"></i>Distribution functions: PDF, CDF, Quantile</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-distribution-mass-and-density-functions-p.d.f."><i class="fa fa-check"></i>Probability distribution (mass and density) functions (p.d.f.)</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#cumulative-distribution-functions-c.d.f."><i class="fa fa-check"></i>Cumulative distribution functions (c.d.f.)</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#quantile-functions-inverse-cdf."><i class="fa fa-check"></i>Quantile functions (inverse CDF).</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-expectation"><i class="fa fa-check"></i>Expectation and moments</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#there-are-a-few-useful-properties-about-how-the-mean-variance-skewness-and-excess-kurtosis-behave-under-various-operations"><i class="fa fa-check"></i>There are a few useful properties about how the Mean, Variance, Skewness, and Excess Kurtosis behave under various operations:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-clt"><i class="fa fa-check"></i>Central limit theorem and the normal distribution</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#requirements-for-clt-to-hold"><i class="fa fa-check"></i>Requirements for CLT to hold</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#normal-distribution"><i class="fa fa-check"></i>Normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html"><i class="fa fa-check"></i>Foundations of Statistics</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#NHST-simulation"><i class="fa fa-check"></i>Frequentist statistics via simulation</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#critical-values-alpha-power"><i class="fa fa-check"></i>Critical values, alpha, power</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#setting-up-the-alternate-hypothesis"><i class="fa fa-check"></i>Setting up the “Alternate hypothesis”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#figuring-out-power"><i class="fa fa-check"></i>Figuring out “power”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#figuring-out-alpha"><i class="fa fa-check"></i>Figuring out “alpha”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#showing-alpha-power"><i class="fa fa-check"></i>Showing alpha, power</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#figuring-out-the-critical-value."><i class="fa fa-check"></i>Figuring out the critical value.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#NHST-sampling"><i class="fa fa-check"></i>Sampling distributions</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#tl-dr."><i class="fa fa-check"></i>TL; DR.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#logic"><i class="fa fa-check"></i>Logic</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#expectation-about-the-sampling-distribution-of-the-sample-mean."><i class="fa fa-check"></i>Expectation about the sampling distribution of the sample mean.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#standard-error-of-the-sample-mean"><i class="fa fa-check"></i>Standard error (of the sample mean)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#NHST-basics-normal"><i class="fa fa-check"></i>Statistics via the Normal distribution</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#normal-null-hypothesis-significance-testing-nhst"><i class="fa fa-check"></i>(Normal) Null hypothesis significance testing (NHST)</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#normal-tests-with-sample-means"><i class="fa fa-check"></i>Normal tests with sample means</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#z-scores-and-z-tests"><i class="fa fa-check"></i>Z-scores and Z-tests</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#z-tests"><i class="fa fa-check"></i>Z-tests</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#normal-confidence-intervals-on-the-sample-mean"><i class="fa fa-check"></i>(Normal) Confidence intervals on the sample mean</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#what-are-these-percents-and-probabilities"><i class="fa fa-check"></i>What are these percents and probabilities?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#nhst-theory-normal"><i class="fa fa-check"></i>Null hypothesis significance testing</a>
<ul>
<li><a href="NHST.html#type-1-error-rate-alpha-alpha">Type 1 error rate: alpha (<span class="math inline">\(\alpha\)</span>)</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#the-alternate-model"><i class="fa fa-check"></i>The “alternate model”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#effect-size"><i class="fa fa-check"></i>Effect size</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#calculating-power-from-effect-size"><i class="fa fa-check"></i>Calculating power from effect size</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#visualizing-alpha-and-power"><i class="fa fa-check"></i>Visualizing alpha and power</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#how-power-changes."><i class="fa fa-check"></i>How power changes.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#calculating-n-for-a-desired-level-of-power."><i class="fa fa-check"></i>Calculating n for a desired level of power.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sign-and-magnitude-errors."><i class="fa fa-check"></i>Sign and magnitude errors.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#theory-binomial"><i class="fa fa-check"></i>Binomial: Probability to statistics</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#data-description-summary"><i class="fa fa-check"></i>Data description / summary</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#estimation"><i class="fa fa-check"></i>Estimation</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#null-hypothesis-significance-testing-nhst"><i class="fa fa-check"></i>Null Hypothesis Significance testing (NHST)</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#model-selection"><i class="fa fa-check"></i>Model selection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#t-distribution"><i class="fa fa-check"></i>t-distribution</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#tl-dr.-1"><i class="fa fa-check"></i>TL; DR.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sampling-distribution-of-sample-variance-and-t-statistic"><i class="fa fa-check"></i>Sampling distribution of sample variance, and t-statistic</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sample-variance"><i class="fa fa-check"></i>Sample variance</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sampling-distribution-of-the-sample-variance"><i class="fa fa-check"></i>Sampling distribution of the sample variance</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sampling-distribution-of-t-statistic"><i class="fa fa-check"></i>Sampling distribution of t-statistic</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#t-distribution-1"><i class="fa fa-check"></i>T-distribution</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#degrees-of-freedom."><i class="fa fa-check"></i>Degrees of freedom.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#summary."><i class="fa fa-check"></i>Summary.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html"><i class="fa fa-check"></i>(Student’s) t-tests</a>
<ul>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#sample-t-test"><i class="fa fa-check"></i>1-sample t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#paired-repeated-measures-t-test"><i class="fa fa-check"></i>Paired / repeated-measures t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#sample-presumed-equal-variance-t-test"><i class="fa fa-check"></i>2-sample, presumed equal variance, t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#sample-unequal-variance-t-test"><i class="fa fa-check"></i>2-sample, unequal variance, t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#power-calculations."><i class="fa fa-check"></i>Power calculations.</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#summary-of-tests-for-the-mean-and-effect-sizes"><i class="fa fa-check"></i>Summary of tests for the mean and effect sizes</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#math."><i class="fa fa-check"></i>Math.</a>
<ul>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#math-behind-2-sample-equal-variance-t-test"><i class="fa fa-check"></i>Math behind 2-sample equal variance t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#math-behind-unequal-variance-t-test"><i class="fa fa-check"></i>Math behind unequal variance t-test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="binomial-test.html"><a href="binomial-test.html"><i class="fa fa-check"></i>Binomial test.</a>
<ul>
<li class="chapter" data-level="" data-path="binomial-test.html"><a href="binomial-test.html#estimating-proportions."><i class="fa fa-check"></i>Estimating proportions.</a></li>
<li class="chapter" data-level="" data-path="binomial-test.html"><a href="binomial-test.html#sign-test-test-for-percentiles"><i class="fa fa-check"></i>Sign test, test for percentiles</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html"><i class="fa fa-check"></i>Pearson’s Chi-squared test</a>
<ul>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#goodness-of-fit-test"><i class="fa fa-check"></i>“Goodness of fit” test</a>
<ul>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#implementation-in-r"><i class="fa fa-check"></i>Implementation in R</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#chi-squared-test-calculations"><i class="fa fa-check"></i>Chi-squared test calculations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#test-for-independence"><i class="fa fa-check"></i>Test for independence</a>
<ul>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#implementation-in-r-1"><i class="fa fa-check"></i>Implementation in R</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#independence-test-calculations"><i class="fa fa-check"></i>Independence test calculations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#more-than-2-way-contingency-table"><i class="fa fa-check"></i>More than 2-way contingency table</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#mathematical-rationale"><i class="fa fa-check"></i>Mathematical rationale</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#limitations"><i class="fa fa-check"></i>Limitations</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#fishers-exact-test"><i class="fa fa-check"></i>Fisher’s “exact” test</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i>Bivariate linear relationships</a>
<ul>
<li><a href="bivariate.html#linear-relationships"><em>Linear</em> relationships</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#covariance-and-correlation-measuring-the-linear-dependence."><i class="fa fa-check"></i>Covariance and correlation: Measuring the linear dependence.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#covariance"><i class="fa fa-check"></i>Covariance</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#correlation-coefficient"><i class="fa fa-check"></i>Correlation coefficient</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#ols-regression-predicting-the-mean-of-y-for-a-given-x"><i class="fa fa-check"></i>(OLS) Regression: Predicting the mean of y for a given x</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#difference-between-yx-xy-and-the-principle-component-line"><i class="fa fa-check"></i>Difference between y~x, x~y, and the principle component line</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#partitioning-variance"><i class="fa fa-check"></i>Partitioning variance</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-a-linear-relationship."><i class="fa fa-check"></i>Significance of a linear relationship.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#prediction-from-regression."><i class="fa fa-check"></i>Prediction from regression.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-anscombe"><i class="fa fa-check"></i>Anscombe’s quartet</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-covariance"><i class="fa fa-check"></i>Covariance</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#estimating-covariance."><i class="fa fa-check"></i>Estimating covariance.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-correlation"><i class="fa fa-check"></i>Correlation</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#correlation-as-the-slope-of-z-scores"><i class="fa fa-check"></i>Correlation as the slope of z-scores</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#coefficient-of-determination"><i class="fa fa-check"></i>Coefficient of determination</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-ols"><i class="fa fa-check"></i>Ordinary Least-Squares (OLS) Regression</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#regression-terminology"><i class="fa fa-check"></i>Regression terminology</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#estimating-the-regression-line."><i class="fa fa-check"></i>Estimating the regression line.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#standard-errors-of-regression-coefficients"><i class="fa fa-check"></i>Standard errors of regression coefficients</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#confidence-intervals-and-tests-for-regression-coefficients"><i class="fa fa-check"></i>Confidence intervals and tests for regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-lines"><i class="fa fa-check"></i>y~x vs x~y vs principle component line</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-determination"><i class="fa fa-check"></i>Partitioning variance and the coefficient of determination.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#calculating-sums-of-squares."><i class="fa fa-check"></i>Calculating sums of squares.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-significance"><i class="fa fa-check"></i>Significance of linear relationship.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-slope."><i class="fa fa-check"></i>Significance of slope.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-pairwise-correlation"><i class="fa fa-check"></i>Significance of pairwise correlation</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-variance-partition."><i class="fa fa-check"></i>Significance of variance partition.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#isomorphism-with-one-response-and-one-predictor"><i class="fa fa-check"></i>Isomorphism with one response and one predictor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-prediction"><i class="fa fa-check"></i>Regression prediction.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#predicting-mean-y-given-x."><i class="fa fa-check"></i>Predicting mean y given x.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#predicting-new-y-given-x."><i class="fa fa-check"></i>Predicting new y given x.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#visualizing-the-difference"><i class="fa fa-check"></i>Visualizing the difference</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-ols-diagnostics"><i class="fa fa-check"></i>Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#assumption-relationship-between-y-and-x-is-well-described-by-a-line."><i class="fa fa-check"></i>Assumption: relationship between y and x is well described by a line.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#assumption-out-estimates-are-not-driven-by-a-few-huge-outliers"><i class="fa fa-check"></i>Assumption: out estimates are not driven by a few huge outliers</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#assumption-errors-are-independent-identically-distributed-normal."><i class="fa fa-check"></i>Assumption: errors are independent, identically distributed, normal.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#testing-assumptions"><i class="fa fa-check"></i>Testing assumptions</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">UCSD Psyc 201ab / CSS 205 / Psyc 193</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bivariate" class="section level1">
<h1>Bivariate linear relationships</h1>
<p>When we have two variables measured per “unit” (e.g., measure height and weight for each person), we can refer to this as “bivariate” data. We already discussed how to analyze <a href="#nhst-chi-squared">contingency tables</a> when dealing with bivariate categorical data, here we are concerned with bivariate numerical data that may have a <em>linear relationship</em>.</p>
<p>To play with these measures, we will consider Karl Pearson’s data on the heights of fathers and their sons. (In 1895 Pearson worked out the formula for calculating what we now call the correlation coefficient.) Generally, we will look at a bivariate numerical relationship in a scatterplot, like the one below.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="bivariate.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse, <span class="at">quietly=</span><span class="cn">TRUE</span>)</span>
<span id="cb332-2"><a href="bivariate.html#cb332-2" aria-hidden="true" tabindex="-1"></a>heights <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&#39;http://vulstats.ucsd.edu/data/Pearson.csv&#39;</span>)</span>
<span id="cb332-3"><a href="bivariate.html#cb332-3" aria-hidden="true" tabindex="-1"></a>heights <span class="sc">%&gt;%</span> </span>
<span id="cb332-4"><a href="bivariate.html#cb332-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>Father, <span class="at">y=</span>Son))<span class="sc">+</span></span>
<span id="cb332-5"><a href="bivariate.html#cb332-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-161-1.png" width="1200" /></p>
<div id="linear-relationships" class="section level2">
<h2><em>Linear</em> relationships</h2>
<p>Everything we discuss in this section is specific to measuring a <em>linear</em> relationship, meaning (informally) that the scatterplot looks like it would be meaningful to draw a line through it. This has two implications.</p>
<ol style="list-style-type: decimal">
<li>Lots of patterns of bivariate data have a relationship between x and y (meaning that we can learn something about y by knowing x, or vice versa), but that relationship is not linear. The bottom row of the graph below (from wikipedia) does a great job showing many such cases.</li>
</ol>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Many different patterns of data might yield exactly the same line, and line statistics, and our simple measures of a linear relationship will not be able to distinguish among them. The canonical example of this is Anscombe’s quartet (below; <a href="bivariate.html#bivariate-anscombe">graph it yourself</a>), which shows 4 sets of data with obviously very different relationships, but which have the same correlation, covariance, linear slope, as well as marginal means and standard deviations of x and y.</li>
</ol>
<p><img src="_main_files/figure-html/unnamed-chunk-162-1.png" width="2400" /></p>
<p>What this means, is that you should always look at a scatterplot of the data, don’t just blindly rely on the numerical summaries.</p>
</div>
<div id="covariance-and-correlation-measuring-the-linear-dependence." class="section level2">
<h2>Covariance and correlation: Measuring the linear dependence.</h2>
<div id="covariance" class="section level3">
<h3>Covariance</h3>
<p>The co-variance measures whether, when x is bigger than the mean of x, is y also bigger than the mean of y, and what is the overall scale of this co-variation?</p>
<p><span class="math display">\[\operatorname{covariance}(x,y) =  s_{xy} = \frac{1}{n-1}\sum\limits_{i=1}^n (x_i - \bar x)(y_i - \bar y)\]</span></p>
<ul>
<li><p>To get an intuition for what this is doing, consider when the expression being summed (<span class="math inline">\((x_i - \bar x)(y_i - \bar y)\)</span>) will be positive, and when it will be negative. When x and y both deviate from their means in the same direction (either both larger, or both smaller than their mean), this product will be positive. When they deviate in different directions (one smaller and the other larger than their mean), this product will be negative. Thus the whole sum will be very positive if x and y tend to deviate in the same direction as their respective means, and will be very negative if they tend to deviate in opposite directions.</p></li>
<li><p>Note that this expression (<span class="math inline">\((x_i - \bar x)(y_i - \bar y)\)</span>) will give the same answer regardless of which variable we call x, and which we call y. So the covariance of x and y is the same as the covariance of y and x.</p></li>
<li><p>Also consider how this expression (<span class="math inline">\((x_i - \bar x)(y_i - \bar y)\)</span>) will change if we (a) add a constant to x, or y, or both, and (b) multiply x or y or both by a constant. If we add a constant, it will just end up being subtracted out, since we consider only differences of a given value from its mean, thus the covariance will be the same regardless of where we “center” x and y. However, if we multiply x (or y) by something, we will end up scaling the distance between x and its mean. Thus, if we scale by a factor larger than 1, we will get a larger magnitude of this expression, and thus the covariance; and if we scale by a factor between 0 and 1, we will get a smaller covariance. So while the covariance doesn’t depend on the <em>location</em> it does depend on the <em>scale</em> of our variables.</p></li>
</ul>
<p>In R we can calculate the covariance with the <code>cov</code> function:</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="bivariate.html#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(heights<span class="sc">$</span>Father, heights<span class="sc">$</span>Son)</span></code></pre></div>
<pre><code>## [1] 3.875382</code></pre>
<p>Here are a few more <a href="bivariate.html#bivariate-covariance">detailed notes on the covariance</a>.</p>
</div>
<div id="correlation-coefficient" class="section level3">
<h3>Correlation coefficient</h3>
<p>The correlation coefficient re-scales the covariance by the standard deviations of x and y, so it yields a measure of the linear relationship that is scale invariant, and is always between -1 and +1.</p>
<p><span class="math display">\[\operatorname{correlation}(x,y) =  r_{xy} = \frac{\operatorname{covariance}(x,y)}{s_x s_y} = \frac{s_{xy}}{s_x s_y}\]</span></p>
<p>There are a bunch of ways to think about what the correlation means (<a href="https://www.jstor.org/stable/2685263">this paper lists 13</a>), but there are a few that I consider to be particularly useful:</p>
<ul>
<li><p>The <strong>correlation is a scale-invariant covariance</strong>. It is equal to the covariance of the z-score of x and the z-score of y. Thus, it disregards both the location and the scale of the variables, so we will get the same correlation regardless of how we <em>linearly transform</em> (multiply by a constant and/or add a constant) x and y (provided we don’t multiply by a negative number – that will change the sign, but not the magnitude, of the correlation).</p></li>
<li><p>The <strong>correlation is the slope of the z-scores</strong>: meaning that if an x is 2 standard deviations above the mean of x, we would expect the corresponding y to be <span class="math inline">\(r_{xy}*2\)</span> standard deviations above the mean of y (and vice versa).</p></li>
<li><p>The <strong>correlation squared is the “coefficient of determination”</strong>. We will talk more about this when we consider the partitioning of variance, but this basically means: the proportion of the variance of y that we can explain by its relationship with x (and vice versa) is <span class="math inline">\(r_{xy}^2\)</span>.</p></li>
</ul>
<p>In R, we can calculate the correlation via <code>cor</code>, or calculate the correlation and test it’s significance via <code>cor.test</code>:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="bivariate.html#cb335-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(heights<span class="sc">$</span>Father, heights<span class="sc">$</span>Son)</span></code></pre></div>
<pre><code>## [1] 0.5011627</code></pre>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="bivariate.html#cb337-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor.test</span>(heights<span class="sc">$</span>Father, heights<span class="sc">$</span>Son)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment
##  correlation
## 
## data:  heights$Father and heights$Son
## t = 18.997, df = 1076, p-value &lt;
## 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4550726 0.5445746
## sample estimates:
##       cor 
## 0.5011627</code></pre>
<p>For more details on the correlation, here are some <a href="#bivariate-corelation">detailed notes</a>.</p>
</div>
</div>
<div id="ols-regression-predicting-the-mean-of-y-for-a-given-x" class="section level2">
<h2>(OLS) Regression: Predicting the mean of y for a given x</h2>
<p>A linear regression of y~x (“~” here is read “as a function of”) finds the “best fitting” line of the form <span class="math inline">\(a \cdot x + b\)</span> to estimate the “conditional mean” of y at a given x.</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="bivariate.html#cb339-1" aria-hidden="true" tabindex="-1"></a>heights <span class="sc">%&gt;%</span> </span>
<span id="cb339-2"><a href="bivariate.html#cb339-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>Father, <span class="at">y=</span>Son))<span class="sc">+</span></span>
<span id="cb339-3"><a href="bivariate.html#cb339-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb339-4"><a href="bivariate.html#cb339-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-165-1.png" width="1200" /></p>
<p>In the <a href="bivariate.html#bivariate-ols">detailed notes</a> we go through the mathematical definitions, but here, let’s just sort out what the different words in the sentence above mean.</p>
<ul>
<li><p>Regression line of y~x estimates the <strong>conditional mean</strong> of y for a given x. Therefore, the y value of the line at a given x is the estimated mean of the y values with that paired x. For instance, the Son ~ Father line shown above has y=72.4 at x=75. This indicates that we estimate the mean height of Son, whose fathers are 75" tall, to be 72.4" tall.</p></li>
<li><p>A <strong>line</strong> (<span class="math inline">\(\hat y = a \cdot x + b\)</span>) is characterized by its <strong>slope</strong> (<span class="math inline">\(a\)</span>) and <strong>intercept</strong> (<span class="math inline">\(b\)</span>).<br />
The slope indicates how many units <span class="math inline">\(\hat y\)</span> will increase (or decrease in the case of a negative slope) every time x goes up by one unit, thus it is in units of <span class="math inline">\(y/x\)</span>. For instance if the slope of a line predicting weight (kg) as a function of height (cm) is 0.44 (kg/cm), that means that we expect someone who is 1 cm taller to weight 0.44 kg more.<br />
The intercept tells us the value of the line (<span class="math inline">\(\hat y\)</span>), and thus our predicted mean y, when <span class="math inline">\(x=0\)</span>. So an intercept of of 2.46 on a line predicting weight (kg) as a function of height (cm) says that we predict people who are 0 cm tall to weigh 2.46 kg.</p></li>
<li><p>For some parameters (in our case, a slope and intercept of a line) to be “<strong>best fitting</strong>,” that means that of all the values the parameters could have taken on, the “best fitting” values optimize some function that evaluates the overall fit. In the simple regression case, we evaluate the overall fit as the sum of squared errors, the smaller the better: <span class="math inline">\(\operatorname{SSE} = \sum_{i=1}^n (y_i - \hat y_i)^2 = \sum_{i=1}^n (y_i - (a\cdot x_i + b))^2\)</span>. So the “best fitting” line is the line with a slope (<span class="math inline">\(a\)</span>) and intercept (<span class="math inline">\(b\)</span>) that yields the smallest sum of squared errors. These are also the “maximum likelihood” slope/intercept because we usually specify a probability model for the data which says that the y values are Normally distributed around a mean which varies linearly with x: <span class="math inline">\(y_i \sim \operatorname{Normal}(a\cdot x_i + b, \sigma_e)\)</span>. Thus, although ‘least squares’ regression can be motivated simply by asserting that we want to minimize squared error, it also happens to be the correct procedure for estimating the maximum likelihood parameters under the standard regression model.</p></li>
</ul>
<p>In R, we can do all this (and a whole lot more) with a single command <code>lm</code>, which stands for “linear model.” It takes as an argument a formula, and a data frame. A formula is written with the syntax <code>respons.variable ~ explanatory.variable</code>, so if we want to estimate the regression line predicting Son’ height as a function of fathers’ height, we would invoke the following incantation:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="bivariate.html#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(<span class="at">data=</span>heights, Son <span class="sc">~</span> Father)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Son ~ Father, data = heights)
## 
## Coefficients:
## (Intercept)       Father  
##      33.893        0.514</code></pre>
<div id="difference-between-yx-xy-and-the-principle-component-line" class="section level3">
<h3>Difference between y~x, x~y, and the principle component line</h3>
<p>The “best fitting” line shown in the above section, doesn’t really <em>look</em> like the best fitting line. What gives? Consider the three lines below…</p>
<p><img src="_main_files/figure-html/unnamed-chunk-167-1.png" width="1800" /></p>
<p>I bet the black line above looks like the <em>best</em> line to you, and both the red and blue lines seem off, right? So what are these lines?<br />
Blue: the <strong>y~x line</strong>.<br />
Red: the <strong>x~y line</strong>.<br />
Black: the <strong>principle component line</strong>.</p>
<p>Why is the “best looking” line not the “best fitting” y~x line? While we’re at it, why is the y~x line different from the x~y line? The answer to all of these is that when we do a regression predicting y from x, we <em>only care about error in y</em>. We minimize the squared deviations of each data point from the line in y, while keeping x constant. Consider this a teaser explanation, and go read the more <a href="bivariate.html#bivariate-lines">detailed explanation</a>.</p>
</div>
</div>
<div id="partitioning-variance" class="section level2">
<h2>Partitioning variance</h2>
<p>As we move toward more complicated data, and away from simply comparing means between groups, it is useful to consider our analysis goals as <em>partitioning variance</em>. We want to separate variability in some response variable into different sources. In the simple linear regression case, we are just going to separate out the variability of y into the linear ‘signal’ (variability in y which we can explain via a linear relationship with x), and ‘noise’ (all other variability). When we move on to more complicated regression setups, we will be using more explanatory variables, and thus will be partitioning variability into more sources. It is important to keep in mind that ‘noise’ depends very much on the model we are considering. Perhaps all the variability in sons’ heights we can’t explain with fathers’ heights might be explained by ‘volume of breast milk consumed in the first year of life,’ or some other variable we don’t have access to. Thus, the ‘noise,’ is just <em>unexplained/unmodeled variance</em>.</p>
<p>We have some detailed notes on <a href="bivariate.html#bivariate-determination">partitioning variance</a>, but briefly. When we partition variance, we mostly just consider the partitioning of the “sums of squares,” which we can get in R via the <code>anova</code> (analysis of variance) command, which tells us the variability in y attributable to our explanatory variable, and that which is left unexplained (residuals).</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="bivariate.html#cb342-1" aria-hidden="true" tabindex="-1"></a>lm.son.father <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>heights, Son<span class="sc">~</span>Father)</span>
<span id="cb342-2"><a href="bivariate.html#cb342-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.son.father)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Son
##             Df Sum Sq Mean Sq F value
## Father       1 2145.4 2145.35   360.9
## Residuals 1076 6396.3    5.94        
##              Pr(&gt;F)    
## Father    &lt; 2.2e-16 ***
## Residuals              
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1</code></pre>
<p>The proportion of variability in y that we can explain by taking into account the linear relationship with x, is the correlation squared (<span class="math inline">\(r_{xy}^2\)</span>). However, R will just give us this number if we look at the detailed summary of the linear model we fit (via the <code>summary</code> function).</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="bivariate.html#cb344-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.son.father)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Son ~ Father, data = heights)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.8910 -1.5361 -0.0092  1.6359  8.9894 
## 
## Coefficients:
##             Estimate Std. Error t value
## (Intercept) 33.89280    1.83289   18.49
## Father       0.51401    0.02706   19.00
##             Pr(&gt;|t|)    
## (Intercept)   &lt;2e-16 ***
## Father        &lt;2e-16 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 2.438 on 1076 degrees of freedom
## Multiple R-squared:  0.2512, Adjusted R-squared:  0.2505 
## F-statistic: 360.9 on 1 and 1076 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>That ‘Multiple R-squared’ number is “the coefficient of determination,” which is the proportion of variance we can explain with the linear model, and in this simple one-variable regression case, it is also just the correlation squared.</p>
</div>
<div id="significance-of-a-linear-relationship." class="section level2">
<h2>Significance of a linear relationship.</h2>
<p>We have seen a few p values above. One in <code>cor.test</code>, that told us the the correlation between fathers’ and sons’ heights is larger than expected by chance. One in <code>anova(lm(...))</code> which told us that a linear model of sons~fathers explained more variance in sons heights than we would expect by chance. Another in <code>summary(lm(...))</code> which told us that the father coefficient in the linear model we fit (<span class="math inline">\(\mbox{son} = \hat \beta_{\mbox{father}} \cdot \mbox{father} + \hat \beta_0\)</span>) is significantly greater than zero.</p>
<p>For the simple linear regression case, these are all the same <em>by definition</em>! They are all asking “is there more of a linear relationship than we expect from chance?” In the case of <em>multiple regression</em> these will all be different, and will all be asking different questions.<br />
(In this father-son data, all these p-values are so small, that they are past the limit of our computer’s ability to represent tiny numbers. Thus, even if they were different, we wouldn’t be able to tell. To see a case in which they are less tiny, <a href="bivariate.html#bivariate-significance">look here</a>.</p>
</div>
<div id="prediction-from-regression." class="section level2">
<h2>Prediction from regression.</h2>
<p>A regression line is our estimate of the <em>mean of y for a given x</em> (given the assumption that these conditional means fall on a line). Since there is uncertainty inherent in all estimation, the slope and intercept are uncertain, and thus the estimated mean for a given x is also uncertain. We can translate our standard errors of the slope and intercept into a standard error of the conditional mean of y for a given x. This is our uncertainty about the <em>conditional mean of y</em>, and we can extract it from a given linear model using the <code>predict.lm</code> function, with <code>interval='coonfidence'</code> (Note that the syntax for the predict function is to provide the fitted model object (<code>m</code>) and a data frame of new observations that we want to make new predictions on):</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="bivariate.html#cb346-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(m <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data=</span>heights, Son <span class="sc">~</span> Father))</span>
<span id="cb346-2"><a href="bivariate.html#cb346-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">Father=</span><span class="dv">72</span>), <span class="at">interval=</span><span class="st">&#39;confidence&#39;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 70.90123 70.62981 71.17264</code></pre>
<p>By default, this returns the estimated conditional mean of y, at the x values provided in <code>newdata</code>; here, the estimated mean height of sons whose fathers were 72" tall, and a 95% confidence interval on that mean.</p>
<p>However, the confidence interval on the mean doesn’t give us a good idea of what heights we might expect of these sons – it tells us that on average they will be about 70.9" tall, but any given son will vary a lot from the mean. To put a confidence interval on the height of a particular son we might see, we need not only take into account out uncertainty in estimating the mean height, but also how much variability there is in heights around the mean. The confidence interval for that is given with the <code>interval = 'prediction'</code> option:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="bivariate.html#cb348-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.lm</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">Father=</span><span class="dv">72</span>), <span class="at">interval=</span><span class="st">&#39;prediction&#39;</span>)</span></code></pre></div>
<pre><code>##        fit     lwr      upr
## 1 70.90123 66.1095 75.69296</code></pre>
<p>This interval is much broader because, although we can estimate the best fitting line (and thus the mean) very well, the individual heights have a lot of variability around that line. For more on these intervals, <a href="bivariate.html#bivariate-prediction">read here</a></p>

</div>
<div id="bivariate-anscombe" class="section level2">
<h2>Anscombe’s quartet</h2>
<p>The data for <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe’s quartet</a> are built into R, and it is a somewhat useful exercise to reshape the data frame and produce the plot.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="bivariate.html#cb350-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(anscombe)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    11 obs. of  8 variables:
##  $ x1: num  10 8 13 9 11 14 6 4 12 7 ...
##  $ x2: num  10 8 13 9 11 14 6 4 12 7 ...
##  $ x3: num  10 8 13 9 11 14 6 4 12 7 ...
##  $ x4: num  8 8 8 8 8 8 8 19 8 8 ...
##  $ y1: num  8.04 6.95 7.58 8.81 8.33 ...
##  $ y2: num  9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ...
##  $ y3: num  7.46 6.77 12.74 7.11 7.81 ...
##  $ y4: num  6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ...</code></pre>
<p>Note that these data are in “wide” format, we have 4 x columns and 4 y columns, rather than having an x column, a y column, and a column indicating whether the values correspond to set 1, 2, 3, or 4. We will reshape these data.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="bivariate.html#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s use tidyr and dplyr to reshape this into long format.</span></span>
<span id="cb352-2"><a href="bivariate.html#cb352-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb352-3"><a href="bivariate.html#cb352-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb352-4"><a href="bivariate.html#cb352-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb352-5"><a href="bivariate.html#cb352-5" aria-hidden="true" tabindex="-1"></a><span class="co"># first, we add a new variable (observation number)</span></span>
<span id="cb352-6"><a href="bivariate.html#cb352-6" aria-hidden="true" tabindex="-1"></a>anscombe.tidy <span class="ot">=</span> <span class="fu">mutate</span>(anscombe, <span class="at">observation=</span><span class="fu">seq_len</span>(<span class="fu">n</span>()))</span>
<span id="cb352-7"><a href="bivariate.html#cb352-7" aria-hidden="true" tabindex="-1"></a><span class="co"># now we reshape into (too) long format (all x and y values in one column)</span></span>
<span id="cb352-8"><a href="bivariate.html#cb352-8" aria-hidden="true" tabindex="-1"></a>anscombe.tidy <span class="ot">=</span> <span class="fu">gather</span>(anscombe.tidy, key, value, <span class="sc">-</span>observation)</span>
<span id="cb352-9"><a href="bivariate.html#cb352-9" aria-hidden="true" tabindex="-1"></a><span class="co"># now we separate the key variable (&quot;x1&quot;, &quot;x2&quot;, .. &quot;y1&quot;) into two variables:</span></span>
<span id="cb352-10"><a href="bivariate.html#cb352-10" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;variable&quot; (&quot;x&quot; or &quot;y&quot;) and &quot;set&quot; (1, 2, 3, 4)</span></span>
<span id="cb352-11"><a href="bivariate.html#cb352-11" aria-hidden="true" tabindex="-1"></a>anscombe.tidy <span class="ot">=</span> <span class="fu">separate</span>(anscombe.tidy, </span>
<span id="cb352-12"><a href="bivariate.html#cb352-12" aria-hidden="true" tabindex="-1"></a>                         key, </span>
<span id="cb352-13"><a href="bivariate.html#cb352-13" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">c</span>(<span class="st">&quot;variable&quot;</span>, <span class="st">&quot;set&quot;</span>), </span>
<span id="cb352-14"><a href="bivariate.html#cb352-14" aria-hidden="true" tabindex="-1"></a>                         <span class="dv">1</span>, </span>
<span id="cb352-15"><a href="bivariate.html#cb352-15" aria-hidden="true" tabindex="-1"></a>                         <span class="at">convert =</span> <span class="cn">TRUE</span>)</span>
<span id="cb352-16"><a href="bivariate.html#cb352-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we change the &quot;set&quot; to be a character variable</span></span>
<span id="cb352-17"><a href="bivariate.html#cb352-17" aria-hidden="true" tabindex="-1"></a>anscombe.tidy <span class="ot">=</span> <span class="fu">mutate</span>(anscombe.tidy, </span>
<span id="cb352-18"><a href="bivariate.html#cb352-18" aria-hidden="true" tabindex="-1"></a>                       <span class="at">set =</span> <span class="fu">c</span>(<span class="st">&quot;I&quot;</span>, <span class="st">&quot;II&quot;</span>, <span class="st">&quot;III&quot;</span>, <span class="st">&quot;IV&quot;</span>)[set])</span>
<span id="cb352-19"><a href="bivariate.html#cb352-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Finally, we spread into a wide format x and y values:</span></span>
<span id="cb352-20"><a href="bivariate.html#cb352-20" aria-hidden="true" tabindex="-1"></a>anscombe.tidy <span class="ot">=</span> <span class="fu">spread</span>(anscombe.tidy, variable, value)</span>
<span id="cb352-21"><a href="bivariate.html#cb352-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-22"><a href="bivariate.html#cb352-22" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(anscombe.tidy)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    44 obs. of  4 variables:
##  $ observation: int  1 1 1 1 2 2 2 2 3 3 ...
##  $ set        : chr  &quot;I&quot; &quot;II&quot; &quot;III&quot; &quot;IV&quot; ...
##  $ x          : num  10 10 10 8 8 8 8 8 13 13 ...
##  $ y          : num  8.04 9.14 7.46 6.58 6.95 8.14 6.77 5.76 7.58 8.74 ...</code></pre>
<p>Great, now these data are in “long” format, and we can easily handle them in ggplot to produce a multi-panel plot.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="bivariate.html#cb354-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(anscombe.tidy, <span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y))<span class="sc">+</span><span class="fu">facet_grid</span>(.<span class="sc">~</span>set)<span class="sc">+</span></span>
<span id="cb354-2"><a href="bivariate.html#cb354-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>, <span class="at">formula=</span>y<span class="sc">~</span>x)<span class="sc">+</span></span>
<span id="cb354-3"><a href="bivariate.html#cb354-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;black&quot;</span>, <span class="at">fill=</span><span class="st">&quot;orange&quot;</span>, <span class="at">shape=</span><span class="dv">21</span>, <span class="at">size=</span><span class="fl">3.5</span>)<span class="sc">+</span><span class="fu">theme_minimal</span>()<span class="sc">+</span><span class="fu">theme</span>(<span class="at">text=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">16</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-174-1.png" width="3000" /></p>
<p>What’s notable about Anscombe’s quartet is that their summary statistics are so similar.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="bivariate.html#cb355-1" aria-hidden="true" tabindex="-1"></a>anscombe.tidy <span class="sc">%&gt;%</span> </span>
<span id="cb355-2"><a href="bivariate.html#cb355-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(set) <span class="sc">%&gt;%</span></span>
<span id="cb355-3"><a href="bivariate.html#cb355-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb355-4"><a href="bivariate.html#cb355-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">n=</span><span class="fu">n</span>(),</span>
<span id="cb355-5"><a href="bivariate.html#cb355-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">mean.x=</span><span class="fu">round</span>(<span class="fu">mean</span>(x),<span class="dv">2</span>), </span>
<span id="cb355-6"><a href="bivariate.html#cb355-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">mean.y=</span><span class="fu">round</span>(<span class="fu">mean</span>(y),<span class="dv">2</span>), </span>
<span id="cb355-7"><a href="bivariate.html#cb355-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">sd.x=</span><span class="fu">round</span>(<span class="fu">sd</span>(x),<span class="dv">2</span>), </span>
<span id="cb355-8"><a href="bivariate.html#cb355-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">sd.y=</span><span class="fu">round</span>(<span class="fu">sd</span>(y),<span class="dv">2</span>), </span>
<span id="cb355-9"><a href="bivariate.html#cb355-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">correlation=</span><span class="fu">round</span>(<span class="fu">cor</span>(x,y),<span class="dv">2</span>))</span></code></pre></div>
<pre><code>## # A tibble: 4 x 7
##   set       n mean.x mean.y  sd.x  sd.y
##   &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 I        11      9    7.5  3.32  2.03
## 2 II       11      9    7.5  3.32  2.03
## 3 III      11      9    7.5  3.32  2.03
## 4 IV       11      9    7.5  3.32  2.03
## # … with 1 more variable:
## #   correlation &lt;dbl&gt;</code></pre>
<p>These matching ns, means, standard deviations, and correlation, means that the slope and intercept, and corresponding statistical tests, are all equivalent; yet the data look clearly different.</p>
<p>The point here is: <strong>summary statistics of marginal distributions and linear relationships necessarily overlook some (potentially important) aspects of the data.</strong></p>

<p><img src="_main_files/figure-html/unnamed-chunk-176-1.png" width="1800" /></p>
</div>
<div id="bivariate-covariance" class="section level2">
<h2>Covariance</h2>
<p>Generally, bivariate numerical data are often summarized in terms of their mean and <a href="https://en.wikipedia.org/wiki/Covariance_matrix"><em>covariance matrix</em></a>. Since we are avoiding dealing with linear algebra in this class, we will not deal with this matrix directly. Instead we will consider the different components of a covariance matrix for a bivariate distribution.</p>
<p>The elements of a covariance matrix (usually denoted with a capital sigma: <span class="math inline">\(\Sigma\)</span>) for two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are <span class="math inline">\(\sigma_x^2\)</span> (the variance of <span class="math inline">\(x\)</span>), <span class="math inline">\(\sigma_y^2\)</span> (the variance of <span class="math inline">\(y\)</span>), and <span class="math inline">\(\sigma_{xy}\)</span> the <strong>covariance</strong> of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p><span class="math inline">\(\Sigma_{xy} = \begin{bmatrix}\sigma_x^2 &amp; \sigma_{xy} \\ \sigma_{xy} &amp; \sigma_y^2\end{bmatrix}\)</span></p>
<p>The covariance of two random variables (<span class="math inline">\(\sigma_{xy}\)</span>) is defined as the expectation of the products of deviations from the mean:</p>
<p><span class="math inline">\(\sigma_{XY} = \operatorname{Cov}[X,Y] = \mathbb{E}\left[{(X-\mu_X)(Y-\mu_Y)}\right] = \int\limits_X \int\limits_Y (X-\mu_X)(Y-\mu_Y) P(X,Y) \, dX \, dY\)</span></p>
<p>It is worth noting the similarity between the definition of the <em>variance</em> and the definition of the covariance:</p>
<p><span class="math inline">\(\operatorname{Var}[X] = \mathbb{E}\left[{(X-\mu_X)^2}\right]= \mathbb{E}\left[{(X-\mu_X)(X-\mu_X)}\right]\)</span></p>
<p>The similarity here is that the variance of <span class="math inline">\(X\)</span> can be thought of as the covariance of <span class="math inline">\(X\)</span> with itself:<br />
<span class="math inline">\(\operatorname{Var}[X] = \operatorname{Cov}[X,X]\)</span></p>
<p>What will the definition of <span class="math inline">\(\operatorname{Cov}[X,Y]\)</span> do?</p>
<ul>
<li><span class="math inline">\((X-\mu_X)(Y-\mu_Y)\)</span> will be positive for combinations of X and Y that both deviate in the same direction from their respective means (either both higher, or both lower, than their means)<br />
</li>
<li><span class="math inline">\((X-\mu_X)(Y-\mu_Y)\)</span> will be negative for combinations of X and Y that deviate in different directions from their means (X is higher than its mean, Y is lower)<br />
</li>
<li>Consequently, if combinations of X and Y that deviate in the same direction are more common than those that deviate in different directions, the covariance will be positive; vice versa for negative; and same-direction and different-direction deviation pairs are equally common, the covariance will be zero.</li>
</ul>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="bivariate.html#cb357-1" aria-hidden="true" tabindex="-1"></a>cvs <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.8</span>, <span class="dv">0</span>, <span class="fl">0.8</span>)</span>
<span id="cb357-2"><a href="bivariate.html#cb357-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>()</span>
<span id="cb357-3"><a href="bivariate.html#cb357-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(cv <span class="cf">in</span> cvs){</span>
<span id="cb357-4"><a href="bivariate.html#cb357-4" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">=</span> <span class="fu">rbind</span>(df, <span class="fu">data.frame</span>(MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(<span class="at">n=</span><span class="dv">500</span>, <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, cv, cv, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)),</span>
<span id="cb357-5"><a href="bivariate.html#cb357-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">Covariance=</span><span class="fu">as.character</span>(cv)))</span>
<span id="cb357-6"><a href="bivariate.html#cb357-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb357-7"><a href="bivariate.html#cb357-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(X1,X2))<span class="sc">+</span><span class="fu">facet_grid</span>(.<span class="sc">~</span>Covariance)<span class="sc">+</span><span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-177-1.png" width="3000" /></p>
<ul>
<li>If the overall variance of X or Y changes, the scale of the scared deviations of them from their means will change, and consequently, so will the covariance. This means that the covariance measures both the strength of the linear relationship between X and Y, as well as the overall spread of X and Y.</li>
</ul>
<div id="covariance-decomposed-into-shared-variance." class="section level4">
<h4>Covariance decomposed into shared variance.</h4>
<p>One helpful way to think about the “covariance” is as arising from some shared variable.</p>
<p><span class="math inline">\(W \sim \operatorname{Normal}(0,\sigma_W)\)</span><br />
<span class="math inline">\(X = \mu_X + aW + U \text{, where  } U \sim \operatorname{Normal}(0,\sigma_U)\)</span><br />
<span class="math inline">\(Y = \mu_Y + bW + V \text{, where  } V \sim \operatorname{Normal}(0,\sigma_V)\)</span></p>
<p>In this case, the variance of <span class="math inline">\(X\)</span> will be <span class="math inline">\(\sigma_X^2 = a^2 \sigma_W^2 + \sigma_U^2\)</span>,<br />
the variance of <span class="math inline">\(Y\)</span> will be <span class="math inline">\(\sigma_Y^2 = b^2 \sigma_W^2 + \sigma_V^2\)</span>, and<br />
the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> will be <span class="math inline">\(\sigma_{XY} = a\,b\,\sigma_W^2\)</span>.</p>
<p>In short, the covariance arises from some shared variance.</p>
</div>
</div>
<div id="estimating-covariance." class="section level2">
<h2>Estimating covariance.</h2>
<p>We estimate the variance from a sample by summing up the squared deviations to yield a “sum of squares,” which we divide by n-1 to obtain an estimator for the variance (<span class="math inline">\(s_X^2\)</span>):</p>
<p><span class="math inline">\(s_X^2 = \frac{\sum\limits_{i=1}^n (x_i - \bar x)(x_i - \bar x)}{n-1}\)</span></p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="bivariate.html#cb358-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(df.cm)</span>
<span id="cb358-2"><a href="bivariate.html#cb358-2" aria-hidden="true" tabindex="-1"></a>(<span class="at">SS.h =</span> <span class="fu">sum</span>((df.cm<span class="sc">$</span>height <span class="sc">-</span> <span class="fu">mean</span>(df.cm<span class="sc">$</span>height))<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 78177.3</code></pre>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="bivariate.html#cb360-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(SS.h<span class="sc">/</span>(n<span class="dv">-1</span>), <span class="fu">var</span>(df.cm<span class="sc">$</span>height))</span></code></pre></div>
<pre><code>## [1] 156.6679 156.6679</code></pre>
<p>Similarly, we calculate the sample covariance by calculating the “sum of products” of the deviations of X and Y from their respective means, then dividing by n-1.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="bivariate.html#cb362-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(df.cm)</span>
<span id="cb362-2"><a href="bivariate.html#cb362-2" aria-hidden="true" tabindex="-1"></a>(<span class="at">SP.hf =</span> <span class="fu">sum</span>((df.cm<span class="sc">$</span>height <span class="sc">-</span> <span class="fu">mean</span>(df.cm<span class="sc">$</span>height))<span class="sc">*</span></span>
<span id="cb362-3"><a href="bivariate.html#cb362-3" aria-hidden="true" tabindex="-1"></a>             (df.cm<span class="sc">$</span>foot <span class="sc">-</span> <span class="fu">mean</span>(df.cm<span class="sc">$</span>foot))))</span></code></pre></div>
<pre><code>## [1] 9571.319</code></pre>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="bivariate.html#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(SP.hf<span class="sc">/</span>(n<span class="dv">-1</span>), <span class="fu">cov</span>(df.cm<span class="sc">$</span>height, df.cm<span class="sc">$</span>foot))</span></code></pre></div>
<pre><code>## [1] 19.181 19.181</code></pre>
<p>You might wonder: why are we dividing by n-1, when we estimate two parameters (mean of x and mean of y). The answer comes from thinking about each observation as a two element vector (x,y), we have n of these vectors, and we estimate the mean the mean of those vectors – a two element mean.</p>
<p>As discussed earlier, the covariance scales with the variance of X and Y. Generally, though, we want a measure of association that is scaled to the spread of the two variables, scaling the covariance yields the <a href="correlation.html">correlation</a>.</p>

<p><img src="_main_files/figure-html/unnamed-chunk-180-1.png" width="1800" /></p>
</div>
<div id="bivariate-correlation" class="section level2">
<h2>Correlation</h2>
<p>The correlation is a scaled/normalized <a href="covariance.html">covariance</a>, which we calculate by dividing the covariance of x and y by their standard deviations. The population correlation (usually referred to as the greek letter “rho” <span class="math inline">\(\rho_{XY}\)</span>) can be calculated from the population covariance (<span class="math inline">\(\sigma_{XY}\)</span>), and population standard deviations of X and Y (<span class="math inline">\(\sigma_X\)</span>, <span class="math inline">\(\sigma_Y\)</span>):</p>
<p><span class="math inline">\(\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X\,\sigma_Y}\)</span></p>
<p>And we can calculate the sample correlation (<span class="math inline">\(r_{XY}\)</span>) – which we use as the estimator of population correlation – the same way, using the sample covariance and sample standard deviations:</p>
<p><span class="math inline">\(\hat \rho_{XY} = r_{XY} = \frac{s_{XY}}{s_X\,s_Y}\)</span></p>
<p>In <code>R</code>, we just use the <code>cor</code> function to calculate the sample correlation.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="bivariate.html#cb366-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">r.hf =</span> <span class="fu">cor</span>(df.cm<span class="sc">$</span>height, df.cm<span class="sc">$</span>foot))</span></code></pre></div>
<pre><code>## [1] 0.7137651</code></pre>
<p>The correlation is a scale-free measure of covariance – since we calculate it by dividing by the standard deviations of X and Y, it has lost any units X and Y had. This has some benefits and costs. The benefits are: The correlation will not care what units X was measured in (centimeters? inches? parsecs?), so we get a measure of the strength and direction of the linear relationship that abstracts away from these units. The cost is that we got rid of the physical units and thus our measure is further removed from reality.</p>
<div id="correlation-as-the-slope-of-z-scores" class="section level3">
<h3>Correlation as the slope of z-scores</h3>
<p>One helpful way to think about the correlation is as the slope of the z-scores, or the slope in standard deviations. If the correlation between x and y is 0.8, that means that for an x that is 1 sd above the mean of x, we expect the y to be 0.8 sds above the mean of y. (more on this in <a href="bivariate-ols.html">OLS regression</a></p>
</div>
<div id="coefficient-of-determination" class="section level3">
<h3>Coefficient of determination</h3>
<p>If we square the correlation, we get “R-squared,” or the “coefficient of determination,” which roughly describes what proportion of the overall variance is shared (more on this when we get to regression).</p>

</div>
</div>
<div id="bivariate-ols" class="section level2">
<h2>Ordinary Least-Squares (OLS) Regression</h2>
<p>In OLS regression we have one numerical response variable (<span class="math inline">\(y\)</span>), and one numerical explanatory variable (<span class="math inline">\(x\)</span>), and we model the relationship between the two as a line plus some error in (<span class="math inline">\(y\)</span>):</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\]</span>, where<br />
<span class="math display">\[\varepsilon_i \sim \operatorname{Normal}(0, \sigma_\varepsilon)\]</span></p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="bivariate.html#cb368-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb368-2"><a href="bivariate.html#cb368-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="at">by=</span><span class="fl">0.1</span>)</span>
<span id="cb368-3"><a href="bivariate.html#cb368-3" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span></span>
<span id="cb368-4"><a href="bivariate.html#cb368-4" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span></span>
<span id="cb368-5"><a href="bivariate.html#cb368-5" aria-hidden="true" tabindex="-1"></a>y.p <span class="ot">=</span> b0 <span class="sc">+</span> b1<span class="sc">*</span>x</span>
<span id="cb368-6"><a href="bivariate.html#cb368-6" aria-hidden="true" tabindex="-1"></a>e <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x), <span class="dv">0</span>, <span class="fl">0.4</span>)</span>
<span id="cb368-7"><a href="bivariate.html#cb368-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb368-8"><a href="bivariate.html#cb368-8" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y.p, <span class="at">e =</span> e)</span>
<span id="cb368-9"><a href="bivariate.html#cb368-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb368-10"><a href="bivariate.html#cb368-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x,y))<span class="sc">+</span></span>
<span id="cb368-11"><a href="bivariate.html#cb368-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size=</span><span class="fl">1.5</span>, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb368-12"><a href="bivariate.html#cb368-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y=</span>y<span class="sc">+</span>e), <span class="at">size=</span><span class="dv">3</span>)<span class="sc">+</span></span>
<span id="cb368-13"><a href="bivariate.html#cb368-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">ymin=</span>y, <span class="at">ymax=</span>y<span class="sc">+</span>e), <span class="at">color=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-182-1.png" width="3000" /></p>
<p>Here we show the individual <span class="math inline">\(x,y\)</span> points as black dots, the underlying linear relationship in blue, and the error/residual (deviations in of <span class="math inline">\(y\)</span> from the exact line) in red. A few things are noteworthy about the relationship this model assumes:</p>
<ol style="list-style-type: decimal">
<li><p>The relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> is a line. We can always fit a line to some data, but the fact that we can do so does not mean that the relationship really is linear, or that a line fit to the relationship tells you anything meaningful at all. This is a strong structural assumption made in regression modeling, and you should always check (at least by looking at a scatterplot), that this is an adequate description of the data.</p></li>
<li><p>There is error only in <span class="math inline">\(y\)</span>; there is no error in the <span class="math inline">\(x\)</span> values. This assumption is violated often, without particularly adverse consequences; however, one should be thoughtful when assigning <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> variables. As a consequence, the regression line for <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(x\)</span> is different than the regression line for <span class="math inline">\(x\)</span> as a function of <span class="math inline">\(y\)</span> (for more, see the difference between <a href="ols-lines.html">y~x and x~y</a>). The correlation, on the other hand, is symmetric: <span class="math inline">\(r_{xy} = r_{yx}\)</span>.</p></li>
<li><p><span class="math inline">\(y\)</span> value errors for different points are independent and identically distributed. This means that errors should not be correlated (as usually happens in timeseries data, or otherwise structured data), and that the distribution of errors does not change with <span class="math inline">\(x\)</span> (errors are “homoscedastic”). Small violations of these assumptions often do not have much of an effect, but sometimes they do. We will talk about this at length later.</p></li>
<li><p>Errors are normally distributed. While this assumption is not necessary for calculating the least squares <span class="math inline">\(B\)</span> values, it is necessary for some of the null hypothesis tests we will be using to assess statistical significance of estimated model parameters.</p></li>
</ol>
<div id="regression-terminology" class="section level3">
<h3>Regression terminology</h3>
<p>Let’s start with some terminology:</p>
<p><span class="math inline">\(y_i = B_0 + B_1 x_i + \varepsilon_i\)</span></p>
<p><span class="math inline">\(B_0\)</span> is the <strong>y intercept</strong>: the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>. It shifts the line up and down along the <span class="math inline">\(y\)</span> axis.</p>
<p><span class="math inline">\(B_1\)</span> is the <strong>slope</strong>: how many units of <span class="math inline">\(y\)</span> you gain/lose per unit change in <span class="math inline">\(x\)</span>. <span class="math inline">\(B_1\)</span> is in the units <span class="math inline">\(y/x\)</span>. E.g., if we are predicting women’s height from the height of her mother – both in inches – and we find a slope of 0.8, that means that (all else equal) for every inch taller that a mother is, we expect the daughter to be 0.8 inches taller.</p>
<p>The <strong>identity line</strong> is <span class="math inline">\(y=x\)</span> (in other words: <span class="math inline">\(B_0=0\)</span>, <span class="math inline">\(B_1=1\)</span>).</p>
<p><span class="math inline">\(\varepsilon_i\)</span> is the <strong>residual</strong>, or <strong>error</strong>, of the <span class="math inline">\(i\)</span>th point: how far the real <span class="math inline">\(y\)</span> value differs from the <span class="math inline">\(y\)</span> value we predict using our linear function of <span class="math inline">\(x\)</span>. In all regression, the goal is to estimate <span class="math inline">\(B\)</span> so as to minimize the sum of the squares of these residuals – the sum of squared errors.</p>
</div>
<div id="estimating-the-regression-line." class="section level3">
<h3>Estimating the regression line.</h3>
<p>Let’s generate some fake data, and then fit a line to them.</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="bivariate.html#cb369-1" aria-hidden="true" tabindex="-1"></a>IQ <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb369-2"><a href="bivariate.html#cb369-2" aria-hidden="true" tabindex="-1"></a>GPA <span class="ot">=</span> <span class="fu">pmax</span>(<span class="dv">1</span>, <span class="fu">pmin</span>(<span class="fl">4.3</span>, <span class="fu">round</span>((IQ<span class="dv">-100</span>)<span class="sc">/</span><span class="dv">15</span><span class="sc">*</span><span class="fl">0.5+2.8</span><span class="sc">+</span><span class="fu">rnorm</span>(<span class="fu">length</span>(IQ),<span class="dv">0</span>,<span class="fl">0.7</span>), <span class="dv">1</span>)))</span>
<span id="cb369-3"><a href="bivariate.html#cb369-3" aria-hidden="true" tabindex="-1"></a>iq.gpa <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">iq=</span>IQ, <span class="at">gpa=</span>GPA)</span>
<span id="cb369-4"><a href="bivariate.html#cb369-4" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(iq,gpa))<span class="sc">+</span></span>
<span id="cb369-5"><a href="bivariate.html#cb369-5" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb369-6"><a href="bivariate.html#cb369-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-7"><a href="bivariate.html#cb369-7" aria-hidden="true" tabindex="-1"></a>( <span class="at">lm.fit =</span> <span class="fu">lm</span>(<span class="at">data =</span> iq.gpa, gpa<span class="sc">~</span>iq) )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gpa ~ iq, data = iq.gpa)
## 
## Coefficients:
## (Intercept)           iq  
##    -0.09590      0.02931</code></pre>
<p>This fit gives us an intercept (<span class="math inline">\(B_0\)</span>) and a slope (<span class="math inline">\(B_1\)</span>) for the line that minimizes the sum of squared errors.</p>
<div id="math-behind-estimating-the-regression-line" class="section level4">
<h4>Math behind estimating the regression line</h4>
<p>When estimating the regression line we are interested in finding the slope (<span class="math inline">\(B_1\)</span>) and intercept (<span class="math inline">\(B_0\)</span>) values that will make the predicted y values <span class="math inline">\(\hat y_i = B_0 + B_1 x_i\)</span> as close to actual <span class="math inline">\(y_i\)</span> values as possible. Formally, we want to find the <span class="math inline">\(B\)</span> values that minimize the sum of squared errors: <span class="math inline">\(\sum (y_i - \hat y_i)^2\)</span>.</p>
<p>It is useful to work through the algebra that allows us to obtain least squares estimates of the slope and intercept from the summary statistics of x and y and their correlation.</p>
<p>Mean of x and y:</p>
<ul>
<li><span class="math inline">\(\bar x = \sum\limits_{i=1}^n x_i\)</span>, and<br />
</li>
<li><span class="math inline">\(\bar y = \sum\limits_{i=1}^n y_i\)</span>.</li>
</ul>
<p>Standard deviations of x and y (by way of the sum of squares of x and y):</p>
<ul>
<li><span class="math inline">\(\operatorname{SS}[x] = \sum\limits_{i=1}^n (x_i - \bar x)^2\)</span><br />
</li>
<li><span class="math inline">\(s_x = \sqrt{\frac{1}{n-1} \operatorname{SS}[x]}\)</span><br />
</li>
<li><span class="math inline">\(\operatorname{SS}[y] = \sum\limits_{i=1}^n (y_i - \bar y)^2\)</span><br />
</li>
<li><span class="math inline">\(s_y = \sqrt{\frac{1}{n-1} \operatorname{SS}[y]}\)</span></li>
</ul>
<p>The correlation of x and y by way of their sum of products and their covariance:</p>
<ul>
<li><span class="math inline">\(\operatorname{SP}[x,y] = \sum\limits_{i=1}^n (x_i - \bar x)(y_i - \bar y)\)</span><br />
</li>
<li><span class="math inline">\(s_{xy} = \frac{1}{n-1} \operatorname{SP}[x,y]\)</span><br />
</li>
<li><span class="math inline">\(r_{xy} = \frac{s_{xy}}{s_x s_y}\)</span></li>
</ul>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="bivariate.html#cb371-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(iq.gpa)</span>
<span id="cb371-2"><a href="bivariate.html#cb371-2" aria-hidden="true" tabindex="-1"></a>m.x <span class="ot">=</span> <span class="fu">mean</span>(iq.gpa<span class="sc">$</span>iq)</span>
<span id="cb371-3"><a href="bivariate.html#cb371-3" aria-hidden="true" tabindex="-1"></a>m.y <span class="ot">=</span> <span class="fu">mean</span>(iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb371-4"><a href="bivariate.html#cb371-4" aria-hidden="true" tabindex="-1"></a>SS.x <span class="ot">=</span> <span class="fu">sum</span>((iq.gpa<span class="sc">$</span>iq<span class="sc">-</span>m.x)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb371-5"><a href="bivariate.html#cb371-5" aria-hidden="true" tabindex="-1"></a>s.x <span class="ot">=</span> <span class="fu">sd</span>(iq.gpa<span class="sc">$</span>iq)</span>
<span id="cb371-6"><a href="bivariate.html#cb371-6" aria-hidden="true" tabindex="-1"></a>SS.y <span class="ot">=</span> <span class="fu">sum</span>((iq.gpa<span class="sc">$</span>gpa<span class="sc">-</span>m.y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb371-7"><a href="bivariate.html#cb371-7" aria-hidden="true" tabindex="-1"></a>s.y <span class="ot">=</span> <span class="fu">sd</span>(iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb371-8"><a href="bivariate.html#cb371-8" aria-hidden="true" tabindex="-1"></a>SP.xy <span class="ot">=</span> <span class="fu">sum</span>((iq.gpa<span class="sc">$</span>iq<span class="sc">-</span>m.x)<span class="sc">*</span>(iq.gpa<span class="sc">$</span>gpa<span class="sc">-</span>m.y))</span>
<span id="cb371-9"><a href="bivariate.html#cb371-9" aria-hidden="true" tabindex="-1"></a>s.xy <span class="ot">=</span> <span class="fu">cov</span>(iq.gpa<span class="sc">$</span>iq, iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb371-10"><a href="bivariate.html#cb371-10" aria-hidden="true" tabindex="-1"></a>r.xy <span class="ot">=</span> <span class="fu">cor</span>(iq.gpa<span class="sc">$</span>iq, iq.gpa<span class="sc">$</span>gpa)</span></code></pre></div>
<p>The least squares estimate of the slope is obtained by rescaling the correlation (the slope of the z-scores), to the standard deviations of y and x:</p>
<p><span class="math inline">\(B_1 = r_{xy}\frac{s_y}{s_x}\)</span></p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="bivariate.html#cb372-1" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">=</span> r.xy<span class="sc">*</span>s.y<span class="sc">/</span>s.x</span></code></pre></div>
<p>The least squares estimate of the intercept is obtained by knowing that the least-squares regression line has to pass through the mean of x and y. Consequently, <span class="math inline">\(B_1 \bar x + B_0 = \bar y\)</span>, and we can solve for the intercept as:</p>
<p><span class="math inline">\(B_0 = \bar y - B_1 \bar x\)</span></p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="bivariate.html#cb373-1" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">=</span> m.y <span class="sc">-</span> b1<span class="sc">*</span>m.x</span></code></pre></div>
<p>With these estimates we can obtain the predicted y values for each observed x:</p>
<p><span class="math inline">\(\hat y_i = B_0 + B_1 x_i\)</span></p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="bivariate.html#cb374-1" aria-hidden="true" tabindex="-1"></a>iq.gpa<span class="sc">$</span>gpa.hat <span class="ot">=</span> iq.gpa<span class="sc">$</span>iq<span class="sc">*</span>b1 <span class="sc">+</span> b0</span></code></pre></div>
<p>And from these we can calculate the individual residuals, the deviation of each y value from the y value predicted by the regression line:</p>
<p><span class="math inline">\(e_i = y_i - \hat y_i\)</span></p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="bivariate.html#cb375-1" aria-hidden="true" tabindex="-1"></a>iq.gpa<span class="sc">$</span>residual <span class="ot">=</span> iq.gpa<span class="sc">$</span>gpa <span class="sc">-</span> iq.gpa<span class="sc">$</span>gpa.hat</span></code></pre></div>
<p>From these residuals we can calculate the sum of squared error – that is, the sum of squared residuals:</p>
<p><span class="math inline">\(\operatorname{SS}[e] = \sum\limits_{i=1}^n e_i^2\)</span></p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="bivariate.html#cb376-1" aria-hidden="true" tabindex="-1"></a>SS.error <span class="ot">=</span> <span class="fu">sum</span>(iq.gpa<span class="sc">$</span>residual<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>And thus, we can estimate the standard deviation of the residuals by dividing the sum of squared error by <span class="math inline">\(n-2\)</span> to get the variance, and then taking the square root. We use <span class="math inline">\(n-2\)</span> because those are the degrees of freedom that are left after we estimate two parameters (the slope and intercept):</p>
<p><span class="math inline">\(s_e = \sqrt{\frac{1}{n-2} \operatorname{SS}[e]}\)</span></p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="bivariate.html#cb377-1" aria-hidden="true" tabindex="-1"></a>s.e <span class="ot">=</span> <span class="fu">sqrt</span>(SS.error<span class="sc">/</span>(n<span class="dv">-2</span>))</span></code></pre></div>
</div>
</div>
<div id="standard-errors-of-regression-coefficients" class="section level3">
<h3>Standard errors of regression coefficients</h3>
<p>We can get the (marginal) standard errors of the slope and intercept using the <code>summary</code> function to get further details of the model fit:</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="bivariate.html#cb378-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gpa ~ iq, data = iq.gpa)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4882 -0.3912 -0.1421  0.4354  1.7348 
## 
## Coefficients:
##              Estimate Std. Error t value
## (Intercept) -0.095899   0.688319  -0.139
## iq           0.029315   0.006719   4.363
##             Pr(&gt;|t|)    
## (Intercept)     0.89    
## iq          6.78e-05 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6986 on 48 degrees of freedom
## Multiple R-squared:  0.2839, Adjusted R-squared:  0.269 
## F-statistic: 19.03 on 1 and 48 DF,  p-value: 6.783e-05</code></pre>
<div id="math-behind-regression-line-errors" class="section level4">
<h4>Math behind regression line errors</h4>
<p>It is worth looking at the equations used to calculate the marginal standard errors for the slope and intercept. Both standard errors increase with greater standard deviations of the residuals, and decrease with sample size; however, they also change in interesting ways with the standard deviation of <span class="math inline">\(x\)</span>, and the mean of <span class="math inline">\(x\)</span>.</p>
<p>The standard error of the slope is proportional to the standard deviation of the residuals, inversely proportional to the square root of sample size, and also inversely proportion to the standard deviation of <span class="math inline">\(x\)</span>. This last fact is perhaps most intriguing, but should make sense: the more spread-out the x values are, the greater the change in y due to changes in x (rather than error), thus the better our estimate of the slope.</p>
<p><span class="math inline">\(s\{B_1\} = s_e \frac{1}{s_x \sqrt{n-1}}\)</span></p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="bivariate.html#cb380-1" aria-hidden="true" tabindex="-1"></a>s.b1 <span class="ot">=</span> s.e<span class="sc">/</span>(s.x<span class="sc">*</span><span class="fu">sqrt</span>(n<span class="dv">-1</span>))</span></code></pre></div>
<p>The standard error of the intercept is more interesting. Remember that we calculate the intercept by relying on the fact that the least squares regression line must go through the mean of y and the mean of x. Consequently, we calculate the standard error of the intercept by summing the variance due to error in estimating the mean y value (which is inversely proportional to n), and the variance due to extrapolating the line with our uncertainty in the slope to x=0 (which is proportional to the squared standard error of the slope and the squared distance of the mean of x from 0).</p>
<p><span class="math inline">\(s\{B_0\} = \sqrt{\left({\frac{s_e}{\sqrt(n)}}\right)^2 + \left({\bar x s_e \frac{1}{s_x \sqrt{n-1}}}\right)^2 }\)</span></p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="bivariate.html#cb381-1" aria-hidden="true" tabindex="-1"></a>s.b0 <span class="ot">=</span> s.e<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n <span class="sc">+</span> m.x<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>s.x<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(n<span class="dv">-1</span>))</span></code></pre></div>
</div>
</div>
<div id="confidence-intervals-and-tests-for-regression-coefficients" class="section level3">
<h3>Confidence intervals and tests for regression coefficients</h3>
<p>The <code>summary</code> function by default returns the t-test statistic and p-values for comparing parameter values to 0, which we can extract via the <code>coef</code> function:</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="bivariate.html#cb382-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">s.lm.fit =</span> <span class="fu">summary</span>(lm.fit))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gpa ~ iq, data = iq.gpa)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4882 -0.3912 -0.1421  0.4354  1.7348 
## 
## Coefficients:
##              Estimate Std. Error t value
## (Intercept) -0.095899   0.688319  -0.139
## iq           0.029315   0.006719   4.363
##             Pr(&gt;|t|)    
## (Intercept)     0.89    
## iq          6.78e-05 ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6986 on 48 degrees of freedom
## Multiple R-squared:  0.2839, Adjusted R-squared:  0.269 
## F-statistic: 19.03 on 1 and 48 DF,  p-value: 6.783e-05</code></pre>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="bivariate.html#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(s.lm.fit) <span class="co"># returns the matrix of coefficients, errors, t-stats, and p-values</span></span></code></pre></div>
<pre><code>##                Estimate  Std. Error
## (Intercept) -0.09589893 0.688318743
## iq           0.02931482 0.006719284
##                t value     Pr(&gt;|t|)
## (Intercept) -0.1393234 8.897775e-01
## iq           4.3627893 6.783065e-05</code></pre>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="bivariate.html#cb386-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can index that matrix to get specific rows and columns out:</span></span>
<span id="cb386-2"><a href="bivariate.html#cb386-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(s.lm.fit)[<span class="fu">c</span>(<span class="st">&quot;(Intercept)&quot;</span>), <span class="fu">c</span>(<span class="st">&quot;t value&quot;</span>, <span class="st">&quot;Pr(&gt;|t|)&quot;</span>)]</span></code></pre></div>
<pre><code>##    t value   Pr(&gt;|t|) 
## -0.1393234  0.8897775</code></pre>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="bivariate.html#cb388-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(s.lm.fit)[<span class="fu">c</span>(<span class="st">&quot;iq&quot;</span>), <span class="fu">c</span>(<span class="st">&quot;t value&quot;</span>, <span class="st">&quot;Pr(&gt;|t|)&quot;</span>)]</span></code></pre></div>
<pre><code>##      t value     Pr(&gt;|t|) 
## 4.362789e+00 6.783065e-05</code></pre>
<p>Similarly, we can get confidence intervals on these coefficients using <code>confint</code>, which calculates the standard t-distribution confidence intervals using the standard errors.</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="bivariate.html#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm.fit, <span class="st">&quot;(Intercept)&quot;</span>, <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -1.479857 1.288059</code></pre>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="bivariate.html#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm.fit, <span class="st">&quot;iq&quot;</span>, <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##         2.5 %     97.5 %
## iq 0.01580479 0.04282484</code></pre>
<div id="calculating-t-tests-and-intervals" class="section level4">
<h4>Calculating t-tests and intervals</h4>
<p>Just as in the case of all of our t-tests for various mean comparisons, we are going to use the t-distribution to obtain p-values and get confidence intervals.</p>
<p>T-test for slope being non-zero:</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="bivariate.html#cb394-1" aria-hidden="true" tabindex="-1"></a>t.b1 <span class="ot">=</span> (b1<span class="dv">-0</span>)<span class="sc">/</span>s.b1</span>
<span id="cb394-2"><a href="bivariate.html#cb394-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> n<span class="dv">-2</span>        </span>
<span id="cb394-3"><a href="bivariate.html#cb394-3" aria-hidden="true" tabindex="-1"></a><span class="co"># n-2 because we lose two degrees of freedom for the slope and intercept when calculating the sample standard deviation of the residuals</span></span>
<span id="cb394-4"><a href="bivariate.html#cb394-4" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="sc">-</span><span class="fu">abs</span>(t.b1), df) <span class="co">#two-tail p value</span></span></code></pre></div>
<pre><code>## [1] 6.783065e-05</code></pre>
<p>Confidence interval on the slope:</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="bivariate.html#cb396-1" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="fl">0.95</span></span>
<span id="cb396-2"><a href="bivariate.html#cb396-2" aria-hidden="true" tabindex="-1"></a>t.crit <span class="ot">=</span> <span class="fu">qt</span>((<span class="dv">1</span><span class="sc">-</span>q)<span class="sc">/</span><span class="dv">2</span>,df)</span>
<span id="cb396-3"><a href="bivariate.html#cb396-3" aria-hidden="true" tabindex="-1"></a>b1 <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span>t.crit<span class="sc">*</span>s.b1</span></code></pre></div>
<pre><code>## [1] 0.01580479 0.04282484</code></pre>
<p>T-test for intercept being different from some null (0 by default). Note that it is quite rare to test for some null intercept value; we do something analogous in ANCOVA, but for a simple regression this is rarely a useful question to ask.</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="bivariate.html#cb398-1" aria-hidden="true" tabindex="-1"></a>t.b0 <span class="ot">=</span> (b0<span class="dv">-0</span>)<span class="sc">/</span>s.b0</span>
<span id="cb398-2"><a href="bivariate.html#cb398-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> n<span class="dv">-2</span>        </span>
<span id="cb398-3"><a href="bivariate.html#cb398-3" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="sc">-</span><span class="fu">abs</span>(t.b0), df) <span class="co">#two-tail p value</span></span></code></pre></div>
<pre><code>## [1] 0.8897775</code></pre>
<p>Confidence interval on the intercept:</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="bivariate.html#cb400-1" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="fl">0.95</span></span>
<span id="cb400-2"><a href="bivariate.html#cb400-2" aria-hidden="true" tabindex="-1"></a>t.crit <span class="ot">=</span> <span class="fu">qt</span>((<span class="dv">1</span><span class="sc">-</span>q)<span class="sc">/</span><span class="dv">2</span>,df)</span>
<span id="cb400-3"><a href="bivariate.html#cb400-3" aria-hidden="true" tabindex="-1"></a>b0 <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span>t.crit<span class="sc">*</span>s.b0</span></code></pre></div>
<pre><code>## [1] -1.479857  1.288059</code></pre>

</div>
</div>
</div>
<div id="bivariate-lines" class="section level2">
<h2>y~x vs x~y vs principle component line</h2>
<p>Let’s consider the fake IQ-GPA from our overview of <a href="ols.html">OLS regression</a> again.</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="bivariate.html#cb402-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb402-2"><a href="bivariate.html#cb402-2" aria-hidden="true" tabindex="-1"></a>IQ <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb402-3"><a href="bivariate.html#cb402-3" aria-hidden="true" tabindex="-1"></a>GPA <span class="ot">=</span> <span class="fu">pmax</span>(<span class="dv">1</span>, <span class="fu">pmin</span>(<span class="fl">4.3</span>, <span class="fu">round</span>((IQ<span class="dv">-100</span>)<span class="sc">/</span><span class="dv">15</span><span class="sc">*</span><span class="fl">0.5+2.8</span><span class="sc">+</span><span class="fu">rnorm</span>(<span class="fu">length</span>(IQ),<span class="dv">0</span>,<span class="fl">0.7</span>), <span class="dv">1</span>)))</span>
<span id="cb402-4"><a href="bivariate.html#cb402-4" aria-hidden="true" tabindex="-1"></a>iq.gpa <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">iq=</span>IQ, <span class="at">gpa=</span>GPA)</span>
<span id="cb402-5"><a href="bivariate.html#cb402-5" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(iq,gpa))<span class="sc">+</span></span>
<span id="cb402-6"><a href="bivariate.html#cb402-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb402-7"><a href="bivariate.html#cb402-7" aria-hidden="true" tabindex="-1"></a>g1</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-200-1.png" width="3000" /></p>
<p>We can do two sorts of regression here: GPA as a function of IQ, or IQ as a function of GPA:</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="bivariate.html#cb403-1" aria-hidden="true" tabindex="-1"></a>( <span class="at">lm.g.i =</span> <span class="fu">lm</span>(<span class="at">data=</span>iq.gpa, gpa<span class="sc">~</span>iq) )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gpa ~ iq, data = iq.gpa)
## 
## Coefficients:
## (Intercept)           iq  
##    -0.36541      0.03183</code></pre>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="bivariate.html#cb405-1" aria-hidden="true" tabindex="-1"></a>( <span class="at">lm.i.g =</span> <span class="fu">lm</span>(<span class="at">data=</span>iq.gpa, iq<span class="sc">~</span>gpa) )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = iq ~ gpa, data = iq.gpa)
## 
## Coefficients:
## (Intercept)          gpa  
##       62.21        13.63</code></pre>
<p>Note that the slopes are different, and critically, they are not inverses of each other. You might imagine that the IQ/GPA slope will be the inverse of the GPA/IQ slope, but they are not:</p>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="bivariate.html#cb407-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">coef</span>(lm.g.i)[<span class="st">&#39;iq&#39;</span>], <span class="fu">coef</span>(lm.i.g)[<span class="st">&#39;gpa&#39;</span>])</span></code></pre></div>
<pre><code>##       iq      gpa 
## 31.41480 13.62728</code></pre>
<p>Why are they different?</p>
<p><span class="math inline">\(B\{\frac{y}{x}\} = r_{xy}\frac{s_y}{s_x}\)</span></p>
<p><span class="math inline">\(B\{\frac{x}{y}\} = r_{xy}\frac{s_x}{s_y}\)</span></p>
<p>Consequently, the slopes will be inverses of each other only if the correlation is 1. But why is this so?</p>
<p>Remember, the y~x regression line is the line that minimizes squared error <strong>in y</strong>, and considers there to be no error in x. Similarly, the x~y regression line minimizes squared error <strong>in x</strong> and considers there to be no error in y. This discrepancy yields potentially very different slopes.</p>
<p>Let’s look at the two lines, along with the “principle component” line that minimizes squared error <em>orthogonal</em> to the line (which is the intuitive line of best fit for most people).</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="bivariate.html#cb409-1" aria-hidden="true" tabindex="-1"></a>m.y <span class="ot">=</span> <span class="fu">mean</span>(iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb409-2"><a href="bivariate.html#cb409-2" aria-hidden="true" tabindex="-1"></a>m.x <span class="ot">=</span> <span class="fu">mean</span>(iq.gpa<span class="sc">$</span>iq)</span>
<span id="cb409-3"><a href="bivariate.html#cb409-3" aria-hidden="true" tabindex="-1"></a>s.y <span class="ot">=</span> <span class="fu">sd</span>(iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb409-4"><a href="bivariate.html#cb409-4" aria-hidden="true" tabindex="-1"></a>s.x <span class="ot">=</span> <span class="fu">sd</span>(iq.gpa<span class="sc">$</span>iq)</span>
<span id="cb409-5"><a href="bivariate.html#cb409-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-6"><a href="bivariate.html#cb409-6" aria-hidden="true" tabindex="-1"></a>b1.y.x <span class="ot">=</span> <span class="fu">coef</span>(lm.g.i)[<span class="st">&#39;iq&#39;</span>]</span>
<span id="cb409-7"><a href="bivariate.html#cb409-7" aria-hidden="true" tabindex="-1"></a>b0.y.x <span class="ot">=</span> <span class="fu">coef</span>(lm.g.i)[<span class="st">&#39;(Intercept)&#39;</span>]</span>
<span id="cb409-8"><a href="bivariate.html#cb409-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-9"><a href="bivariate.html#cb409-9" aria-hidden="true" tabindex="-1"></a>b1.x.y <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">coef</span>(lm.i.g)[<span class="st">&#39;gpa&#39;</span>]</span>
<span id="cb409-10"><a href="bivariate.html#cb409-10" aria-hidden="true" tabindex="-1"></a>b0.x.y <span class="ot">=</span> m.y <span class="sc">-</span> b1.x.y<span class="sc">*</span>m.x</span>
<span id="cb409-11"><a href="bivariate.html#cb409-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-12"><a href="bivariate.html#cb409-12" aria-hidden="true" tabindex="-1"></a>pc.load <span class="ot">=</span> <span class="fu">prcomp</span>(iq.gpa, <span class="at">scale=</span>T, <span class="at">retx=</span>T)</span>
<span id="cb409-13"><a href="bivariate.html#cb409-13" aria-hidden="true" tabindex="-1"></a>b1.yx <span class="ot">=</span> pc.load<span class="sc">$</span>rotation[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">/</span>pc.load<span class="sc">$</span>rotation[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">*</span>s.y<span class="sc">/</span>s.x</span>
<span id="cb409-14"><a href="bivariate.html#cb409-14" aria-hidden="true" tabindex="-1"></a>b0.yx <span class="ot">=</span> m.y <span class="sc">-</span> b1.yx<span class="sc">*</span>m.x</span>
<span id="cb409-15"><a href="bivariate.html#cb409-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-16"><a href="bivariate.html#cb409-16" aria-hidden="true" tabindex="-1"></a>iq.gpa<span class="sc">$</span>yh.y.x <span class="ot">=</span> b1.y.x<span class="sc">*</span>iq.gpa<span class="sc">$</span>iq <span class="sc">+</span> b0.y.x</span>
<span id="cb409-17"><a href="bivariate.html#cb409-17" aria-hidden="true" tabindex="-1"></a>iq.gpa<span class="sc">$</span>xh.x.y <span class="ot">=</span> <span class="fu">coef</span>(lm.i.g)[<span class="st">&#39;gpa&#39;</span>]<span class="sc">*</span>iq.gpa<span class="sc">$</span>gpa <span class="sc">+</span> <span class="fu">coef</span>(lm.i.g)[<span class="st">&#39;(Intercept)&#39;</span>]</span>
<span id="cb409-18"><a href="bivariate.html#cb409-18" aria-hidden="true" tabindex="-1"></a>iq.gpa<span class="sc">$</span>xh.yx  <span class="ot">=</span> pc.load<span class="sc">$</span>x[,<span class="dv">1</span>]<span class="sc">*</span>pc.load<span class="sc">$</span>rotation[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">*</span>s.x<span class="sc">+</span>m.x</span>
<span id="cb409-19"><a href="bivariate.html#cb409-19" aria-hidden="true" tabindex="-1"></a>iq.gpa<span class="sc">$</span>yh.yx  <span class="ot">=</span> pc.load<span class="sc">$</span>x[,<span class="dv">1</span>]<span class="sc">*</span>pc.load<span class="sc">$</span>rotation[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">*</span>s.y<span class="sc">+</span>m.y</span>
<span id="cb409-20"><a href="bivariate.html#cb409-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-21"><a href="bivariate.html#cb409-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-22"><a href="bivariate.html#cb409-22" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa))<span class="sc">+</span></span>
<span id="cb409-23"><a href="bivariate.html#cb409-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> b0.y.x, <span class="at">slope=</span>b1.y.x, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)<span class="sc">+</span></span>
<span id="cb409-24"><a href="bivariate.html#cb409-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> b0.x.y, <span class="at">slope=</span>b1.x.y, <span class="at">color=</span><span class="st">&quot;red&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)<span class="sc">+</span></span>
<span id="cb409-25"><a href="bivariate.html#cb409-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> b0.yx, <span class="at">slope=</span>b1.yx, <span class="at">color=</span><span class="st">&quot;gray&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)<span class="sc">+</span></span>
<span id="cb409-26"><a href="bivariate.html#cb409-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="fl">2.5</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-203-1.png" width="1800" /></p>
<p>Here the blue line shows the gpa~iq fit, the red line shows iq~gpa, and the gray line shows the principle component line (which is the intuitive line most people would draw to describe the best fit).</p>
<p>Why are they different? They are minimizing different squared errors. The gpa~iq line minimizes error in gpa, the iq~gpa line minimizes error in iq, and the principle component line minimizes error orthogonal to the line (counting both iq and gpa deviations as error).</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="bivariate.html#cb410-1" aria-hidden="true" tabindex="-1"></a>g2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa))<span class="sc">+</span></span>
<span id="cb410-2"><a href="bivariate.html#cb410-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> b0.y.x, <span class="at">slope=</span>b1.y.x, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)<span class="sc">+</span></span>
<span id="cb410-3"><a href="bivariate.html#cb410-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa, <span class="at">xend=</span>iq, <span class="at">yend=</span>yh.y.x), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb410-4"><a href="bivariate.html#cb410-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="fl">2.5</span>)</span>
<span id="cb410-5"><a href="bivariate.html#cb410-5" aria-hidden="true" tabindex="-1"></a>g3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa))<span class="sc">+</span></span>
<span id="cb410-6"><a href="bivariate.html#cb410-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> b0.x.y, <span class="at">slope=</span>b1.x.y, <span class="at">color=</span><span class="st">&quot;red&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)<span class="sc">+</span></span>
<span id="cb410-7"><a href="bivariate.html#cb410-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa, <span class="at">yend=</span>gpa, <span class="at">xend=</span>xh.x.y), <span class="at">color=</span><span class="st">&quot;red&quot;</span>)<span class="sc">+</span></span>
<span id="cb410-8"><a href="bivariate.html#cb410-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="fl">2.5</span>)</span>
<span id="cb410-9"><a href="bivariate.html#cb410-9" aria-hidden="true" tabindex="-1"></a>g4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa))<span class="sc">+</span></span>
<span id="cb410-10"><a href="bivariate.html#cb410-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> b0.yx, <span class="at">slope=</span>b1.yx, <span class="at">color=</span><span class="st">&quot;gray&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)<span class="sc">+</span></span>
<span id="cb410-11"><a href="bivariate.html#cb410-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa, <span class="at">yend=</span>yh.yx, <span class="at">xend=</span>xh.yx), <span class="at">color=</span><span class="st">&quot;gray&quot;</span>)<span class="sc">+</span></span>
<span id="cb410-12"><a href="bivariate.html#cb410-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="fl">2.5</span>)</span>
<span id="cb410-13"><a href="bivariate.html#cb410-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb410-14"><a href="bivariate.html#cb410-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb410-15"><a href="bivariate.html#cb410-15" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(g2,g3,g4,<span class="at">ncol=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-204-1.png" width="3000" /></p>

</div>
<div id="bivariate-determination" class="section level2">
<h2>Partitioning variance and the coefficient of determination.</h2>
<p>Let’s start with the fake IQ-GPA data from our discussion of <a href="bivariate.html#bivariate-ols">ordinary least squared regression</a>.</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="bivariate.html#cb411-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb411-2"><a href="bivariate.html#cb411-2" aria-hidden="true" tabindex="-1"></a>IQ <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb411-3"><a href="bivariate.html#cb411-3" aria-hidden="true" tabindex="-1"></a>GPA <span class="ot">=</span> <span class="fu">pmax</span>(<span class="dv">1</span>, <span class="fu">pmin</span>(<span class="fl">4.3</span>, <span class="fu">round</span>((IQ<span class="dv">-100</span>)<span class="sc">/</span><span class="dv">15</span><span class="sc">*</span><span class="fl">0.5+2.8</span><span class="sc">+</span><span class="fu">rnorm</span>(<span class="fu">length</span>(IQ),<span class="dv">0</span>,<span class="fl">0.7</span>), <span class="dv">1</span>)))</span>
<span id="cb411-4"><a href="bivariate.html#cb411-4" aria-hidden="true" tabindex="-1"></a>iq.gpa <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">iq=</span>IQ, <span class="at">gpa=</span>GPA)</span>
<span id="cb411-5"><a href="bivariate.html#cb411-5" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(iq,gpa))<span class="sc">+</span></span>
<span id="cb411-6"><a href="bivariate.html#cb411-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb411-7"><a href="bivariate.html#cb411-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb411-8"><a href="bivariate.html#cb411-8" aria-hidden="true" tabindex="-1"></a>( <span class="at">lm.fit =</span> <span class="fu">lm</span>(<span class="at">data =</span> iq.gpa, gpa<span class="sc">~</span>iq) )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gpa ~ iq, data = iq.gpa)
## 
## Coefficients:
## (Intercept)           iq  
##      0.5128       0.0219</code></pre>
<p>Linear regression brings to the forefront a running theme in classical statistics: partitioning the overall variance of measurements into the variance that may be attributed to different sources. This kind of partitioning is most commonly associated with an “analysis of variance,” which is a particular way of analyzing the results of a linear regression.</p>
<p>In R, we can get the ANOVA results for a given linear model fit via the <code>anova</code> command:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="bivariate.html#cb413-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.fit)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: gpa
##           Df  Sum Sq Mean Sq F value
## iq         1  4.3624  4.3624  10.118
## Residuals 48 20.6954  0.4312        
##             Pr(&gt;F)   
## iq        0.002574 **
## Residuals            
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1</code></pre>
<p>Tha ANOVA table shows the degrees of freedom, sums of squares, and other values for different sources of variance in y. The two sources of variance in play in our simple regression are (1) variance in GPA that can be attributed to variance in IQ, and its impact on GPA, and (2) variance in GPA that we cannot account for with our predictors (here just IQ) – the left over variance of the residuals.</p>
<p>These sums of squares are not <em>quite</em> actual variance estimates, they are variance estimates unnormalized by the number of data points that went into those estimates (the sums of squares, rather than the sums of squares divided by the number of things that go into that sum). However, the number of elements that goes into each of these sums is the same, so we can compare them. For instance we can divide the sum of squares attributed to IQ, by the sum of IQ and residual sums of squares, to calculate the proportion of the variance in GPA that can be explained by IQ:</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="bivariate.html#cb415-1" aria-hidden="true" tabindex="-1"></a>ss.iq <span class="ot">=</span> <span class="fu">anova</span>(lm.fit)[<span class="st">&#39;iq&#39;</span>,<span class="st">&#39;Sum Sq&#39;</span>]</span>
<span id="cb415-2"><a href="bivariate.html#cb415-2" aria-hidden="true" tabindex="-1"></a>ss.error <span class="ot">=</span> <span class="fu">anova</span>(lm.fit)[<span class="st">&#39;Residuals&#39;</span>,<span class="st">&#39;Sum Sq&#39;</span>]</span>
<span id="cb415-3"><a href="bivariate.html#cb415-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb415-4"><a href="bivariate.html#cb415-4" aria-hidden="true" tabindex="-1"></a>(<span class="at">r.sq =</span> ss.iq<span class="sc">/</span>(ss.iq<span class="sc">+</span>ss.error))</span></code></pre></div>
<pre><code>## [1] 0.1740935</code></pre>
<p>This “proportion of variance explained,” or “coefficient of determination” can also be calculated by squaring the sample correlation (for this simple case of one response and one explanatory variable):</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="bivariate.html#cb417-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(iq.gpa<span class="sc">$</span>iq, iq.gpa<span class="sc">$</span>gpa)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.1740935</code></pre>
<p>So what are these sums of squares?</p>
<div id="calculating-sums-of-squares." class="section level3">
<h3>Calculating sums of squares.</h3>
<p>The basic partitioning of sums of squares follows the logic that the “total sum of squares” is equal to the sum of all the sums of squares of different candidate sources. In our case, the “total” sum of squares is the total variation of <span class="math inline">\(y\)</span> (GPA) around its mean:</p>
<p><span class="math inline">\(\text{SST} = \operatorname{SS}[y] = \sum\limits_{i=1}^n (y_i - \bar y)^2\)</span></p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="bivariate.html#cb419-1" aria-hidden="true" tabindex="-1"></a>m.y <span class="ot">=</span> <span class="fu">mean</span>(iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb419-2"><a href="bivariate.html#cb419-2" aria-hidden="true" tabindex="-1"></a>iq.gpa<span class="sc">$</span>gpa.hat <span class="ot">=</span> <span class="fu">predict</span>(lm.fit, <span class="at">newdata =</span> iq.gpa)</span>
<span id="cb419-3"><a href="bivariate.html#cb419-3" aria-hidden="true" tabindex="-1"></a>( <span class="at">SS.y =</span> <span class="fu">sum</span>((iq.gpa<span class="sc">$</span>gpa<span class="sc">-</span>m.y)<span class="sc">^</span><span class="dv">2</span>) )</span></code></pre></div>
<pre><code>## [1] 25.0578</code></pre>
<p>The sum of squares of the “regression,” that is – the sum of squares that can be explained by the linear model we fit, can be calculated as the sum of the squared deviations of the predicted y values, from the mean of y:</p>
<p><span class="math inline">\(\text{SSR} = \operatorname{SS}[\hat y] = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2\)</span></p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="bivariate.html#cb421-1" aria-hidden="true" tabindex="-1"></a>( <span class="at">SS.yhat =</span> <span class="fu">sum</span>((iq.gpa<span class="sc">$</span>gpa.hat <span class="sc">-</span> m.y)<span class="sc">^</span><span class="dv">2</span>) )</span></code></pre></div>
<pre><code>## [1] 4.362399</code></pre>
<p>The sum of squares of the residuals (or the error), is the sum of squared deviations of the actual y values from those predicted by the linear regression.</p>
<p><span class="math inline">\(\text{SSE} = \operatorname{SS}[e] = \sum\limits_{i=1}^n (y_i - \hat y_i)^2\)</span></p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="bivariate.html#cb423-1" aria-hidden="true" tabindex="-1"></a>( <span class="at">SS.error =</span> <span class="fu">sum</span>((iq.gpa<span class="sc">$</span>gpa <span class="sc">-</span> iq.gpa<span class="sc">$</span>gpa.hat)<span class="sc">^</span><span class="dv">2</span>) )</span></code></pre></div>
<pre><code>## [1] 20.6954</code></pre>
<p>We can generate a plot of these. The black line segments indicate the deviation of y values from the mean y value (black horizontal line); the blue line segments indicates the deviation of the predicted y value (blue slope) from the mean y value; and the red segments indicate the error – the deviation of the actual y value from the predicted y value.</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="bivariate.html#cb425-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa))<span class="sc">+</span></span>
<span id="cb425-2"><a href="bivariate.html#cb425-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">5</span>)<span class="sc">+</span></span>
<span id="cb425-3"><a href="bivariate.html#cb425-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> m.y)<span class="sc">+</span></span>
<span id="cb425-4"><a href="bivariate.html#cb425-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span><span class="fu">coef</span>(lm.fit)[<span class="st">&quot;(Intercept)&quot;</span>], <span class="at">slope=</span><span class="fu">coef</span>(lm.fit)[<span class="st">&quot;iq&quot;</span>], <span class="at">color=</span><span class="st">&quot;blue&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)<span class="sc">+</span></span>
<span id="cb425-5"><a href="bivariate.html#cb425-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="at">ymin =</span> m.y, <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">ymax=</span>gpa), <span class="at">color=</span><span class="st">&quot;black&quot;</span>)<span class="sc">+</span></span>
<span id="cb425-6"><a href="bivariate.html#cb425-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="at">ymin=</span>m.y, <span class="fu">aes</span>(<span class="at">x=</span>iq<span class="fl">-0.35</span>, <span class="at">ymax=</span>gpa.hat), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb425-7"><a href="bivariate.html#cb425-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">x=</span>iq<span class="fl">+0.35</span>, <span class="at">ymin=</span>gpa, <span class="at">ymax=</span>gpa.hat), <span class="at">color=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-212-1.png" width="3000" /></p>
<p>The partitioning of the deviation of a given y value from the mean into the deviation of the corresponding regression prediction from the mean, and the deviation of the y value from the regression prediction maps onto the partition of sums of squares:<br />
SST = SSR + SSE</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="bivariate.html#cb426-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(SS.y, SS.yhat<span class="sc">+</span>SS.error)</span></code></pre></div>
<pre><code>## [1] 25.0578 25.0578</code></pre>

</div>
</div>
<div id="bivariate-significance" class="section level2">
<h2>Significance of linear relationship.</h2>
<p>Here we will work with the fake IQ-GPA data we generated when talking about <a href="bivariate-ols.html">Ordinary Least-Squares (OLS) Regression</a>.</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="bivariate.html#cb428-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb428-2"><a href="bivariate.html#cb428-2" aria-hidden="true" tabindex="-1"></a>IQ <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb428-3"><a href="bivariate.html#cb428-3" aria-hidden="true" tabindex="-1"></a>GPA <span class="ot">=</span> <span class="fu">pmax</span>(<span class="dv">1</span>, <span class="fu">pmin</span>(<span class="fl">4.3</span>, <span class="fu">round</span>((IQ<span class="dv">-100</span>)<span class="sc">/</span><span class="dv">15</span><span class="sc">*</span><span class="fl">0.5+2.8</span><span class="sc">+</span><span class="fu">rnorm</span>(<span class="fu">length</span>(IQ),<span class="dv">0</span>,<span class="fl">0.7</span>), <span class="dv">1</span>)))</span>
<span id="cb428-4"><a href="bivariate.html#cb428-4" aria-hidden="true" tabindex="-1"></a>iq.gpa <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">iq=</span>IQ, <span class="at">gpa=</span>GPA)</span>
<span id="cb428-5"><a href="bivariate.html#cb428-5" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(iq,gpa))<span class="sc">+</span></span>
<span id="cb428-6"><a href="bivariate.html#cb428-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb428-7"><a href="bivariate.html#cb428-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(g1)</span>
<span id="cb428-8"><a href="bivariate.html#cb428-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb428-9"><a href="bivariate.html#cb428-9" aria-hidden="true" tabindex="-1"></a>( <span class="at">lm.fit =</span> <span class="fu">lm</span>(<span class="at">data =</span> iq.gpa, gpa<span class="sc">~</span>iq) )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gpa ~ iq, data = iq.gpa)
## 
## Coefficients:
## (Intercept)           iq  
##    -1.37996      0.04014</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-214-1.png" width="3000" /></p>
<p>There are many <strong>equivalent</strong> ways to ascertain whether a single-variable ordinary least-squares regression is “significant.”</p>
<div id="significance-of-slope." class="section level3">
<h3>Significance of slope.</h3>
<p>We already saw that we can test whether the slope is significantly different from 0 by calculating a t statistic by using the estimated slope and its standard error:</p>
<p><span class="math inline">\(t_{n-2} = \frac{B_1}{s\{B_1\}}\)</span>.</p>
<p>This is the statistic R calculates when estimating the significance of the coefficient from the <a href="ols.html">linear model</a> summary:</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="bivariate.html#cb430-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(lm.fit))[<span class="st">&#39;iq&#39;</span>,]</span></code></pre></div>
<pre><code>##     Estimate   Std. Error      t value 
## 4.014378e-02 7.567184e-03 5.304983e+00 
##     Pr(&gt;|t|) 
## 2.832145e-06</code></pre>
</div>
<div id="significance-of-pairwise-correlation" class="section level3">
<h3>Significance of pairwise correlation</h3>
<p>Similarly, we can obtain this same t statistic via the pairwise <a href="correlation.html">correlation</a> via:</p>
<p><span class="math inline">\(t_{n-2} = r_{xy}\sqrt{\frac{n-2}{1-r_{xy}^2}}\)</span></p>
<p>We can obtain this test of the significance of a pairwise correlation via the <code>cor.test</code> function:</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="bivariate.html#cb432-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor.test</span>(iq.gpa<span class="sc">$</span>iq, iq.gpa<span class="sc">$</span>gpa)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment
##  correlation
## 
## data:  iq.gpa$iq and iq.gpa$gpa
## t = 5.305, df = 48, p-value =
## 2.832e-06
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.3967420 0.7580253
## sample estimates:
##       cor 
## 0.6079522</code></pre>
</div>
<div id="significance-of-variance-partition." class="section level3">
<h3>Significance of variance partition.</h3>
<p>Finally, we can calculate an F statistic based on the <a href="determination.html">partition of the variance</a> attributable to the regression as:</p>
<p><span class="math inline">\(F_{(1,n-2)} = \frac{\operatorname{SS}[\hat y]}{\operatorname{SS}[error]/(n-2)}\)</span></p>
<p>This is the statistic we get from the <code>anova</code> command:</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="bivariate.html#cb434-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(lm.fit)[<span class="st">&#39;iq&#39;</span>,]</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: gpa
##    Df Sum Sq Mean Sq F value    Pr(&gt;F)
## iq  1 15.323  15.323  28.143 2.832e-06
##       
## iq ***
## ---
## Signif. codes:  
##   0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39;
##   0.1 &#39; &#39; 1</code></pre>
<p>Which we can calculate manually as:</p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="bivariate.html#cb436-1" aria-hidden="true" tabindex="-1"></a>SS.yhat <span class="ot">=</span> <span class="fu">anova</span>(lm.fit)[<span class="st">&#39;iq&#39;</span>, <span class="st">&#39;Sum Sq&#39;</span>]</span>
<span id="cb436-2"><a href="bivariate.html#cb436-2" aria-hidden="true" tabindex="-1"></a>SS.error <span class="ot">=</span> <span class="fu">anova</span>(lm.fit)[<span class="st">&#39;Residuals&#39;</span>, <span class="st">&#39;Sum Sq&#39;</span>]</span>
<span id="cb436-3"><a href="bivariate.html#cb436-3" aria-hidden="true" tabindex="-1"></a>df<span class="fl">.1</span> <span class="ot">=</span> <span class="dv">1</span> <span class="co"># anova(lm.fit)[&#39;iq&#39;, &#39;Df&#39;]</span></span>
<span id="cb436-4"><a href="bivariate.html#cb436-4" aria-hidden="true" tabindex="-1"></a>df<span class="fl">.2</span> <span class="ot">=</span> <span class="fu">nrow</span>(iq.gpa)<span class="sc">-</span><span class="dv">2</span> <span class="co"># anova(lm.fit)[&#39;Residuals&#39;, &#39;Df&#39;]</span></span>
<span id="cb436-5"><a href="bivariate.html#cb436-5" aria-hidden="true" tabindex="-1"></a>( <span class="at">F =</span> (SS.yhat<span class="sc">/</span>df<span class="fl">.1</span>) <span class="sc">/</span> (SS.error<span class="sc">/</span>df<span class="fl">.2</span>) )</span></code></pre></div>
<pre><code>## [1] 28.14285</code></pre>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="bivariate.html#cb438-1" aria-hidden="true" tabindex="-1"></a>( <span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pf</span>(F, df<span class="fl">.1</span>, df<span class="fl">.2</span>) )</span></code></pre></div>
<pre><code>## [1] 2.832145e-06</code></pre>
</div>
<div id="isomorphism-with-one-response-and-one-predictor" class="section level3">
<h3>Isomorphism with one response and one predictor</h3>
<p>In this special case of a linear regression with just one explanatory variable, all of these are equivalent. The t statistic for the correlation is the t-statistic for the coefficient, and that t-value squared is the F value from the analysis of variance. The resulting p-values are also the same.</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="bivariate.html#cb440-1" aria-hidden="true" tabindex="-1"></a><span class="co"># t statistics from slope and correlation</span></span>
<span id="cb440-2"><a href="bivariate.html#cb440-2" aria-hidden="true" tabindex="-1"></a>(<span class="at">ts =</span> <span class="fu">c</span>(<span class="fu">coef</span>(<span class="fu">summary</span>(lm.fit))[<span class="st">&#39;iq&#39;</span>, <span class="st">&#39;t value&#39;</span>], </span>
<span id="cb440-3"><a href="bivariate.html#cb440-3" aria-hidden="true" tabindex="-1"></a>        <span class="fu">cor.test</span>(iq.gpa<span class="sc">$</span>iq, iq.gpa<span class="sc">$</span>gpa)<span class="sc">$</span>statistic) )</span></code></pre></div>
<pre><code>##                 t 
## 5.304983 5.304983</code></pre>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="bivariate.html#cb442-1" aria-hidden="true" tabindex="-1"></a><span class="co"># squared t statistics and anova F value</span></span>
<span id="cb442-2"><a href="bivariate.html#cb442-2" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(ts<span class="sc">^</span><span class="dv">2</span>, <span class="fu">anova</span>(lm.fit)[<span class="st">&#39;iq&#39;</span>, <span class="st">&#39;F value&#39;</span>])</span></code></pre></div>
<pre><code>##                 t          
## 28.14285 28.14285 28.14285</code></pre>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="bivariate.html#cb444-1" aria-hidden="true" tabindex="-1"></a><span class="co"># p values from slope, correlation, and anova</span></span>
<span id="cb444-2"><a href="bivariate.html#cb444-2" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">coef</span>(<span class="fu">summary</span>(lm.fit))[<span class="st">&#39;iq&#39;</span>, <span class="st">&#39;Pr(&gt;|t|)&#39;</span>], </span>
<span id="cb444-3"><a href="bivariate.html#cb444-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cor.test</span>(iq.gpa<span class="sc">$</span>iq, iq.gpa<span class="sc">$</span>gpa)<span class="sc">$</span>p.value,</span>
<span id="cb444-4"><a href="bivariate.html#cb444-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anova</span>(lm.fit)[<span class="st">&#39;iq&#39;</span>, <span class="st">&#39;Pr(&gt;F)&#39;</span>])</span></code></pre></div>
<pre><code>## [1] 2.832145e-06 2.832145e-06
## [3] 2.832145e-06</code></pre>
<p>This isomorphism between the test for the pairwise correlation (<code>cor.test(x,y)</code>), the test of the estimated regression slope (<code>summary(lm(y~x))</code>), and the test for the variance in y attributable to x (<code>anova(lm(y~x))</code>), is specific to the simple case of one response and one explanatory variable. With multiple explanatory variables (multiple regression) these will all yield different results (as they all ask subtly different questions – more on this when we get to multiple regression).</p>

</div>
</div>
<div id="bivariate-prediction" class="section level2">
<h2>Regression prediction.</h2>
<p>We will start our discussion of prediction intervals with the same fake IQ-GPA data that we covered in <a href="ols.html">OLS regression</a>.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="bivariate.html#cb446-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb446-2"><a href="bivariate.html#cb446-2" aria-hidden="true" tabindex="-1"></a>IQ <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb446-3"><a href="bivariate.html#cb446-3" aria-hidden="true" tabindex="-1"></a>GPA <span class="ot">=</span> <span class="fu">pmax</span>(<span class="dv">1</span>, <span class="fu">pmin</span>(<span class="fl">4.3</span>, <span class="fu">round</span>((IQ<span class="dv">-100</span>)<span class="sc">/</span><span class="dv">15</span><span class="sc">*</span><span class="fl">0.5+2.8</span><span class="sc">+</span><span class="fu">rnorm</span>(<span class="fu">length</span>(IQ),<span class="dv">0</span>,<span class="fl">0.7</span>), <span class="dv">1</span>)))</span>
<span id="cb446-4"><a href="bivariate.html#cb446-4" aria-hidden="true" tabindex="-1"></a>iq.gpa <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">iq=</span>IQ, <span class="at">gpa=</span>GPA)</span>
<span id="cb446-5"><a href="bivariate.html#cb446-5" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">ggplot</span>(iq.gpa, <span class="fu">aes</span>(iq,gpa))<span class="sc">+</span></span>
<span id="cb446-6"><a href="bivariate.html#cb446-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">2</span>)</span>
<span id="cb446-7"><a href="bivariate.html#cb446-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb446-8"><a href="bivariate.html#cb446-8" aria-hidden="true" tabindex="-1"></a>( <span class="at">lm.fit =</span> <span class="fu">lm</span>(<span class="at">data =</span> iq.gpa, gpa<span class="sc">~</span>iq) )</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gpa ~ iq, data = iq.gpa)
## 
## Coefficients:
## (Intercept)           iq  
##     0.44571      0.02496</code></pre>
<p>Now that we have estimated the best fitting intercept (<span class="math inline">\(B_0\)</span>) and slope (<span class="math inline">\(B_1\)</span>), we can ask what y values we predict for an arbitrary x.</p>
<p>There are two kinds of predictions we might make: a prediction for the <strong>mean y value</strong> at a given x, and a prediction for a <strong>new data point</strong> at a given x.</p>
<div id="predicting-mean-y-given-x." class="section level3">
<h3>Predicting mean y given x.</h3>
<p>Our uncertainty about the slope and our uncertainty about the mean of y will combine to give us uncertainty about the y value that the line will pass through at a given x. This line describes the mean of y at each x; consequently, our uncertainty about the line, is our uncertainty about mean y at a given x.</p>
<p>Two sources of uncertainty contribute to our error in estimating the y value at a given x: (1) uncertainty about the mean of y, which contributes a constant amount of error regardless of which x we are talking about, and (2) extrapolation uncertainty, due to our uncertainty about the slope – this source of error grows the further from the mean of x we try to predict a new y value. These combine into the net error in predicted y values as:</p>
<p><span class="math inline">\(s\{\hat y \mid x\} = s_e \sqrt{\frac{1}{n} + \frac{(x-\bar x)^2}{s_x^2 (n-1)}}\)</span></p>
<p>To calculate this error in R, and get a corresponding confidence interval, we use the <code>predict</code> function, which yields this standard error, the confidence interval on the y value of the line at a given x, which is defined by:</p>
<p><span class="math inline">\((\hat y \mid x) \pm t^*_{alpha} s\{\hat y \mid x\}\)</span></p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="bivariate.html#cb448-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm.fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">iq=</span><span class="dv">160</span>), <span class="at">se.fit =</span> T, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>, <span class="at">level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## $fit
##        fit      lwr      upr
## 1 4.438798 3.648578 5.229017
## 
## $se.fit
## [1] 0.39302
## 
## $df
## [1] 48
## 
## $residual.scale
## [1] 0.7039122</code></pre>
<p>We can do this calculation manually:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="bivariate.html#cb450-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(iq.gpa)</span>
<span id="cb450-2"><a href="bivariate.html#cb450-2" aria-hidden="true" tabindex="-1"></a>m.x <span class="ot">=</span> <span class="fu">mean</span>(iq.gpa<span class="sc">$</span>iq)</span>
<span id="cb450-3"><a href="bivariate.html#cb450-3" aria-hidden="true" tabindex="-1"></a>m.y <span class="ot">=</span> <span class="fu">mean</span>(iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb450-4"><a href="bivariate.html#cb450-4" aria-hidden="true" tabindex="-1"></a>s.x <span class="ot">=</span> <span class="fu">sd</span>(iq.gpa<span class="sc">$</span>iq)</span>
<span id="cb450-5"><a href="bivariate.html#cb450-5" aria-hidden="true" tabindex="-1"></a>s.y <span class="ot">=</span> <span class="fu">sd</span>(iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb450-6"><a href="bivariate.html#cb450-6" aria-hidden="true" tabindex="-1"></a>r.xy <span class="ot">=</span> <span class="fu">cor</span>(iq.gpa<span class="sc">$</span>iq, iq.gpa<span class="sc">$</span>gpa)</span>
<span id="cb450-7"><a href="bivariate.html#cb450-7" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">=</span> r.xy<span class="sc">*</span>s.y<span class="sc">/</span>s.x</span>
<span id="cb450-8"><a href="bivariate.html#cb450-8" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">=</span> m.y <span class="sc">-</span> b1<span class="sc">*</span>m.x</span>
<span id="cb450-9"><a href="bivariate.html#cb450-9" aria-hidden="true" tabindex="-1"></a>s.e <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((iq.gpa<span class="sc">$</span>gpa <span class="sc">-</span> (iq.gpa<span class="sc">$</span>iq<span class="sc">*</span>b1 <span class="sc">+</span> b0))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(n<span class="dv">-2</span>))</span>
<span id="cb450-10"><a href="bivariate.html#cb450-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb450-11"><a href="bivariate.html#cb450-11" aria-hidden="true" tabindex="-1"></a>new.x <span class="ot">=</span> <span class="dv">160</span></span>
<span id="cb450-12"><a href="bivariate.html#cb450-12" aria-hidden="true" tabindex="-1"></a>s.yhat.x <span class="ot">=</span> <span class="cf">function</span>(new.x){s.e<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n <span class="sc">+</span> (new.x <span class="sc">-</span> m.x)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(s.x<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(n<span class="dv">-1</span>)))}</span>
<span id="cb450-13"><a href="bivariate.html#cb450-13" aria-hidden="true" tabindex="-1"></a>( <span class="at">s.yhat.160 =</span> <span class="fu">s.yhat.x</span>(new.x) )</span></code></pre></div>
<pre><code>## [1] 0.39302</code></pre>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="bivariate.html#cb452-1" aria-hidden="true" tabindex="-1"></a>t.crit <span class="ot">=</span> <span class="fu">qt</span>((<span class="dv">1</span><span class="fl">-0.95</span>)<span class="sc">/</span><span class="dv">2</span>,n<span class="dv">-2</span>)</span>
<span id="cb452-2"><a href="bivariate.html#cb452-2" aria-hidden="true" tabindex="-1"></a>y.hat <span class="ot">=</span> b0 <span class="sc">+</span> b1<span class="sc">*</span>new.x</span>
<span id="cb452-3"><a href="bivariate.html#cb452-3" aria-hidden="true" tabindex="-1"></a>( y.hat <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span>t.crit<span class="sc">*</span><span class="fu">s.yhat.x</span>(new.x) )</span></code></pre></div>
<pre><code>## [1] 3.648578 5.229017</code></pre>
</div>
<div id="predicting-new-y-given-x." class="section level3">
<h3>Predicting new y given x.</h3>
<p>If instead of putting confidence intervals on mean y at a given x, we want confidence intervals on a new data point, we have to add to our uncertainty about the mean, our uncertainty about where data points are relative to the mean. Since our data do not fall exactly on the line, there is some spread of data around the line, and we have to take that into account when predicting a new data point. We do so by adding the variance of the residuals (the spread of data around the mean), to the variance of the line position (described in the previous section).</p>
<p><span class="math inline">\(s\{y \mid x\} = \sqrt{s_e^2 + s\{\hat y \mid x\}^2}\)</span></p>
<p>In R this is called a “prediction” interval, and we can get it with the <code>predict</code> function as well. Note that the “standard error” <code>predict</code> returns is still just the standard error of the line (same as the previous section), but the confidence intervals are defined by further incorporating the standard deviation of data points around the line (the standard deviation of the residuals).</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="bivariate.html#cb454-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm.fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">iq=</span><span class="dv">160</span>), <span class="at">se.fit =</span> T, <span class="at">interval =</span> <span class="st">&quot;predict&quot;</span>, <span class="at">level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## $fit
##        fit      lwr     upr
## 1 4.438798 2.817826 6.05977
## 
## $se.fit
## [1] 0.39302
## 
## $df
## [1] 48
## 
## $residual.scale
## [1] 0.7039122</code></pre>
<p>To do this manually, we just need to add the variance of the residuals to the variance of the line:</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="bivariate.html#cb456-1" aria-hidden="true" tabindex="-1"></a>new.x <span class="ot">=</span> <span class="dv">160</span></span>
<span id="cb456-2"><a href="bivariate.html#cb456-2" aria-hidden="true" tabindex="-1"></a>s.y.x <span class="ot">=</span> <span class="cf">function</span>(new.x){<span class="fu">sqrt</span>(s.e<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">s.yhat.x</span>(new.x)<span class="sc">^</span><span class="dv">2</span>)}</span>
<span id="cb456-3"><a href="bivariate.html#cb456-3" aria-hidden="true" tabindex="-1"></a>( <span class="at">s.y.160 =</span> <span class="fu">s.y.x</span>(new.x) )</span></code></pre></div>
<pre><code>## [1] 0.8061992</code></pre>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="bivariate.html#cb458-1" aria-hidden="true" tabindex="-1"></a>t.crit <span class="ot">=</span> <span class="fu">qt</span>((<span class="dv">1</span><span class="fl">-0.95</span>)<span class="sc">/</span><span class="dv">2</span>,n<span class="dv">-2</span>)</span>
<span id="cb458-2"><a href="bivariate.html#cb458-2" aria-hidden="true" tabindex="-1"></a>y.hat <span class="ot">=</span> b0 <span class="sc">+</span> b1<span class="sc">*</span>new.x</span>
<span id="cb458-3"><a href="bivariate.html#cb458-3" aria-hidden="true" tabindex="-1"></a>( y.hat <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span>t.crit<span class="sc">*</span><span class="fu">s.y.x</span>(new.x) )</span></code></pre></div>
<pre><code>## [1] 2.817826 6.059770</code></pre>
</div>
<div id="visualizing-the-difference" class="section level3">
<h3>Visualizing the difference</h3>
<p>It is useful to see how these two kinds of prediction confidence intervals change as a function of x. We see that the interval for predicting a new data point (gray) is much wider, due to the considerable amount of variability of data points around the line. The interval on the line is narrower and is also very saliently inhomogenous – it grows the further from the mean we are. Technically, the prediction (gray) interval also grows, but often this is not easily detectable by eye because so much of that variability is swamped by the variance of the data points around the line.</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="bivariate.html#cb460-1" aria-hidden="true" tabindex="-1"></a>iq <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">40</span>,<span class="dv">160</span>)</span>
<span id="cb460-2"><a href="bivariate.html#cb460-2" aria-hidden="true" tabindex="-1"></a>y.x <span class="ot">=</span> <span class="cf">function</span>(x){b0<span class="sc">+</span>b1<span class="sc">*</span>x}</span>
<span id="cb460-3"><a href="bivariate.html#cb460-3" aria-hidden="true" tabindex="-1"></a>t.crit <span class="ot">=</span> <span class="fu">qt</span>(<span class="fl">0.05</span>,n<span class="dv">-2</span>)</span>
<span id="cb460-4"><a href="bivariate.html#cb460-4" aria-hidden="true" tabindex="-1"></a>pred.df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">iq=</span>iq,</span>
<span id="cb460-5"><a href="bivariate.html#cb460-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">gpa =</span> <span class="fu">y.x</span>(iq),</span>
<span id="cb460-6"><a href="bivariate.html#cb460-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">s.yhat.x =</span> <span class="fu">s.yhat.x</span>(iq),</span>
<span id="cb460-7"><a href="bivariate.html#cb460-7" aria-hidden="true" tabindex="-1"></a>                     <span class="at">s.y.x =</span> <span class="fu">s.y.x</span>(iq))</span>
<span id="cb460-8"><a href="bivariate.html#cb460-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(pred.df, <span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa))<span class="sc">+</span></span>
<span id="cb460-9"><a href="bivariate.html#cb460-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymax=</span>gpa<span class="sc">+</span>s.y.x, <span class="at">ymin=</span>gpa<span class="sc">-</span>s.y.x), <span class="at">fill=</span><span class="st">&quot;gray&quot;</span>)<span class="sc">+</span></span>
<span id="cb460-10"><a href="bivariate.html#cb460-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymax=</span>gpa<span class="sc">+</span>s.yhat.x, <span class="at">ymin=</span>gpa<span class="sc">-</span>s.yhat.x), <span class="at">fill=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb460-11"><a href="bivariate.html#cb460-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>iq, <span class="at">y=</span>gpa), <span class="at">color=</span><span class="st">&quot;red&quot;</span>, <span class="at">size=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-225-1.png" width="3000" /></p>
<p>As we look at this plot, another salient feature should jump out: for very large or very small iq values, our predicted GPA is not contained in a reasonable range of GPAs. This is a problem of relying too much on a linear fit to a fundamentally non-linear relationship: the IQ-GPA relationship cannot be linear, because GPA has a lower and an upper bound. Consequently, if we extrapolate too far outside the range we studied, we will get predictions outside the reasonable bound (and we will also get very wide confidence intervals).</p>

</div>
</div>
<div id="bivariate-ols-diagnostics" class="section level2">
<h2>Regression Diagnostics</h2>
<p>When we fit a line to some data using ordinary least squares regression and then interpret the coefficients, corresponding t-tests, etc. we are making quite a few assumptions. When some of these are violated, we will git a misleading answer from the regression. In this section, we will conjure up various plots and tests to check some of these assumptions.</p>
<p>Some of the most important assumptions we make cannot be tested by looking at the data, instead they require domain knowledge. The most important assumption is that we are measuring the things we want to be. Usually we use some observable proxy variables for unobservable latent properties we are interested in. For instance, we might use GDP/capita of a country as a proxy for income of its citizens, but, this measure does not consider cost of living, income distributions, etc. Domain knowledge and critical thought is requires to guess whether the difference between the measurable proxy and the latent variable is a big deal with respect to our question of interest.</p>
<div id="assumption-relationship-between-y-and-x-is-well-described-by-a-line." class="section level3">
<h3>Assumption: relationship between y and x is well described by a line.</h3>
<p>We can always fit a line, that doesn’t mean it’s a good idea. The simplest, and most important way to check if it’s a good idea to fit a line is to look at the scatterplot. If it looks clearly non-linear, don’t fit a line to it!</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="bivariate.html#cb461-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressMessages</span>(<span class="fu">library</span>(dplyr))</span>
<span id="cb461-2"><a href="bivariate.html#cb461-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressMessages</span>(<span class="fu">library</span>(ggplot2))</span>
<span id="cb461-3"><a href="bivariate.html#cb461-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">200</span></span>
<span id="cb461-4"><a href="bivariate.html#cb461-4" aria-hidden="true" tabindex="-1"></a>quadratic.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>, <span class="at">length.out =</span> n), </span>
<span id="cb461-5"><a href="bivariate.html#cb461-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">y=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">4</span>, <span class="at">length.out =</span> n)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="fu">rnorm</span>(n))</span>
<span id="cb461-6"><a href="bivariate.html#cb461-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(quadratic.data, <span class="fu">aes</span>(x,y))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span><span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-226-1.png" width="3000" /></p>
<p>When we have a simple regression with just one predictor, it’s really easy to look at the scatterplot, so that’s usually enough, but we can also look at the residuals (difference between y and the y we predict from the regression) as a function of x:</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="bivariate.html#cb462-1" aria-hidden="true" tabindex="-1"></a>quadratic.data<span class="sc">$</span>residuals <span class="ot">=</span> <span class="fu">residuals</span>(<span class="fu">lm</span>(<span class="at">data=</span>quadratic.data, y<span class="sc">~</span>x))</span>
<span id="cb462-2"><a href="bivariate.html#cb462-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(quadratic.data, <span class="fu">aes</span>(x,residuals))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span><span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">color=</span><span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-227-1.png" width="3000" /></p>
<p>As we move to multiple regression, we can only look at one variable at a time in plots like the one above, so they tend to not be so useful. Instead, we will look at the residuals as a function of the fitted y value.</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="bivariate.html#cb463-1" aria-hidden="true" tabindex="-1"></a>quadratic.data<span class="sc">$</span>fitted.y <span class="ot">=</span> <span class="fu">fitted</span>(<span class="fu">lm</span>(<span class="at">data=</span>quadratic.data, y<span class="sc">~</span>x))</span>
<span id="cb463-2"><a href="bivariate.html#cb463-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(quadratic.data, <span class="fu">aes</span>(fitted.y,residuals))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span><span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">color=</span><span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-228-1.png" width="3000" /></p>
<p>This plot is also returned as the first diagnostic plot of a linear model, from <code>plot(lm(..), 1)</code>.</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="bivariate.html#cb464-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">lm</span>(<span class="at">data=</span>quadratic.data, y<span class="sc">~</span>x), <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-229-1.png" width="3000" /></p>
<p>All of these plots are basically showing us what was obvious from the get-go: the line misses the really big quadratic component we built into our data. So we probably shouldn’t be fitting a line.</p>
</div>
<div id="assumption-out-estimates-are-not-driven-by-a-few-huge-outliers" class="section level3">
<h3>Assumption: out estimates are not driven by a few huge outliers</h3>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="bivariate.html#cb465-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">99</span></span>
<span id="cb465-2"><a href="bivariate.html#cb465-2" aria-hidden="true" tabindex="-1"></a>outlier.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n),<span class="sc">-</span><span class="dv">15</span>), <span class="at">y=</span><span class="fu">c</span>(<span class="fu">rnorm</span>(n), <span class="dv">15</span>))</span>
<span id="cb465-3"><a href="bivariate.html#cb465-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(outlier.data, <span class="fu">aes</span>(x,y))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span><span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&#39;lm&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-230-1.png" width="3000" /></p>
<p>These fake data make it patently obvious that the one outlier at (-15,15) is driving the regression line, but often it might be more subtle.</p>
<p>What allows an outlier to greatly influence a regression? It needs to have a lot of <em>leverage</em> – meaning that it is an outlier with respect to the predictors (here, just x), which we can measure with <code>hat()</code>. Second, it needs to use that leverage by also being an outlier in y. We can check if such a thing happens by looking at the residuals as a function of the leverage.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="bivariate.html#cb466-1" aria-hidden="true" tabindex="-1"></a>outlier.data<span class="sc">$</span>leverage <span class="ot">=</span> <span class="fu">hat</span>(outlier.data<span class="sc">$</span>x)</span>
<span id="cb466-2"><a href="bivariate.html#cb466-2" aria-hidden="true" tabindex="-1"></a>outlier.data<span class="sc">$</span>residuals <span class="ot">=</span> <span class="fu">residuals</span>(<span class="fu">lm</span>(<span class="at">data=</span>outlier.data, y<span class="sc">~</span>x))</span>
<span id="cb466-3"><a href="bivariate.html#cb466-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(outlier.data, <span class="fu">aes</span>(leverage, residuals))<span class="sc">+</span><span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-231-1.png" width="3000" /></p>
<p>An more useful version of this plot is generated via <code>plot(lm(), 5)</code>:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="bivariate.html#cb467-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">lm</span>(<span class="at">data=</span>outlier.data, y<span class="sc">~</span>x), <span class="at">which =</span> <span class="dv">5</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-232-1.png" width="3000" /></p>
<p>This shows residuals (here they are standardized, meaning, scaled by their sd) as a function of leverage, just as in the plot we made above. However, it also shows contours corresponding to a particular “Cook’s distance.” Cook’s distance is high when a data point has a lot of <em>influence</em> on the regression, meaning that it has a lot of leverage and uses it (by also being an outlier in y). It basically tells us how much this data point is changing our regression line. Proposed cutoffs for a Cook’s distance being too large are 1, or 4/n.</p>
<p>We can look at the Cook’s distance for each data point with <code>plot(lm(), 4)</code></p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="bivariate.html#cb468-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">lm</span>(<span class="at">data=</span>outlier.data, y<span class="sc">~</span>x), <span class="at">which =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-233-1.png" width="3000" /></p>
<p>If you have data points with very large Cook’s distances, it means that the regression line you have estimated is largely reflecting the influence of a few data points, rather than the bulk of your data.</p>
</div>
<div id="assumption-errors-are-independent-identically-distributed-normal." class="section level3">
<h3>Assumption: errors are independent, identically distributed, normal.</h3>
<p>There is also a set of assumptions about the behavior of the errors/residuals. They should be normal (so that we are justified in using t and F statistics), they should be identically distributed, and they should be independent.</p>
<div id="errors-are-independent-independent-of-x-fitted-y-order-each-other." class="section level4">
<h4>Errors are independent: independent of x, fitted y, order, each other.</h4>
<p>Probably the most important of these assumptions is that the errors are independent, but not being independent can mean a few different things.</p>
<ul>
<li><p>Errors can be autocorrelated in order (e.g., if I measure things over time, errors tend to be correlated over time) – this is a major issue in time-series analysis, but for the kind of data we usually deal with, we can mostly ignore it.</p></li>
<li><p>Errors might depend on x, or fitted y; this is the kind of thing we saw when we had obviously non-linear data. This is important, but we’ve already considered it.</p></li>
<li><p>Errors might correlate with each other – this is what happens when we incorrectly specify a nested / hierarchical model. For instance, if I measure the weight and height of 10 people, each 5 times, I will get 50 measurements; however, I only really have 10 independent units (people) – all measurements of the same person will have a correlated error with repsect to the overall weight~height relationship. We really need to avoid this kind of non-independence in errors, as it will lead to really wrong conclusions; but this is not something we can easily check for; we just need to understand the structure of our data, and specify the appropriate model for the error correlation (with <code>lmer</code> or <code>aov</code>, etc).</p></li>
</ul>
</div>
<div id="errors-are-identically-distributed" class="section level4">
<h4>Errors are identically distributed</h4>
<p>Typically, ‘identically distributed’ for errors refers to them being ‘homoscedastic,’ or ‘equal variance’ – meaning that the magnitude of the residuals is roughly consistent across our regression (rather than ‘heteroscedastic,’ in which there is more variability in y for some values of x than others).</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="bivariate.html#cb469-1" aria-hidden="true" tabindex="-1"></a>heteroscedastic.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">100</span>,<span class="at">length.out=</span><span class="dv">100</span>),</span>
<span id="cb469-2"><a href="bivariate.html#cb469-2" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">y=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">100</span>,<span class="at">length.out=</span><span class="dv">100</span>)<span class="sc">*</span><span class="fl">0.1</span><span class="sc">+</span></span>
<span id="cb469-3"><a href="bivariate.html#cb469-3" aria-hidden="true" tabindex="-1"></a>                                    <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">100</span>,<span class="at">length.out=</span><span class="dv">100</span>)<span class="sc">/</span><span class="dv">10</span>))</span>
<span id="cb469-4"><a href="bivariate.html#cb469-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(heteroscedastic.data, <span class="fu">aes</span>(x,y))<span class="sc">+</span><span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-234-1.png" width="3000" /></p>
<p>We can check for this using <code>plot(lm, 3)</code></p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="bivariate.html#cb470-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">lm</span>(<span class="at">data=</span>heteroscedastic.data, y<span class="sc">~</span>x),<span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-235-1.png" width="3000" /></p>
<p>This shows the absolute, standardized residuals as a function of the fitted y value (not as a function of x, so that this plot will also work for multiple regression with lots of explanatory variables). Clearly, we have bigger errors at bigger y values.</p>
<p>Sometimes such heteroscedasticity arises from our model being wrong. For instance, if we consider how much net worth fluctuates over time, we will see that wealthy people have much larger fluctuations in absolute terms. This is because fluctuations are constant not in dollars, but in percent. Thus, the heteroscedasticity in this case reflects that we are applying a linear model to dollars, but we should instead be considering log(dollars), or percent fluctuations.</p>
<p>So, if we see very large, blatant, heteroscedasticity, we should carefully evaluate if we are measuring something that we really believe should have linear, additive errors. If not, we should use an appropriate non-linear transform (like a logarithm), to get a measure of something that we <em>do</em> believe is linear and additive. However, if we have a variable that seems to us should be linear, and otherwise seems to behave linearly, a slight amount of heteroscedasticity is not that big of a deal, in the grand scheme of things.</p>
</div>
<div id="errors-are-normal" class="section level4">
<h4>Errors are normal</h4>
<p>The last technical assumption is that the errors (residuals) are normally distributed. Just as in the case of heteroscedasticity, if they clearly are not, we should think hard if we are specifying the correct model; however, slight deviations from normality are practically guaranteed, and are largely irrelevant.</p>
<p>We can check for this using a qq-plot of the residuals with <code>plot(lm, 2)</code>.</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="bivariate.html#cb471-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb471-2"><a href="bivariate.html#cb471-2" aria-hidden="true" tabindex="-1"></a>non.normal <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x, <span class="at">y=</span>x<span class="sc">*</span><span class="dv">1</span><span class="sc">+</span><span class="fu">rexp</span>(<span class="dv">100</span>, <span class="fl">0.5</span>))</span>
<span id="cb471-3"><a href="bivariate.html#cb471-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(non.normal, <span class="fu">aes</span>(x,y))<span class="sc">+</span><span class="fu">geom_point</span>()</span>
<span id="cb471-4"><a href="bivariate.html#cb471-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">lm</span>(<span class="at">data=</span>non.normal, y<span class="sc">~</span>x),<span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-236-1.png" width="3000" /><img src="_main_files/figure-html/unnamed-chunk-236-2.png" width="3000" /></p>
<p>Insofar as the stadardized residual quantiles don’t fall in line with the theoretical quantiles, we have non-normal residuals. I think a better way to look at weirdness of residuals would be to look at their histogram:</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="bivariate.html#cb472-1" aria-hidden="true" tabindex="-1"></a>non.normal<span class="sc">$</span>residuals <span class="ot">=</span> <span class="fu">residuals</span>(<span class="fu">lm</span>(<span class="at">data=</span>non.normal, y<span class="sc">~</span>x))</span>
<span id="cb472-2"><a href="bivariate.html#cb472-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(non.normal, <span class="fu">aes</span>(<span class="at">x=</span>residuals))<span class="sc">+</span><span class="fu">geom_density</span>(<span class="at">fill=</span><span class="st">&#39;gray&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-237-1.png" width="3000" /></p>
<p>If the histogram of the residuals looks really skewed, then it’s worth trying to figure out how to transform the y variable to get the data to behave more sensibly. Slight deviations from normality won’t matter much.</p>
</div>
</div>
<div id="testing-assumptions" class="section level3">
<h3>Testing assumptions</h3>
<p>There are assorted null hypothesis tests to see if these assumptions are violated. With enough (real) data, the answer will almost certainly be yes. With too little data, glaringly obvious violations might not reach statistical significance. So, in practice, I find the null hypothesis tests for assumption violations to not be especially useful, and the diagnostic plots to be far more practical.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chi-squared.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/notes/bivariate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
