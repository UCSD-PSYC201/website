<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Foundations of Statistics | UCSD Psyc 201ab / CSS 205 / Psyc 193</title>
  <meta name="description" content="This is the class website and lecture notes for psyc 201ab and CSS 205" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Foundations of Statistics | UCSD Psyc 201ab / CSS 205 / Psyc 193" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the class website and lecture notes for psyc 201ab and CSS 205" />
  <meta name="github-repo" content="UCSD-PSYC201/website" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Foundations of Statistics | UCSD Psyc 201ab / CSS 205 / Psyc 193" />
  
  <meta name="twitter:description" content="This is the class website and lecture notes for psyc 201ab and CSS 205" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability.html"/>
<link rel="next" href="t-tests.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html"><i class="fa fa-check"></i>Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#instructors"><i class="fa fa-check"></i>Instructors</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#class-meetings"><i class="fa fa-check"></i>Class meetings</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#grading"><i class="fa fa-check"></i>Grading</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#class-resources"><i class="fa fa-check"></i>Class Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html"><i class="fa fa-check"></i>201a Schedule</a>
<ul>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-0-introduction"><i class="fa fa-check"></i>Week 0: Introduction</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-1-data"><i class="fa fa-check"></i>Week 1: Data</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-2-visualization"><i class="fa fa-check"></i>Week 2: Visualization</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-3-theoretical-foundations"><i class="fa fa-check"></i>Week 3: Theoretical foundations</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-4-linear-model-regression"><i class="fa fa-check"></i>Week 4: Linear model: Regression</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-5-linear-model-midterm"><i class="fa fa-check"></i>Week 5: Linear model, midterm</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-6-linear-model-categorical-predictors"><i class="fa fa-check"></i>Week 6: Linear model: Categorical predictors</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-7-linear-model-ancova-diagnostics"><i class="fa fa-check"></i>Week 7: Linear model: ANCOVA, diagnostics</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-8-linear-model-linearizing-transforms"><i class="fa fa-check"></i>Week 8: Linear model: Linearizing transforms</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-9-covarying-errors-repeated-measures-random-effects"><i class="fa fa-check"></i>Week 9: Covarying errors (repeated measures / random effects)</a></li>
<li class="chapter" data-level="" data-path="course-201a.html"><a href="course-201a.html#week-10-review-and-preview"><i class="fa fa-check"></i>Week 10: Review and preview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html"><i class="fa fa-check"></i>Projects</a>
<ul>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#examples-of-this-sort-of-thing"><i class="fa fa-check"></i>Examples of this sort of thing</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a"><i class="fa fa-check"></i>201a</a>
<ul>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-timeline"><i class="fa fa-check"></i>201a Timeline</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#groups"><i class="fa fa-check"></i>Groups</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-project-plan"><i class="fa fa-check"></i>201a Project plan</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-preliminary-data-summaries"><i class="fa fa-check"></i>201a Preliminary data summaries</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-write-ups"><i class="fa fa-check"></i>201a Write-ups</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-presentation"><i class="fa fa-check"></i>201a Presentation</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#a-group-evaluation"><i class="fa fa-check"></i>201a Group-evaluation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#b"><i class="fa fa-check"></i>201b</a></li>
<li class="chapter" data-level="" data-path="course-projects.html"><a href="course-projects.html#data-sources"><i class="fa fa-check"></i>Data sources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html"><i class="fa fa-check"></i>R homework</a>
<ul>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#grading-1"><i class="fa fa-check"></i>Grading</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#collaboration"><i class="fa fa-check"></i>Collaboration</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#submitting-r-assignments"><i class="fa fa-check"></i>Submitting R Assignments</a>
<ul>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#writing-r-scripts"><i class="fa fa-check"></i>Writing R scripts</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#submitting-your-assignment"><i class="fa fa-check"></i>Submitting your assignment</a></li>
<li class="chapter" data-level="" data-path="r-homework.html"><a href="r-homework.html#additional-resources."><i class="fa fa-check"></i>Additional resources.</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Notes</b></span></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html"><i class="fa fa-check"></i>Getting started with R</a>
<ul>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#R-install"><i class="fa fa-check"></i>Installing R</a>
<ul>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#packages"><i class="fa fa-check"></i>Packages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#R-intro"><i class="fa fa-check"></i>Introduction to R</a>
<ul>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#getting-started"><i class="fa fa-check"></i>Getting started</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#better-data-analysis-code."><i class="fa fa-check"></i>Better data analysis code.</a></li>
<li class="chapter" data-level="" data-path="R-start.html"><a href="R-start.html#using-r-markdown"><i class="fa fa-check"></i>Using R-markdown</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i>Visualizations</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#general-rules-for-scientific-data-visualization."><i class="fa fa-check"></i>General rules for scientific data visualization.</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#picking-a-plot-whats-convention"><i class="fa fa-check"></i>Picking a plot (what’s convention)</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#categorical-0"><i class="fa fa-check"></i>Categorical ~ 0</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#histogram"><i class="fa fa-check"></i>Histogram</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#pie-chart"><i class="fa fa-check"></i>Pie chart</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#stacked-area"><i class="fa fa-check"></i>Stacked area</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-0"><i class="fa fa-check"></i>numerical ~ 0</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#histogram-density"><i class="fa fa-check"></i>Histogram &amp; density</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-categorical"><i class="fa fa-check"></i>numerical ~ categorical</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#bar-plot-with-error-bars"><i class="fa fa-check"></i>Bar plot with error bars</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#jittered-data-points."><i class="fa fa-check"></i>Jittered data points.</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#violaviolin-plot"><i class="fa fa-check"></i>Viola/Violin plot</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#box-and-whiskers-plot"><i class="fa fa-check"></i>Box and whiskers plot</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#overlayed-densities"><i class="fa fa-check"></i>Overlayed densities</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#empirical-cumulative-distribution"><i class="fa fa-check"></i>Empirical cumulative distribution</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#comparisons"><i class="fa fa-check"></i>Comparisons</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#recommendations"><i class="fa fa-check"></i>Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-numerical-2-x-numerical-0"><i class="fa fa-check"></i>numerical ~ numerical (2 x numerical ~ 0)</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#scatter-and-heatmap"><i class="fa fa-check"></i>Scatter and heatmap</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#conditional-means"><i class="fa fa-check"></i>Conditional means</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-numerical-categorical"><i class="fa fa-check"></i>numerical ~ numerical + categorical</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#categorical-numerical"><i class="fa fa-check"></i>categorical ~ numerical</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#x-categorical-and-categorical-categorical"><i class="fa fa-check"></i>2 x categorical and categorical ~ categorical</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#heatmap"><i class="fa fa-check"></i>Heatmap</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#categorical-categorical"><i class="fa fa-check"></i>categorical ~ categorical</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#extra-plot-notes."><i class="fa fa-check"></i>Extra plot notes.</a>
<ul>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#numerical-2-x-categorical"><i class="fa fa-check"></i>numerical ~ 2 x categorical</a></li>
<li class="chapter" data-level="" data-path="visualization.html"><a href="visualization.html#bin-width-and-bandwidths"><i class="fa fa-check"></i>bin width and bandwidths</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i>Probability</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-terms"><i class="fa fa-check"></i>Probability terms</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#absolute-probability-statements-prob"><i class="fa fa-check"></i>Absolute probability statements {prob}</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-comparisons"><i class="fa fa-check"></i>Probability comparisons</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#proportional-magnitudes-and-confusion"><i class="fa fa-check"></i>Proportional magnitudes and confusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-foundations"><i class="fa fa-check"></i>Foundations of probability</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#set-notation-for-combinations-of-outcomes."><i class="fa fa-check"></i>Set notation for combinations of outcomes.</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#basic-probability-definition-and-axioms"><i class="fa fa-check"></i>Basic probability definition and axioms</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#events-and-the-rules-of-probability."><i class="fa fa-check"></i>Events and the rules of probability.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-conditional"><i class="fa fa-check"></i>Conditional probability and Bayes</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#chain-rule"><i class="fa fa-check"></i>Chain rule</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#partitions-and-total-probability"><i class="fa fa-check"></i>Partitions and total probability</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#bayes-rule"><i class="fa fa-check"></i>Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-simulations"><i class="fa fa-check"></i>Simulation, Sampling and Monte Carlo.</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-long-run-frequency-and-the-law-of-large-numbers."><i class="fa fa-check"></i>Sampling, long-run frequency, and the law of large numbers.</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-to-estimate-event-probabilities."><i class="fa fa-check"></i>Sampling to estimate event probabilities.</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-of-a-conjunction-of-events"><i class="fa fa-check"></i>Probability of a conjunction of events</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-to-get-probability-of-disjunction"><i class="fa fa-check"></i>Sampling to get probability of disjunction</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#sampling-to-calculate-conditional-probability"><i class="fa fa-check"></i>Sampling to calculate conditional probability</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-rv-functions"><i class="fa fa-check"></i>Distribution functions: PDF, CDF, Quantile</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-distribution-mass-and-density-functions-p.d.f."><i class="fa fa-check"></i>Probability distribution (mass and density) functions (p.d.f.)</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#cumulative-distribution-functions-c.d.f."><i class="fa fa-check"></i>Cumulative distribution functions (c.d.f.)</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#quantile-functions-inverse-cdf."><i class="fa fa-check"></i>Quantile functions (inverse CDF).</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-expectation"><i class="fa fa-check"></i>Expectation and moments</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#there-are-a-few-useful-properties-about-how-the-mean-variance-skewness-and-excess-kurtosis-behave-under-various-operations"><i class="fa fa-check"></i>There are a few useful properties about how the Mean, Variance, Skewness, and Excess Kurtosis behave under various operations:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#probability-clt"><i class="fa fa-check"></i>Central limit theorem and the normal distribution</a>
<ul>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#requirements-for-clt-to-hold"><i class="fa fa-check"></i>Requirements for CLT to hold</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#normal-distribution"><i class="fa fa-check"></i>Normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html"><i class="fa fa-check"></i>Foundations of Statistics</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#NHST-simulation"><i class="fa fa-check"></i>Frequentist statistics via simulation</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#critical-values-alpha-power"><i class="fa fa-check"></i>Critical values, alpha, power</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#setting-up-the-alternate-hypothesis"><i class="fa fa-check"></i>Setting up the “Alternate hypothesis”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#figuring-out-power"><i class="fa fa-check"></i>Figuring out “power”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#figuring-out-alpha"><i class="fa fa-check"></i>Figuring out “alpha”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#showing-alpha-power"><i class="fa fa-check"></i>Showing alpha, power</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#figuring-out-the-critical-value."><i class="fa fa-check"></i>Figuring out the critical value.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#NHST-sampling"><i class="fa fa-check"></i>Sampling distributions</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#tl-dr."><i class="fa fa-check"></i>TL; DR.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#logic"><i class="fa fa-check"></i>Logic</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#expectation-about-the-sampling-distribution-of-the-sample-mean."><i class="fa fa-check"></i>Expectation about the sampling distribution of the sample mean.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#standard-error-of-the-sample-mean"><i class="fa fa-check"></i>Standard error (of the sample mean)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#NHST-basics-normal"><i class="fa fa-check"></i>Statistics via the Normal distribution</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#normal-null-hypothesis-significance-testing-nhst"><i class="fa fa-check"></i>(Normal) Null hypothesis significance testing (NHST)</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#normal-tests-with-sample-means"><i class="fa fa-check"></i>Normal tests with sample means</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#z-scores-and-z-tests"><i class="fa fa-check"></i>Z-scores and Z-tests</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#z-tests"><i class="fa fa-check"></i>Z-tests</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#normal-confidence-intervals-on-the-sample-mean"><i class="fa fa-check"></i>(Normal) Confidence intervals on the sample mean</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#what-are-these-percents-and-probabilities"><i class="fa fa-check"></i>What are these percents and probabilities?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#nhst-theory-normal"><i class="fa fa-check"></i>Null hypothesis significance testing</a>
<ul>
<li><a href="NHST.html#type-1-error-rate-alpha-alpha">Type 1 error rate: alpha (<span class="math inline">\(\alpha\)</span>)</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#the-alternate-model"><i class="fa fa-check"></i>The “alternate model”</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#effect-size"><i class="fa fa-check"></i>Effect size</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#calculating-power-from-effect-size"><i class="fa fa-check"></i>Calculating power from effect size</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#visualizing-alpha-and-power"><i class="fa fa-check"></i>Visualizing alpha and power</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#how-power-changes."><i class="fa fa-check"></i>How power changes.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#calculating-n-for-a-desired-level-of-power."><i class="fa fa-check"></i>Calculating n for a desired level of power.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sign-and-magnitude-errors."><i class="fa fa-check"></i>Sign and magnitude errors.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#theory-binomial"><i class="fa fa-check"></i>Binomial: Probability to statistics</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#data-description-summary"><i class="fa fa-check"></i>Data description / summary</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#estimation"><i class="fa fa-check"></i>Estimation</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#null-hypothesis-significance-testing-nhst"><i class="fa fa-check"></i>Null Hypothesis Significance testing (NHST)</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#model-selection"><i class="fa fa-check"></i>Model selection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#t-distribution"><i class="fa fa-check"></i>t-distribution</a>
<ul>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#tl-dr.-1"><i class="fa fa-check"></i>TL; DR.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sampling-distribution-of-sample-variance-and-t-statistic"><i class="fa fa-check"></i>Sampling distribution of sample variance, and t-statistic</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sample-variance"><i class="fa fa-check"></i>Sample variance</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sampling-distribution-of-the-sample-variance"><i class="fa fa-check"></i>Sampling distribution of the sample variance</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#sampling-distribution-of-t-statistic"><i class="fa fa-check"></i>Sampling distribution of t-statistic</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#t-distribution-1"><i class="fa fa-check"></i>T-distribution</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#degrees-of-freedom."><i class="fa fa-check"></i>Degrees of freedom.</a></li>
<li class="chapter" data-level="" data-path="NHST.html"><a href="NHST.html#summary."><i class="fa fa-check"></i>Summary.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html"><i class="fa fa-check"></i>(Student’s) t-tests</a>
<ul>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#sample-t-test"><i class="fa fa-check"></i>1-sample t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#paired-repeated-measures-t-test"><i class="fa fa-check"></i>Paired / repeated-measures t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#sample-presumed-equal-variance-t-test"><i class="fa fa-check"></i>2-sample, presumed equal variance, t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#sample-unequal-variance-t-test"><i class="fa fa-check"></i>2-sample, unequal variance, t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#power-calculations."><i class="fa fa-check"></i>Power calculations.</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#summary-of-tests-for-the-mean-and-effect-sizes"><i class="fa fa-check"></i>Summary of tests for the mean and effect sizes</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#math."><i class="fa fa-check"></i>Math.</a>
<ul>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#math-behind-2-sample-equal-variance-t-test"><i class="fa fa-check"></i>Math behind 2-sample equal variance t-test</a></li>
<li class="chapter" data-level="" data-path="t-tests.html"><a href="t-tests.html#math-behind-unequal-variance-t-test"><i class="fa fa-check"></i>Math behind unequal variance t-test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="binomial-test.html"><a href="binomial-test.html"><i class="fa fa-check"></i>Binomial test.</a>
<ul>
<li class="chapter" data-level="" data-path="binomial-test.html"><a href="binomial-test.html#estimating-proportions."><i class="fa fa-check"></i>Estimating proportions.</a></li>
<li class="chapter" data-level="" data-path="binomial-test.html"><a href="binomial-test.html#sign-test-test-for-percentiles"><i class="fa fa-check"></i>Sign test, test for percentiles</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html"><i class="fa fa-check"></i>Pearson’s Chi-squared test</a>
<ul>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#goodness-of-fit-test"><i class="fa fa-check"></i>“Goodness of fit” test</a>
<ul>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#implementation-in-r"><i class="fa fa-check"></i>Implementation in R</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#chi-squared-test-calculations"><i class="fa fa-check"></i>Chi-squared test calculations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#test-for-independence"><i class="fa fa-check"></i>Test for independence</a>
<ul>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#implementation-in-r-1"><i class="fa fa-check"></i>Implementation in R</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#independence-test-calculations"><i class="fa fa-check"></i>Independence test calculations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#more-than-2-way-contingency-table"><i class="fa fa-check"></i>More than 2-way contingency table</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#mathematical-rationale"><i class="fa fa-check"></i>Mathematical rationale</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#limitations"><i class="fa fa-check"></i>Limitations</a></li>
<li class="chapter" data-level="" data-path="chi-squared.html"><a href="chi-squared.html#fishers-exact-test"><i class="fa fa-check"></i>Fisher’s “exact” test</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i>Bivariate linear relationships</a>
<ul>
<li><a href="bivariate.html#linear-relationships"><em>Linear</em> relationships</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#covariance-and-correlation-measuring-the-linear-dependence."><i class="fa fa-check"></i>Covariance and correlation: Measuring the linear dependence.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#covariance"><i class="fa fa-check"></i>Covariance</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#correlation-coefficient"><i class="fa fa-check"></i>Correlation coefficient</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#ols-regression-predicting-the-mean-of-y-for-a-given-x"><i class="fa fa-check"></i>(OLS) Regression: Predicting the mean of y for a given x</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#difference-between-yx-xy-and-the-principle-component-line"><i class="fa fa-check"></i>Difference between y~x, x~y, and the principle component line</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#partitioning-variance"><i class="fa fa-check"></i>Partitioning variance</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-a-linear-relationship."><i class="fa fa-check"></i>Significance of a linear relationship.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#prediction-from-regression."><i class="fa fa-check"></i>Prediction from regression.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-anscombe"><i class="fa fa-check"></i>Anscombe’s quartet</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-covariance"><i class="fa fa-check"></i>Covariance</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#estimating-covariance."><i class="fa fa-check"></i>Estimating covariance.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-correlation"><i class="fa fa-check"></i>Correlation</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#correlation-as-the-slope-of-z-scores"><i class="fa fa-check"></i>Correlation as the slope of z-scores</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#coefficient-of-determination"><i class="fa fa-check"></i>Coefficient of determination</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-ols"><i class="fa fa-check"></i>Ordinary Least-Squares (OLS) Regression</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#regression-terminology"><i class="fa fa-check"></i>Regression terminology</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#estimating-the-regression-line."><i class="fa fa-check"></i>Estimating the regression line.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#standard-errors-of-regression-coefficients"><i class="fa fa-check"></i>Standard errors of regression coefficients</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#confidence-intervals-and-tests-for-regression-coefficients"><i class="fa fa-check"></i>Confidence intervals and tests for regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-lines"><i class="fa fa-check"></i>y~x vs x~y vs principle component line</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-determination"><i class="fa fa-check"></i>Partitioning variance and the coefficient of determination.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#calculating-sums-of-squares."><i class="fa fa-check"></i>Calculating sums of squares.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-significance"><i class="fa fa-check"></i>Significance of linear relationship.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-slope."><i class="fa fa-check"></i>Significance of slope.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-pairwise-correlation"><i class="fa fa-check"></i>Significance of pairwise correlation</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#significance-of-variance-partition."><i class="fa fa-check"></i>Significance of variance partition.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#isomorphism-with-one-response-and-one-predictor"><i class="fa fa-check"></i>Isomorphism with one response and one predictor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-prediction"><i class="fa fa-check"></i>Regression prediction.</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#predicting-mean-y-given-x."><i class="fa fa-check"></i>Predicting mean y given x.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#predicting-new-y-given-x."><i class="fa fa-check"></i>Predicting new y given x.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#visualizing-the-difference"><i class="fa fa-check"></i>Visualizing the difference</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#bivariate-ols-diagnostics"><i class="fa fa-check"></i>Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#assumption-relationship-between-y-and-x-is-well-described-by-a-line."><i class="fa fa-check"></i>Assumption: relationship between y and x is well described by a line.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#assumption-out-estimates-are-not-driven-by-a-few-huge-outliers"><i class="fa fa-check"></i>Assumption: out estimates are not driven by a few huge outliers</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#assumption-errors-are-independent-identically-distributed-normal."><i class="fa fa-check"></i>Assumption: errors are independent, identically distributed, normal.</a></li>
<li class="chapter" data-level="" data-path="bivariate.html"><a href="bivariate.html#testing-assumptions"><i class="fa fa-check"></i>Testing assumptions</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">UCSD Psyc 201ab / CSS 205 / Psyc 193</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="NHST" class="section level1">
<h1>Foundations of Statistics</h1>
<div id="NHST-simulation" class="section level2">
<h2>Frequentist statistics via simulation</h2>
<p>The logic of null hypothesis testing is based on the sampling distribution of our test statistic under the null hypothesis. I.e., what test statistics do we expect to see if our sample came from some null model? While in general we will use various analytical expressions for these sampling distributions, it may be clearer to generate them ourselves by sampling, to see what exactly we are doing.</p>
<p>Let’s use a coin flipping example to illustrate this logic.</p>
<ol style="list-style-type: decimal">
<li>We have some data. Here: a sequence of coin flips that are either heads (H) or tails (T).</li>
</ol>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="NHST.html#cb126-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">data =</span> <span class="fu">c</span>(<span class="st">&#39;H&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;T&#39;</span>, <span class="st">&#39;T&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;T&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;H&#39;</span>))</span></code></pre></div>
<pre><code>##  [1] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot;
## [10] &quot;H&quot;</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>We define some statistic on our data. Here: the number of heads.</li>
</ol>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="NHST.html#cb128-1" aria-hidden="true" tabindex="-1"></a>statistic <span class="ot">=</span> <span class="cf">function</span>(data){<span class="fu">sum</span>(<span class="fu">ifelse</span>(data<span class="sc">==</span><span class="st">&#39;H&#39;</span>, <span class="dv">1</span>, <span class="dv">0</span>))}</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>We calculate this statistic on our data.</li>
</ol>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="NHST.html#cb129-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">our.stat =</span> <span class="fu">statistic</span>(data))</span></code></pre></div>
<pre><code>## [1] 7</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>We define a null hypothesis: a generative model of our data that we want to reject. Here: flips of a fair coin.</li>
</ol>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="NHST.html#cb131-1" aria-hidden="true" tabindex="-1"></a>sample.from.null <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&#39;H&#39;</span>, <span class="st">&#39;T&#39;</span>), n, <span class="at">replace=</span><span class="cn">TRUE</span>)}</span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>We repeat many times the process of (a) generating some data under the null and (b) calculating the statistic many. This gives us the “sampling distribution of our statistic under the null hypothesis”</li>
</ol>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="NHST.html#cb132-1" aria-hidden="true" tabindex="-1"></a>null.samples <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">statistic</span>(<span class="fu">sample.from.null</span>(<span class="fu">length</span>(data))))</span>
<span id="cb132-2"><a href="NHST.html#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(null.samples)</span></code></pre></div>
<pre><code>##  num [1:10000] 4 3 3 4 4 5 3 5 3 5 ...</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>We compare our statistic to the null samples to see what fraction of them are at least as extreme as the one we saw. Here we define extremeness as “too many heads” (so its a one tailed test).</li>
</ol>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="NHST.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2, <span class="at">quietly =</span> T)</span>
<span id="cb134-2"><a href="NHST.html#cb134-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">statistic =</span> null.samples, </span>
<span id="cb134-3"><a href="NHST.html#cb134-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">more.extreme=</span><span class="fu">ifelse</span>(null.samples<span class="sc">&gt;=</span>our.stat, </span>
<span id="cb134-4"><a href="NHST.html#cb134-4" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">&quot;at least as extreme&quot;</span>, </span>
<span id="cb134-5"><a href="NHST.html#cb134-5" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">&quot;less extreme&quot;</span>)), </span>
<span id="cb134-6"><a href="NHST.html#cb134-6" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>statistic, <span class="at">fill=</span>more.extreme))<span class="sc">+</span></span>
<span id="cb134-7"><a href="NHST.html#cb134-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>()<span class="sc">+</span></span>
<span id="cb134-8"><a href="NHST.html#cb134-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks=</span><span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb134-9"><a href="NHST.html#cb134-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-10"><a href="NHST.html#cb134-10" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate p-value based on null distribution.</span></span>
<span id="cb134-11"><a href="NHST.html#cb134-11" aria-hidden="true" tabindex="-1"></a>(<span class="at">p.value =</span> <span class="fu">sum</span>(null.samples<span class="sc">&gt;=</span>our.stat)<span class="sc">/</span><span class="fu">length</span>(null.samples))</span></code></pre></div>
<pre><code>## [1] 0.1794</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-76-1.png" width="3000" /></p>
<p>The details of this procedure will vary depending on our null hypothesis and statistic in question. Sometimes we know enough about the null hypothesis to literally generate new data. Sometimes, we only know what we think should be invariant under the null hypothesis, and we do some sort of permutation/randomization of the data to generate null samples. Nonetheless, the procedure and logic are roughly the same.</p>
<div id="critical-values-alpha-power" class="section level3">
<h3>Critical values, alpha, power</h3>
<p>Let’s say that we are going to run our coin-flipping experiment on a coin that we suspect is bent. We will:<br />
(1) flip the coin 10 times<br />
(2) calculate the number of heads, and<br />
(3) “reject the null hypothesis” (of a fair coin) if the number of heads is surprisingly high.</p>
<p>What’s the probability that we will reject the null? To answer this question we need to decide a few things, and make some assumptions:</p>
<ol style="list-style-type: lower-alpha">
<li>what constitutes ‘surprisingly high?’ For now, let’s just say that we will declare 8 or more heads to be sufficiently “surprising” to reject the null. We will call this criterion the ‘critical value.’</li>
</ol>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="NHST.html#cb136-1" aria-hidden="true" tabindex="-1"></a>critical.value <span class="ot">=</span> <span class="dv">8</span></span></code></pre></div>
<ol start="2" style="list-style-type: lower-alpha">
<li>Exactly how bent do we think the coin is? Does it come up heads 65% of the time? 70%? 100%? Obviously, the more bent we think the coin is, the more ‘surprising’ the outcomes we would expect to see from it. Let’s say we think our bent coin comes up heads 75% of the time.</li>
</ol>
</div>
<div id="setting-up-the-alternate-hypothesis" class="section level3">
<h3>Setting up the “Alternate hypothesis”</h3>
<p>We have a way of sampling from the null (via <code>sample.from.null</code>), but now we need a way to sample possible outcomes we might see from the truly bent coin.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="NHST.html#cb137-1" aria-hidden="true" tabindex="-1"></a>sample.from.alternate <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&#39;H&#39;</span>, <span class="st">&#39;T&#39;</span>), </span>
<span id="cb137-2"><a href="NHST.html#cb137-2" aria-hidden="true" tabindex="-1"></a>                                             n, </span>
<span id="cb137-3"><a href="NHST.html#cb137-3" aria-hidden="true" tabindex="-1"></a>                                             <span class="at">prob=</span><span class="fu">c</span>(<span class="fl">0.75</span>, <span class="fl">0.25</span>),</span>
<span id="cb137-4"><a href="NHST.html#cb137-4" aria-hidden="true" tabindex="-1"></a>                                             <span class="at">replace=</span><span class="cn">TRUE</span>)}</span></code></pre></div>
<p>If we sample from the alternative many times, and calculate our statistic for each sample, we get samples from the distribution of statistics that we expect to see from the bent coin we hypothesized.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="NHST.html#cb138-1" aria-hidden="true" tabindex="-1"></a>alternate.samples <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">statistic</span>(<span class="fu">sample.from.alternate</span>(<span class="fu">length</span>(data))))</span>
<span id="cb138-2"><a href="NHST.html#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(alternate.samples)</span></code></pre></div>
<pre><code>##  num [1:10000] 9 9 7 7 6 6 6 7 9 6 ...</code></pre>
</div>
<div id="figuring-out-power" class="section level3">
<h3>Figuring out “power”</h3>
<p>Power is the probability that we will reject the null hypothesis for a sample taken from the “alternate” hypothesis. In our case, it just means: what proportion of statistics we simulated from the alternate hypothesis are going to be at least as big as the ‘critical value’ we chose?</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="NHST.html#cb140-1" aria-hidden="true" tabindex="-1"></a>( <span class="at">power =</span> <span class="fu">mean</span>(alternate.samples <span class="sc">&gt;=</span> critical.value) )</span></code></pre></div>
<pre><code>## [1] 0.5325</code></pre>
<p>So there’s our answer: that’s the probability that we would reject the null in an experiment that flipped 10 times a bent coin that comes up heads with probability 0.75, given our critical value of 8.</p>
<p>Note that to figure out power, we <em>have to</em> make some assumption about what the not-null alternative is. Without that, we have no way to figure out what samples from the alternate hypothesis would look like, and what fraction of them we would reject the null for.</p>
</div>
<div id="figuring-out-alpha" class="section level3">
<h3>Figuring out “alpha”</h3>
<p>What’s the probability that we would reject the null hypothesis if it turned out that we were flipping a fair coin after all? I.e., what’s the ‘false positive rate,’ how often would we reject the null, even though the null was true?</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="NHST.html#cb142-1" aria-hidden="true" tabindex="-1"></a>( <span class="at">alpha =</span> <span class="fu">mean</span>(null.samples <span class="sc">&gt;=</span> critical.value) )</span></code></pre></div>
<pre><code>## [1] 0.0559</code></pre>
</div>
<div id="showing-alpha-power" class="section level3">
<h3>Showing alpha, power</h3>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="NHST.html#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr, <span class="at">quietly =</span> T)</span>
<span id="cb144-2"><a href="NHST.html#cb144-2" aria-hidden="true" tabindex="-1"></a>all.data <span class="ot">&lt;-</span> </span>
<span id="cb144-3"><a href="NHST.html#cb144-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">n.heads =</span> null.samples,</span>
<span id="cb144-4"><a href="NHST.html#cb144-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">sampled.from =</span> <span class="st">&#39;null&#39;</span>),</span>
<span id="cb144-5"><a href="NHST.html#cb144-5" aria-hidden="true" tabindex="-1"></a>        <span class="fu">data.frame</span>(<span class="at">n.heads =</span> alternate.samples,</span>
<span id="cb144-6"><a href="NHST.html#cb144-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">sampled.from =</span> <span class="st">&#39;alternate&#39;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb144-7"><a href="NHST.html#cb144-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">null.rejected =</span> <span class="fu">ifelse</span>(n.heads <span class="sc">&gt;=</span> critical.value,</span>
<span id="cb144-8"><a href="NHST.html#cb144-8" aria-hidden="true" tabindex="-1"></a>                                <span class="st">&#39;reject null&#39;</span>,</span>
<span id="cb144-9"><a href="NHST.html#cb144-9" aria-hidden="true" tabindex="-1"></a>                                <span class="st">&#39;retain null&#39;</span>),</span>
<span id="cb144-10"><a href="NHST.html#cb144-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">label =</span> <span class="fu">paste0</span>(<span class="st">&#39;sampled from &#39;</span>, sampled.from, <span class="st">&quot; and &quot;</span> , null.rejected))</span>
<span id="cb144-11"><a href="NHST.html#cb144-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(all.data, <span class="fu">aes</span>(<span class="at">x=</span>n.heads, <span class="at">fill=</span>label))<span class="sc">+</span></span>
<span id="cb144-12"><a href="NHST.html#cb144-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(sampled.from<span class="sc">~</span>.)<span class="sc">+</span></span>
<span id="cb144-13"><a href="NHST.html#cb144-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">position=</span><span class="st">&#39;identity&#39;</span>, <span class="at">alpha=</span><span class="dv">1</span>)<span class="sc">+</span></span>
<span id="cb144-14"><a href="NHST.html#cb144-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">+</span></span>
<span id="cb144-15"><a href="NHST.html#cb144-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&#39;#009900&#39;</span>, <span class="st">&#39;#CC8888&#39;</span>, <span class="st">&#39;#990000&#39;</span>, <span class="st">&#39;#88CC88&#39;</span>))<span class="sc">+</span></span>
<span id="cb144-16"><a href="NHST.html#cb144-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> critical.value<span class="fl">-0.5</span>, <span class="at">color=</span><span class="st">&#39;red&#39;</span>, <span class="at">size=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-82-1.png" width="3000" /></p>
<p>The top panel shows the distribution of samples from the null hypothesis (a fair coin), the bottom panel shows samples from the alternate hypothesis (a bent coin that comes up heads with probability 0.75).<br />
Dark green corresponds to the samples from the bent coin for which we would reject the fair-coin null. These are ‘correct rejections of the null,’ or ‘hits,’ and the probability that this happens for samples from the alternate hypothesis is called “power.”<br />
Light green are samples from the null (fair coin) for which we would <em>not</em> reject the null. These are also correct, but they don’t have a common name.<br />
Dark red are samples from the null (fair coin) for which we <em>do</em> reject the null. These are often called false positives, or Type I errors, and the probability that this happens for samples from the null hypothesis is called alpha.<br />
Light red are samples from the alternate (bent coin) for which we do <em>not</em> reject the null. Thus they too are a mistake, often called ‘false negatives’ or Type II errors. The probability of this happening for samples from the alternate is 1 minus “power.”</p>
</div>
<div id="figuring-out-the-critical-value." class="section level3">
<h3>Figuring out the critical value.</h3>
<p>Above, we sort of just made up a critical value by saying that 8 or more heads out of 10 would be sufficiently surprising to reject. In general, we aren’t just going to make up a critical value, but we will instead pick a particular rate of false positives (Type I errors) that we are willing to tolerate. Thus, we will pick an alpha that we can be satisfied with (i.e., we will be content if we falsely reject the null hypothesis for this fraction of samples from the null hypothesis). So, let’s say that we will tolerate an alpha of 10%, so we want to find the maximum critical value, such that the proportion of samples from the null that would exceed it is 10% or less.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="NHST.html#cb145-1" aria-hidden="true" tabindex="-1"></a>null.distribution <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> null.samples) <span class="sc">%&gt;%</span></span>
<span id="cb145-2"><a href="NHST.html#cb145-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(x) <span class="sc">%&gt;%</span></span>
<span id="cb145-3"><a href="NHST.html#cb145-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>() <span class="sc">%&gt;%</span></span>
<span id="cb145-4"><a href="NHST.html#cb145-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb145-5"><a href="NHST.html#cb145-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(x)) <span class="sc">%&gt;%</span></span>
<span id="cb145-6"><a href="NHST.html#cb145-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> n<span class="sc">/</span><span class="fu">sum</span>(n)) <span class="sc">%&gt;%</span></span>
<span id="cb145-7"><a href="NHST.html#cb145-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob.x.or.more =</span> <span class="fu">cumsum</span>(prob))</span>
<span id="cb145-8"><a href="NHST.html#cb145-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(null.distribution, <span class="dv">11</span>)</span></code></pre></div>
<pre><code>## # A tibble: 11 x 4
##        x     n   prob prob.x.or.more
##    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;          &lt;dbl&gt;
##  1    10    10 0.001          0.001 
##  2     9    98 0.0098         0.0108
##  3     8   451 0.0451         0.0559
##  4     7  1235 0.124          0.179 
##  5     6  2046 0.205          0.384 
##  6     5  2446 0.245          0.629 
##  7     4  2034 0.203          0.832 
##  8     3  1125 0.112          0.945 
##  9     2   444 0.0444         0.989 
## 10     1    99 0.0099         0.999 
## 11     0    12 0.0012         1</code></pre>
<p>So the probability of getting 10/10 heads is about 0.001 under the null, of getting 9 or 10 heads is about 0.01, of getting 8 9 or 10 heads is a bit over 0.05, and the probability of getting 7,8,9, or 10 heads is about 0.17. So we would use 8 as our critical value, as we would expect to see 8 or more heads from the null hypothesis fewer than 10% of the time.</p>
<!---
## Confidence intervals via bootstrap.

If we want to build a confidence interval, we want to find a range of statistics that are likely to arise from new samples that came from the same population as ours.  Bootstrapping is a general procedure for doing so based on the assumption that our sample corresponds to our best guess about the distribution of the population.  Consequently, we could obtain hypothetical new samples from this population by resampling our sample.  To do this, we:

1) Get some data:


```r
data = c('H', 'H', 'H', 'H', 'T', 'H', 'T', 'H', 'H', 'H')
```

2) Define a statistic that we want a confidence interval on (here, the proportion of heads):


```r
statistic = function(data){sum(ifelse(data=='H', 1, 0))/length(data)}
```

3) Define a resampling function that samples new possible data sets (**of the same size as ours**) from our sample data **with replacement**:


```r
resample = function(data){sample(data, length(data), replace=TRUE)}
```

4) Generate lots of resampled samples, calculate our statistic on each:


```r
bootstrapped.samples = replicate(10000, statistic(resample(data)))
```

5) Calculate a confidence interval from these samples, usually by using the empirical quantile function:


```r
quantile(bootstrapped.samples, c(0.025, 0.975))
```

```
##  2.5% 97.5% 
##   0.5   1.0
```

This procedure is also very general, and can be used to define bootstrapped confidence intervals for any statistic.  The details of the procedure might vary (we might introduce extra variability to our resampling procedure), and its legitimacy depends on the size of the sample being resampled (too small a sample limits the fidelity and appropriateness of the confidence interval -- here our sample is probably too small), and the statistic in question (statistics that are more sensitive to extrema require larger sample sizes and may never be fully tractable via resampling).
--->

</div>
</div>
<div id="NHST-sampling" class="section level2">
<h2>Sampling distributions</h2>
<div id="tl-dr." class="section level3">
<h3>TL; DR.</h3>
<ul>
<li><p>Frequentist statistics are based on the distribution of statistics we might expect to see if we were to run the experiment many times. These are called ‘sampling distributions.’</p></li>
<li><p>The most common one is the “sampling distribution of the sample mean,” which is Normal (assuming the central limit theorem holds), has a mean equal to the mean of the underlying population, and has a standard deviation equal to the standard deviation of the underlying population divided by the square root of n (the sample size). This standard deviation of the sampling distribution of the sample mean is often called the “standard error of the mean.”</p></li>
</ul>
</div>
<div id="logic" class="section level3">
<h3>Logic</h3>
<p>Our data are a sample. If we were to rerun the experiment, we would get a different sample. The sampling distribution of something (technically, of a random variable) is the probability distribution describing the probability that this random variable will take on any possible value when we sample it. For instance, let’s consider a measurement of a sampled person’s IQ. Although we actually got some measurement, from the perspective of frequentist statistics, we must consider what other measurements we could have seen – so we say that the sampling distribution of a measurement of IQ is Normal with a mean of 100 and a standard deviation of 15 (this is how IQ is defined).</p>
<p>Just as it makes sense to talk about sampling distributions for measurements, or sets of measurements, it also makes sense to consider sampling distributions for <a href="descriptive.html">statistics</a>. For instance, we got a particular sample of 10 people’s IQs, and calculated the sample mean. Even though we saw one particular sample mean, we must consider what other sample means we could have seen (and their probability distribution) from carrying out our procedure of sampling 10 people and averaging their IQs (this is the sampling distribution of the sample mean). We can mathematically derive the probability distributions for sampling distributions of various statistics by relying on the <em>statistical model</em> we assume underlies our data.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="NHST.html#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb147-2"><a href="NHST.html#cb147-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-3"><a href="NHST.html#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="co"># here is a sampling function that generates a single sample iq.</span></span>
<span id="cb147-4"><a href="NHST.html#cb147-4" aria-hidden="true" tabindex="-1"></a>sample.iq <span class="ot">=</span> <span class="cf">function</span>(){<span class="fu">round</span>(<span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="dv">100</span>,<span class="dv">15</span>))}</span>
<span id="cb147-5"><a href="NHST.html#cb147-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-6"><a href="NHST.html#cb147-6" aria-hidden="true" tabindex="-1"></a><span class="co"># we can run it once to get one sample</span></span>
<span id="cb147-7"><a href="NHST.html#cb147-7" aria-hidden="true" tabindex="-1"></a><span class="fu">sample.iq</span>()</span></code></pre></div>
<pre><code>## [1] 115</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="NHST.html#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># or 10 times to generate a sample of 10 iqs:</span></span>
<span id="cb149-2"><a href="NHST.html#cb149-2" aria-hidden="true" tabindex="-1"></a><span class="fu">replicate</span>(<span class="dv">10</span>, <span class="fu">sample.iq</span>())</span></code></pre></div>
<pre><code>##  [1]  64  77 104 117  89  82 105  83  98
## [10]  89</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="NHST.html#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="co"># or 1000 times to get a sample of 1000 iqs</span></span>
<span id="cb151-2"><a href="NHST.html#cb151-2" aria-hidden="true" tabindex="-1"></a>iqs<span class="fl">.1000</span> <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">sample.iq</span>())</span>
<span id="cb151-3"><a href="NHST.html#cb151-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">iq =</span> iqs<span class="fl">.1000</span>), <span class="fu">aes</span>(<span class="at">x=</span>iq))<span class="sc">+</span><span class="fu">geom_bar</span>()</span>
<span id="cb151-4"><a href="NHST.html#cb151-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-5"><a href="NHST.html#cb151-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we can generalize this to write a function that generates a sample of n iqs</span></span>
<span id="cb151-6"><a href="NHST.html#cb151-6" aria-hidden="true" tabindex="-1"></a>sample.iqs <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">replicate</span>(n, <span class="fu">sample.iq</span>())}</span>
<span id="cb151-7"><a href="NHST.html#cb151-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-8"><a href="NHST.html#cb151-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Here is one possible sample mean of 10 sampled iqs</span></span>
<span id="cb151-9"><a href="NHST.html#cb151-9" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">sample.iqs</span>(<span class="dv">10</span>))</span></code></pre></div>
<pre><code>## [1] 107.8</code></pre>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="NHST.html#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We can generate 5 means of samples of 10 iqs:</span></span>
<span id="cb153-2"><a href="NHST.html#cb153-2" aria-hidden="true" tabindex="-1"></a><span class="fu">replicate</span>(<span class="dv">5</span>, <span class="fu">mean</span>(<span class="fu">sample.iqs</span>(<span class="dv">10</span>)))</span></code></pre></div>
<pre><code>## [1] 100.2 103.5 103.3 103.0 103.4</code></pre>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="NHST.html#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="co"># or a sample of 1000 sample means of 10 sampled iqs</span></span>
<span id="cb155-2"><a href="NHST.html#cb155-2" aria-hidden="true" tabindex="-1"></a>iq.means<span class="fl">.1000</span> <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">mean</span>(<span class="fu">sample.iqs</span>(<span class="dv">10</span>)))</span>
<span id="cb155-3"><a href="NHST.html#cb155-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">mean.iq =</span> iq.means<span class="fl">.1000</span>), <span class="fu">aes</span>(<span class="at">x=</span>mean.iq))<span class="sc">+</span><span class="fu">geom_bar</span>()</span>
<span id="cb155-4"><a href="NHST.html#cb155-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-5"><a href="NHST.html#cb155-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we can write a function that samples n iqs, and returns their mean:</span></span>
<span id="cb155-6"><a href="NHST.html#cb155-6" aria-hidden="true" tabindex="-1"></a>sample.iq.mean <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">mean</span>(<span class="fu">sample.iqs</span>(n))}</span>
<span id="cb155-7"><a href="NHST.html#cb155-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-8"><a href="NHST.html#cb155-8" aria-hidden="true" tabindex="-1"></a><span class="co"># now we can see how this Sampling Distribution of the Sample Mean changes as we change the sample size</span></span>
<span id="cb155-9"><a href="NHST.html#cb155-9" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">n=</span><span class="dv">4</span>, <span class="at">mean.iq =</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">sample.iq.mean</span>(<span class="dv">4</span>))),</span>
<span id="cb155-10"><a href="NHST.html#cb155-10" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">n=</span><span class="dv">16</span>, <span class="at">mean.iq =</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">sample.iq.mean</span>(<span class="dv">16</span>))),</span>
<span id="cb155-11"><a href="NHST.html#cb155-11" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">n=</span><span class="dv">64</span>, <span class="at">mean.iq =</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">sample.iq.mean</span>(<span class="dv">64</span>))),</span>
<span id="cb155-12"><a href="NHST.html#cb155-12" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">n=</span><span class="dv">256</span>, <span class="at">mean.iq =</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">sample.iq.mean</span>(<span class="dv">256</span>))))</span>
<span id="cb155-13"><a href="NHST.html#cb155-13" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb155-14"><a href="NHST.html#cb155-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>mean.iq, <span class="at">color=</span><span class="fu">as.factor</span>(n), <span class="at">fill=</span><span class="fu">as.factor</span>(n)))<span class="sc">+</span><span class="fu">geom_density</span>(<span class="at">alpha=</span><span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-89-1.png" width="3000" /><img src="_main_files/figure-html/unnamed-chunk-89-2.png" width="3000" /><img src="_main_files/figure-html/unnamed-chunk-89-3.png" width="3000" /></p>
<p>What this is designed to illustrate is that if we take a step back from the data we <em>actually have</em> and consider the data we <em>could have had</em>, we see that many different samples are possible, and many different sample means are possible. Which sample means are more or less likely depends on the size of our sample, and the sampling distribution of the individual data points. The resulting probability distribution of sample means we could have seen is the “sampling distribution of the sample mean.” Such sampling distributions exist for every statistic we could conjure up (sample standard deviation, sample kurtosis, etc.).</p>
</div>
<div id="expectation-about-the-sampling-distribution-of-the-sample-mean." class="section level3">
<h3>Expectation about the sampling distribution of the sample mean.</h3>
<p>Formally, we assume that a sample of size <span class="math inline">\(n\)</span> corresponds to <span class="math inline">\(n\)</span> random variables independently and identically distributed according to the sampling distribution of the data:</p>
<p><span class="math inline">\(\{x_1, ... x_n\} \sim P(X)\)</span></p>
<p>We can think of them all as different (independent) instantiations of the same random variable <span class="math inline">\(X\)</span> – the random variable of a single data point.</p>
<p>We might not know the details of the probability distribution of <span class="math inline">\(X\)</span>, but we assume that it has some defined mean and variance (here we use these to refer to the properties of the <em>random variable</em>, obtained by <a href="../prob-expectation.html">expectation</a>, <em>not the sample</em>):</p>
<p><span class="math inline">\(\mu_X = \operatorname{Mean}\left[X\right] = \mathbb{E}[X] = \int\limits_{x \in X} x P(X=x) dx\)</span></p>
<p><span class="math inline">\(\sigma_X^2 = \operatorname{Var}\left[X\right] = \mathbb{E}\left[{(X-\mu_X)^2}\right] = \int\limits_{x \in X} (x-\mu_X)^2 P(X=x) dx\)</span></p>
<p>The probability distribution of <span class="math inline">\(X\)</span>, which we might call the sampling distribution of a single data point, will also have some skewness, kurtosis, and higher order moments describing its shape.</p>
<p>The mean of <span class="math inline">\(n\)</span> data points is defined as:</p>
<p><span class="math inline">\(\bar x^{(n)} = \frac{1}{n}\sum\limits_{i=1}^n x_i\)</span>, (here we superscript x-bar with <span class="math inline">\((n)\)</span> to make it explicit that this is the mean of a sample of size <span class="math inline">\(n\)</span>).</p>
<p>What can we say about the sampling distribution of this sample mean? That is, what do we know about <span class="math inline">\(P(\bar x^{(n)})\)</span>?</p>
<p>The <a href="../prob-clt-normal.html">central limit theorem</a> tells us that if <span class="math inline">\(n\)</span> is large enough, skewness, kurtosis, and higher order moments will all shrink towards their values under a normal distribution. So if <span class="math inline">\(n\)</span> is large enough:</p>
<p><span class="math inline">\(P(\bar x^{(n)}) = \operatorname{Normal}(\operatorname{Mean}[\bar x^{(n)}], \sqrt{\operatorname{Var}[\bar x^{(n)}]})\)</span></p>
<p>In other words, if <span class="math inline">\(n\)</span> is large enough the sampling distribution of the sample mean will be approximately normal, with some mean and variance. What will the mean and variance of this distribution be?</p>
<p>To figure this out, we need to remember a few useful <a href="../prob-expectation.html">expectation</a> identities:</p>
<ul>
<li><p>For the sum of <span class="math inline">\(n\)</span> <em>independent</em> random variables <span class="math inline">\({X_1, ..., X_n}\)</span> all <em>identically distributed</em> as <span class="math inline">\(X\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\operatorname{Mean}\left[\sum_{i=1}^n X_i\right]=n \operatorname{Mean}\left[X\right]\)</span></p></li>
<li><p><span class="math inline">\(\operatorname{Var}\left[\sum_{i=1}^n X_i\right]=n \operatorname{Var}\left[X\right]\)</span></p></li>
</ul></li>
<li><p>For the outcome of multiplying by a constant <span class="math inline">\(a\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\operatorname{Mean}\left[a X\right]=a \operatorname{Mean}\left[X\right]\)</span></p></li>
<li><p><span class="math inline">\(\operatorname{Var}\left[a X\right]=a^2 \operatorname{Var}\left[X\right]\)</span></p></li>
</ul></li>
</ul>
<p>From this we can figure out the mean of the sampling distribution of the sample mean.</p>
<p><span class="math inline">\(\operatorname{Mean}[\bar x^{(n)}] = \operatorname{Mean}\left[\frac{1}{n}\sum\limits_{i=1}^n x_i\right]\)</span></p>
<p>Note that for clarity, we can break this up a bit by defining an intermediate variable: the sum of <span class="math inline">\(n\)</span> samples: <span class="math inline">\(U^{(n)}\)</span></p>
<p><span class="math inline">\(\bar x^{(n)} = \frac{1}{n} U^{(n)}\)</span>, where</p>
<p><span class="math inline">\(U^{(n)} = \sum\limits_{i=1}^n x_i\)</span></p>
<p>Now what can we say about the mean and variance of <span class="math inline">\(U^{(n)}\)</span>? From our expectation rules about the sums of <span class="math inline">\(n\)</span> iid variables, we get:</p>
<p><span class="math inline">\(\operatorname{Mean}[U^{(n)}] = \operatorname{Mean}\left[\sum\limits_{i=1}^n x_i\right] = n*\operatorname{Mean}[X] = n*\mu_X\)</span></p>
<p><span class="math inline">\(\operatorname{Var}[U^{(n)}] = \operatorname{Var}\left[\sum\limits_{i=1}^n x_i\right] = n*\operatorname{Var}[X] = n*\sigma_X^2\)</span></p>
<p>And using our expectation rules about multiplying a random variable by a constant we get:</p>
<p><span class="math inline">\(\operatorname{Mean}[\bar x^{(n)}] = \operatorname{Mean}\left[\frac{1}{n} U^{(n)}\right] = \frac{1}{n} \operatorname{Mean}[U^{(n)}] = \frac{1}{n}*n*\mu_X = \mu_X\)</span></p>
<p><span class="math inline">\(\operatorname{Var}[\bar x^{(n)}] = \operatorname{Var}\left[\frac{1}{n} U^{(n)}\right] = \left(\frac{1}{n}\right)^2 \operatorname{Var}[U^{(n)}] = \left(\frac{1}{n}\right)^2*n*\sigma_X^2 = \frac{1}{n} \sigma_X^2\)</span></p>
<p>So, we learned that mean and variance of the sampling distribution of the sample mean are given by:</p>
<p><span class="math inline">\(\operatorname{Mean}[\bar x^{(n)}] = \mu_X\)</span></p>
<p><span class="math inline">\(\operatorname{Var}[\bar x^{(n)}] = \frac{1}{n} \sigma_X^2\)</span></p>
<p>From our calculation of the variance of the sampling distribution of the sample mean, we can get the standard deviation:</p>
<p><span class="math inline">\(\operatorname{SD}[\bar x^{(n)}] = \sqrt{\frac{1}{n} \sigma_X^2} = \frac{\sigma_X}{\sqrt{n}}\)</span></p>
<p>Let’s see if all this hard work paid off by checking our answer with some simulations:</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="NHST.html#cb156-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">20</span></span>
<span id="cb156-2"><a href="NHST.html#cb156-2" aria-hidden="true" tabindex="-1"></a>mu.x <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb156-3"><a href="NHST.html#cb156-3" aria-hidden="true" tabindex="-1"></a>sigma.x <span class="ot">=</span> <span class="dv">15</span></span>
<span id="cb156-4"><a href="NHST.html#cb156-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-5"><a href="NHST.html#cb156-5" aria-hidden="true" tabindex="-1"></a>mu.xbar <span class="ot">=</span> mu.x</span>
<span id="cb156-6"><a href="NHST.html#cb156-6" aria-hidden="true" tabindex="-1"></a>sigma.xbar <span class="ot">=</span> sigma.x<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb156-7"><a href="NHST.html#cb156-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-8"><a href="NHST.html#cb156-8" aria-hidden="true" tabindex="-1"></a><span class="co"># so we expect the mean and standard deviation of the sample mean (xbar) to be:</span></span>
<span id="cb156-9"><a href="NHST.html#cb156-9" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(mu.xbar, sigma.xbar)</span></code></pre></div>
<pre><code>## [1] 100.000000   3.354102</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="NHST.html#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s generate a lot of simulated sample means, and see if they have the right mean and sd</span></span>
<span id="cb158-2"><a href="NHST.html#cb158-2" aria-hidden="true" tabindex="-1"></a>sample.n <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">rnorm</span>(n, mu.x, sigma.x)}</span>
<span id="cb158-3"><a href="NHST.html#cb158-3" aria-hidden="true" tabindex="-1"></a>sample.xbar.n <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">mean</span>(<span class="fu">sample.n</span>(n))}</span>
<span id="cb158-4"><a href="NHST.html#cb158-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-5"><a href="NHST.html#cb158-5" aria-hidden="true" tabindex="-1"></a>sampled.xbars <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">sample.xbar.n</span>(n))</span>
<span id="cb158-6"><a href="NHST.html#cb158-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-7"><a href="NHST.html#cb158-7" aria-hidden="true" tabindex="-1"></a><span class="co"># do our sampled sample means have the mean and sd we predict?</span></span>
<span id="cb158-8"><a href="NHST.html#cb158-8" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">mean</span>(sampled.xbars), <span class="fu">sd</span>(sampled.xbars))</span></code></pre></div>
<pre><code>## [1] 99.963928  3.374292</code></pre>
<p>Great, these are spot on, modulo some sampling variability (if we wanted to be really, really sure, we could increase the number of sample means we sample).</p>
<p>So now we have learned something about the sampling distribution of the sample mean:</p>
<p><span class="math inline">\(\bar x^{(n)} \sim \operatorname{Normal}\left({\mu_X, \frac{\sigma_X}{\sqrt{n}}}\right)\)</span></p>
<p>This calculation about the sampling distribution of the sample mean is the basis of many statistical procedures.</p>
</div>
<div id="standard-error-of-the-sample-mean" class="section level3">
<h3>Standard error (of the sample mean)</h3>
<p>Often we are interested in how our sample mean (<span class="math inline">\(\bar x^{(n)}\)</span>) differs from the population mean (<span class="math inline">\(\mu_X\)</span>). What can we say about the distribution of the <em>error</em> of our sample mean: <span class="math inline">\(\bar x^{(n)} - \mu_X\)</span>? Well, using our expectation rules for adding a constant, we can see that:</p>
<p><span class="math inline">\(\operatorname{Mean}\left[{(\bar x^{(n)}-\mu_X)}\right] = 0\)</span>, and</p>
<p><span class="math inline">\(\operatorname{SD}\left[{(\bar x^{(n)}-\mu_X)}\right] = \frac{\sigma_X}{\sqrt{n}}\)</span>.</p>
<p>Since the shape of the distribution will not change, we can say that our error is normally distributed:</p>
<p><span class="math inline">\((\bar x^{(n)}-\mu_X) \sim \operatorname{Normal}\left({0, \frac{\sigma_X}{\sqrt{n}}}\right)\)</span></p>
<p>The fact that the sampling distribution of the error of our sample mean has a mean of 0 means that the arithmetic mean is an <em>unbiased</em> estimate of the population mean. The standard deviation of the sampling distribution of the error of the sample mean is called the <strong>standard error of the sample mean</strong>. In general, for any estimator, the standard deviation of the sampling distribution of the error of that estimator is called the <strong>standard error</strong> (we will see such standard errors for slopes in regression, linear contrasts, etc.).</p>
<p>Usually we will denote the standard error <span class="math inline">\(s_{\cdot}\)</span> with a subscript, or <span class="math inline">\(\operatorname{se}\{\cdot\}\)</span> with brackets:</p>
<p><span class="math inline">\(\operatorname{se}\{\bar x^{(n)}\} = s_{\bar x^{(n)}} = \frac{\sigma_X}{\sqrt{n}}\)</span></p>
<p>(Technically, we would be more correct to call this <span class="math inline">\(\sigma_{\bar x^{(n)}}\)</span>, since its a standard error obtained from the population standard deviation, rather than the sample standard deviation, but let’s gloss over that for now.)</p>

</div>
</div>
<div id="NHST-basics-normal" class="section level2">
<h2>Statistics via the Normal distribution</h2>
<p>Based on the <a href="#prob-clt-normal">central limit theorem</a> and our derivation of the properties of the <a href="#nhst-sampling-distribution">sampling distribution of the sample mean</a>, we can undertake some classical (frequentist) statistics.</p>
<p>When we calculate the mean of <span class="math inline">\(n\)</span> independent samples from a population with mean <span class="math inline">\(\mu_X\)</span> and standard deviation <span class="math inline">\(\sigma_X\)</span>, the “sampling distribution of the sample mean” will follow roughly a Normal distribution, centered on the population mean, and with the standard deviation reduced by a factor of <span class="math inline">\(\sqrt{n}\)</span>:</p>
<p><span class="math inline">\(\bar x^{(n)} \sim \operatorname{Normal}\left({\mu_X, \frac{\sigma_X}{\sqrt{n}}}\right)\)</span></p>
<p>If we calculate instead the sampling distribution of the error between the sample and population means, we get:</p>
<p><span class="math inline">\((\bar x^{(n)}-\mu_X) \sim \operatorname{Normal}\left({0, \frac{\sigma_X}{\sqrt{n}}}\right)\)</span></p>
<p>We use the convenient phrase <strong>standard error of the sample mean</strong> to refer to the standard deviation of the sampling distribution of the sample mean (and also the standard deviation of the sampling distribution of the error – the deviation of the sample mean from the population mean). This standard error of the sample mean (or more accurately, its estimate) is something that you will often see as error bars in graphs (and the axis label or figure caption will say something like “mean <span class="math inline">\(\pm\)</span> s.e.m.”).</p>
<div id="normal-null-hypothesis-significance-testing-nhst" class="section level3">
<h3>(Normal) Null hypothesis significance testing (NHST)</h3>
<p>Let’s say we go back in time to carefully study the case of <a href="https://en.wikipedia.org/wiki/Phineas_Gage">Phineas Gage</a>. We compose a battery to measure emotional decision-making, assemble a large number of normal/healthy patients, and administer this battery to all of them. We find that the emotional-decision making scores are distributed in the healthy population as a Normal distribution with mean=50, sd=5. We then measure Phineas Gage, and find he has a score of 39.</p>
<p>The NHST approach postulates a null hypothesis (H0): a statistical model of our data if the effect we care about does not exist in the world. In this case, our null model might be described as “Gage’s emotional decision-making score is a random sample from the distribution of those scores in the normal population”: <span class="math inline">\(x_{\text{Gage}} \sim \operatorname{Normal}(50, 5)\)</span>.</p>
<p>We would then calculate a “test statistic” on our data. Let’s start by just using Gage’s score as our test statistic: <span class="math inline">\(x_{\text{Gage}}\)</span>.</p>
<p>From the null model, we can obtain the “sampling distribution of the test statistic under the null hypothesis,” in this case, it is just the distribution of emotional decision-making scores in the healthy population: <span class="math inline">\(\operatorname{Normal}(50, 5)\)</span></p>
<p>Now we want to assess whether the observed test statistic was sufficiently extreme compared to its null hypothesis distribution. In general this is done by assessing whether a statistic <em>at least as extreme</em> as the one we saw will occur with a probability smaller than <span class="math inline">\(\alpha\)</span> under the null hypothesis. The value of <span class="math inline">\(\alpha\)</span> indicates how often we are willing to <em>falsely reject the null hypothesis</em> (that is, reject the null hypothesis when our observation came from the null hypothesis); generally, folks use <span class="math inline">\(\alpha=0.05\)</span>, meaning we are content to falsely reject the null 1 out of 20 times.</p>
<p>We can do this in several ways:<br />
(1) define “critical” (cut-off) values which correspond to the <span class="math inline">\(\alpha\)</span> value.<br />
(2) compare our observed test statistic directly to the null distribution to obtain a <em>p-value</em> and see if it is less than alpha.</p>
<p>Let’s work through such a Z-test to see if Gage’s score was significantly lower (a one-tailed test) compared to the distribution of population scores.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="NHST.html#cb160-1" aria-hidden="true" tabindex="-1"></a>gage <span class="ot">=</span> <span class="dv">39</span></span>
<span id="cb160-2"><a href="NHST.html#cb160-2" aria-hidden="true" tabindex="-1"></a>H0.mu <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb160-3"><a href="NHST.html#cb160-3" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb160-4"><a href="NHST.html#cb160-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb160-5"><a href="NHST.html#cb160-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-6"><a href="NHST.html#cb160-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha</span></span>
<span id="cb160-7"><a href="NHST.html#cb160-7" aria-hidden="true" tabindex="-1"></a>crit.score <span class="ot">=</span> <span class="fu">qnorm</span>(alpha, H0.mu, H0.sd)</span>
<span id="cb160-8"><a href="NHST.html#cb160-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-9"><a href="NHST.html#cb160-9" aria-hidden="true" tabindex="-1"></a><span class="co"># plot things.</span></span>
<span id="cb160-10"><a href="NHST.html#cb160-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb160-11"><a href="NHST.html#cb160-11" aria-hidden="true" tabindex="-1"></a>xs <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">20</span>,<span class="dv">70</span>,<span class="at">by=</span><span class="fl">0.1</span>)</span>
<span id="cb160-12"><a href="NHST.html#cb160-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> xs, </span>
<span id="cb160-13"><a href="NHST.html#cb160-13" aria-hidden="true" tabindex="-1"></a>                  <span class="at">dens =</span> <span class="fu">dnorm</span>(xs, H0.mu, H0.sd), </span>
<span id="cb160-14"><a href="NHST.html#cb160-14" aria-hidden="true" tabindex="-1"></a>                  <span class="at">reject=</span><span class="fu">ifelse</span>(xs <span class="sc">&lt;=</span> crit.score, <span class="st">&quot;reject H0&quot;</span>, <span class="st">&quot;do not reject&quot;</span>)),</span>
<span id="cb160-15"><a href="NHST.html#cb160-15" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>dens, <span class="at">fill=</span>reject))<span class="sc">+</span><span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin=</span><span class="dv">0</span>, <span class="at">ymax=</span>dens))<span class="sc">+</span></span>
<span id="cb160-16"><a href="NHST.html#cb160-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>gage, <span class="at">size=</span><span class="dv">2</span>, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb160-17"><a href="NHST.html#cb160-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>crit.score, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-91-1.png" width="3000" /></p>
<p>This plot shows Gage’s score (blue line), the “critical score” such that P(score <span class="math inline">\(\leq\)</span> crit.score | H0)=<span class="math inline">\(\alpha\)</span> (black line), and the null hypothesis distribution colored based on whether or not it is above or below the critical value. The area under the curve below the critical value is equal to <span class="math inline">\(\alpha\)</span> (in this case 0.05). The area under the curve at smaller values than Gage’s score (not shaded specially) corresponds to the “p-value.” It should be clear that if Gage’s score is more extreme than (in this case, below) the critical value, then the p-value will be smaller than <span class="math inline">\(\alpha\)</span>; thus whether we choose to compare our test statistic to the critical value, or the p-value directly to <span class="math inline">\(\alpha\)</span>, we will get the same answer.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="NHST.html#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see if gage&#39;s score is lower than the critical score:</span></span>
<span id="cb161-2"><a href="NHST.html#cb161-2" aria-hidden="true" tabindex="-1"></a>gage <span class="sc">&lt;</span> crit.score</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="NHST.html#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculte p-value for gage&#39;s score by evaluating P(score &lt;= gage | H0)</span></span>
<span id="cb163-2"><a href="NHST.html#cb163-2" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="fu">pnorm</span>(gage, H0.mu, H0.sd)</span>
<span id="cb163-3"><a href="NHST.html#cb163-3" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>## [1] 0.01390345</code></pre>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="NHST.html#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare p.value to alpha</span></span>
<span id="cb165-2"><a href="NHST.html#cb165-2" aria-hidden="true" tabindex="-1"></a>p.value <span class="sc">&lt;</span> alpha</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div id="two-tailed-tests" class="section level4">
<h4>Two-tailed tests</h4>
<p>So far we have done a “one-tailed” test, in the sense that we were only testing whether Gage’s score was really low, compared to the normal population. In general, you should favor two-tailed tests, which can reject the null hypothesis whether the score is too extreme in either the positive or negative direction. You should favor two-tailed tests when they are possible, since there are few cases when you would actually ignore an extremely high score even though you expected an extremely low one (which is what a one-tailed test presumes).</p>
<p>To run a two-tailed test on a Normal distribution we need to define two critical values (a positive and a negative one), such that P(score <span class="math inline">\(\leq\)</span> low.crit | H0) = <span class="math inline">\(\alpha/2\)</span> and P(score <span class="math inline">\(\geq\)</span> high.crit ) = <span class="math inline">\(\alpha/2\)</span>. Note that we are “distributing” our <span class="math inline">\(\alpha\)</span> probability across both high and low tails, to maintain the same rate of falsely rejecting the null hypothesis.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="NHST.html#cb167-1" aria-hidden="true" tabindex="-1"></a>gage <span class="ot">=</span> <span class="dv">39</span></span>
<span id="cb167-2"><a href="NHST.html#cb167-2" aria-hidden="true" tabindex="-1"></a>H0.mu <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb167-3"><a href="NHST.html#cb167-3" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb167-4"><a href="NHST.html#cb167-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb167-5"><a href="NHST.html#cb167-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-6"><a href="NHST.html#cb167-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha</span></span>
<span id="cb167-7"><a href="NHST.html#cb167-7" aria-hidden="true" tabindex="-1"></a>low.crit <span class="ot">=</span> <span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>, H0.mu, H0.sd)</span>
<span id="cb167-8"><a href="NHST.html#cb167-8" aria-hidden="true" tabindex="-1"></a>high.crit <span class="ot">=</span> <span class="fu">qnorm</span>((<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>), H0.mu, H0.sd)</span>
<span id="cb167-9"><a href="NHST.html#cb167-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-10"><a href="NHST.html#cb167-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-11"><a href="NHST.html#cb167-11" aria-hidden="true" tabindex="-1"></a><span class="co"># plot things.</span></span>
<span id="cb167-12"><a href="NHST.html#cb167-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> xs, </span>
<span id="cb167-13"><a href="NHST.html#cb167-13" aria-hidden="true" tabindex="-1"></a>                  <span class="at">dens =</span> <span class="fu">dnorm</span>(xs, H0.mu, H0.sd), </span>
<span id="cb167-14"><a href="NHST.html#cb167-14" aria-hidden="true" tabindex="-1"></a>                  <span class="at">reject=</span><span class="fu">ifelse</span>(xs<span class="sc">&lt;=</span>low.crit, <span class="st">&quot;reject low&quot;</span>, <span class="fu">ifelse</span>(xs<span class="sc">&gt;=</span>high.crit, <span class="st">&quot;reject high&quot;</span>, <span class="st">&quot;do not reject&quot;</span>))),</span>
<span id="cb167-15"><a href="NHST.html#cb167-15" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>dens, <span class="at">fill=</span>reject, <span class="at">group=</span>reject))<span class="sc">+</span><span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin=</span><span class="dv">0</span>, <span class="at">ymax=</span>dens))<span class="sc">+</span></span>
<span id="cb167-16"><a href="NHST.html#cb167-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>gage, <span class="at">size=</span><span class="dv">2</span>, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb167-17"><a href="NHST.html#cb167-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>low.crit, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)<span class="sc">+</span></span>
<span id="cb167-18"><a href="NHST.html#cb167-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>high.crit, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-93-1.png" width="3000" /></p>
<p>This plot shows Gage’s score (blue line), the high and low critical scores (black lines), and the null hypothesis distribution colored based on whether or not it would be rejected (by being either below the low critical score, or above the high critical score). The total area under the curve past the critical values is equal to <span class="math inline">\(\alpha\)</span>.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="NHST.html#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see if gage&#39;s score is lower than the low critical score, or higher than the high critical score.</span></span>
<span id="cb168-2"><a href="NHST.html#cb168-2" aria-hidden="true" tabindex="-1"></a>gage <span class="sc">&lt;=</span> low.crit <span class="sc">|</span> gage <span class="sc">&gt;=</span> high.crit</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="NHST.html#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculte p-value for gage&#39;s score by taking the minimum of the lower and upper tails, and multiplying by 2 (to get symmetric 2-tailed p-value)</span></span>
<span id="cb170-2"><a href="NHST.html#cb170-2" aria-hidden="true" tabindex="-1"></a>p.value.low <span class="ot">=</span> <span class="fu">pnorm</span>(gage, H0.mu, H0.sd)</span>
<span id="cb170-3"><a href="NHST.html#cb170-3" aria-hidden="true" tabindex="-1"></a>p.value.high <span class="ot">=</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(gage, H0.mu, H0.sd)</span>
<span id="cb170-4"><a href="NHST.html#cb170-4" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">min</span>(p.value.low, p.value.high)</span>
<span id="cb170-5"><a href="NHST.html#cb170-5" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>## [1] 0.0278069</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="NHST.html#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare p.value to alpha</span></span>
<span id="cb172-2"><a href="NHST.html#cb172-2" aria-hidden="true" tabindex="-1"></a>p.value <span class="sc">&lt;=</span> alpha</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Note that for the two tailed test, we see if the score is more extreme than either the low or high critical value, and we calculate a p-value by taking the minimum of the lower and upper tail probabilities, and multiplying by 2 (because of the symmetry of the Normal).</p>
<p>So what is this p-value that we’ve been calculating? It is the probability that a score <em>sampled from the null hypothesis</em> will be at least as as “extreme” as the one we observed. When we calculate a one-tailed test, “at least as extreme” corresponds to extremeness in the direction of the tail we are testing. When it is a two-tailed test, we need to figure out what the corresponding “extreme” score at the other tail would be (for a Normal, this is easy, as they are symmetric, so we can just multiply by 2).</p>
<p>What a p-value is <strong>not</strong> the probability that the null hypothesis is true (this requires the Bayesian calculation of P(H0 | data), rather than integrating over P(data | H0), as we have done).</p>
</div>
</div>
<div id="normal-tests-with-sample-means" class="section level3">
<h3>Normal tests with sample means</h3>
<p>Imagine that instead of having one Phineas Gage, there was an epidemic of exploding, scull-piercing tamping irons all piercing the frontal lobes of many railroad workers. You can administer your test to 5 such unlucky “Gages,” and you want to compare their average to the population. To do this, you will need to calculate their mean, and you will need to calculate the sampling distribution of the sample mean under the null hypothesis (that these individuals’ scores are samples from the overall population).</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="NHST.html#cb174-1" aria-hidden="true" tabindex="-1"></a>gages <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">39</span>, <span class="dv">44</span>, <span class="dv">38</span>, <span class="dv">40</span>, <span class="dv">51</span>)</span>
<span id="cb174-2"><a href="NHST.html#cb174-2" aria-hidden="true" tabindex="-1"></a>H0.mu <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb174-3"><a href="NHST.html#cb174-3" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb174-4"><a href="NHST.html#cb174-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb174-5"><a href="NHST.html#cb174-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-6"><a href="NHST.html#cb174-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate sample statistics: mean, and n</span></span>
<span id="cb174-7"><a href="NHST.html#cb174-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(gages)</span>
<span id="cb174-8"><a href="NHST.html#cb174-8" aria-hidden="true" tabindex="-1"></a>x.bar <span class="ot">=</span> <span class="fu">mean</span>(gages)</span>
<span id="cb174-9"><a href="NHST.html#cb174-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-10"><a href="NHST.html#cb174-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean and sd of sampling distribution of the sample mean under null</span></span>
<span id="cb174-11"><a href="NHST.html#cb174-11" aria-hidden="true" tabindex="-1"></a>H0.xbar.mu <span class="ot">=</span> H0.mu</span>
<span id="cb174-12"><a href="NHST.html#cb174-12" aria-hidden="true" tabindex="-1"></a>H0.xbar.sd <span class="ot">=</span> H0.sd<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb174-13"><a href="NHST.html#cb174-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-14"><a href="NHST.html#cb174-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha</span></span>
<span id="cb174-15"><a href="NHST.html#cb174-15" aria-hidden="true" tabindex="-1"></a>low.crit <span class="ot">=</span> <span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>, H0.xbar.mu, H0.xbar.sd)</span>
<span id="cb174-16"><a href="NHST.html#cb174-16" aria-hidden="true" tabindex="-1"></a>high.crit <span class="ot">=</span> <span class="fu">qnorm</span>((<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>), H0.xbar.mu, H0.xbar.sd)</span>
<span id="cb174-17"><a href="NHST.html#cb174-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-18"><a href="NHST.html#cb174-18" aria-hidden="true" tabindex="-1"></a><span class="co"># plot things.</span></span>
<span id="cb174-19"><a href="NHST.html#cb174-19" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> xs, </span>
<span id="cb174-20"><a href="NHST.html#cb174-20" aria-hidden="true" tabindex="-1"></a>                  <span class="at">dens =</span> <span class="fu">dnorm</span>(xs, H0.xbar.mu, H0.xbar.sd), </span>
<span id="cb174-21"><a href="NHST.html#cb174-21" aria-hidden="true" tabindex="-1"></a>                  <span class="at">reject=</span><span class="fu">ifelse</span>(xs<span class="sc">&lt;=</span>low.crit, <span class="st">&quot;reject low&quot;</span>, <span class="fu">ifelse</span>(xs<span class="sc">&gt;=</span>high.crit, <span class="st">&quot;reject high&quot;</span>, <span class="st">&quot;do not reject&quot;</span>))),</span>
<span id="cb174-22"><a href="NHST.html#cb174-22" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>dens, <span class="at">fill=</span>reject, <span class="at">group=</span>reject))<span class="sc">+</span><span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin=</span><span class="dv">0</span>, <span class="at">ymax=</span>dens))<span class="sc">+</span></span>
<span id="cb174-23"><a href="NHST.html#cb174-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>x.bar, <span class="at">size=</span><span class="dv">2</span>, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb174-24"><a href="NHST.html#cb174-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>low.crit, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)<span class="sc">+</span></span>
<span id="cb174-25"><a href="NHST.html#cb174-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>high.crit, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-95-1.png" width="3000" /></p>
<p>Note that here we used our derived <a href="nhst-sampling-distribution.html">sampling distribution for the sample mean</a>, consequently, the null distribution is skinnier. Otherwise, all the other calculations of p-values, critical statistic values, etc. is exactly the same:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="NHST.html#cb175-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see if mean score of gages is lower than the low critical score, or higher than the high critical score.</span></span>
<span id="cb175-2"><a href="NHST.html#cb175-2" aria-hidden="true" tabindex="-1"></a>x.bar <span class="sc">&lt;=</span> low.crit <span class="sc">|</span> x.bar <span class="sc">&gt;=</span> high.crit</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="NHST.html#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculte p-value for mean score of gages by taking the minimum of the lower and upper tails, and multiplying by 2 (to get symmetric 2-tailed p-value)</span></span>
<span id="cb177-2"><a href="NHST.html#cb177-2" aria-hidden="true" tabindex="-1"></a>p.value.low <span class="ot">=</span> <span class="fu">pnorm</span>(x.bar, H0.xbar.mu, H0.xbar.sd)</span>
<span id="cb177-3"><a href="NHST.html#cb177-3" aria-hidden="true" tabindex="-1"></a>p.value.high <span class="ot">=</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(x.bar, H0.xbar.mu, H0.xbar.sd)</span>
<span id="cb177-4"><a href="NHST.html#cb177-4" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">min</span>(p.value.low, p.value.high)</span>
<span id="cb177-5"><a href="NHST.html#cb177-5" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>## [1] 0.0006767642</code></pre>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="NHST.html#cb179-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare p.value to alpha</span></span>
<span id="cb179-2"><a href="NHST.html#cb179-2" aria-hidden="true" tabindex="-1"></a>p.value <span class="sc">&lt;=</span> alpha</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="z-scores-and-z-tests" class="section level3">
<h3>Z-scores and Z-tests</h3>
<p>As we see, we now have enough machinery in place to do null hypothesis tests with the normal distribution. However, it is useful to introduce the notion of a <strong>z-score</strong>, as most classical instruction in statistics use such “standardized” statistics (rather than the raw scores and means as we have).</p>
<p>If <span class="math inline">\(x \sim \operatorname{Normal}(\mu_x, \sigma_X)\)</span>, and we apply a linear transformation to obtain a new variable <span class="math inline">\(z = (x-\mu_X)/\sigma_X\)</span> we have calculated a z-score. Following our rules about expectations, we can show that this z-score will have a <strong>standard normal distribution</strong>, meaning it will be distributed with a mean of 0 and a standard deviation of 1:</p>
<p><span class="math inline">\(z \sim \operatorname{Normal}(0,1)\)</span></p>
<p>Because such standardization is so easy to do, and yields the same standard Normal distribution whenever we apply it, it serves as the backbone of most classical statistical methods (which were developed at a time when calculating cumulative probabilities for arbitrary distributions was hard, and by standardizing our procedure, we could simply calculate the cumulative probability and quantile tables for just the standard distribution).</p>
<p>It is critical to note that whenever we calculate a z-score, we do so relative to some distribution. We <em>can</em> z-score someone’s height relative to the IQ distribution (e.g., <span class="math inline">\((69-100)/15\)</span>), but that would be weird and useless. One warning sign that this is weird and useless is to consider the <em>units</em> of the resulting number. Normally, z-scores are <em>unitless</em>: they take a measurement in one unit (say inches), subtracts some mean in those units (yielding a difference in inches), and divides by the standard deviation of such measurements (also in inches), and thus we get a number that has no units. However, if we take height in inches, and divide by the standard deviation of IQ, we get a number in units of inches/IQ – such “<a href="https://en.wikipedia.org/wiki/Dimensional_analysis">unit analyses</a>” are a good way to catch ourselves doing something incoherent.</p>
<p>Generally, we want to z-score some value relative to the (presumed) <em>sampling distribution</em> of that value. If we have an IQ score, we z-score it to the assumed sampling distribution of IQ. If we have the mean of 10 IQ scores, we z-score that sample mean to the sampling distribution of the sample mean of 10 IQ scores. Such z-scores will serve as our “test statistics” for Z-tests, and also will be used to estimate Normal confidence intervals.</p>
</div>
<div id="z-tests" class="section level3">
<h3>Z-tests</h3>
<p>To run the “Z-test” to assess whether the mean of our 5 Gages’ scores was sufficiently different from the null hypothesis distribution of the mean of 5 regular person scores we would first Z-transform it: subtract the mean of the sampling distribution of the sample mean, and divide by the standard deviation of the sampling distribution of the sample mean:<br />
<span class="math inline">\(Z_{\bar x} = (\bar x - \mu_{\bar x})/\sigma_{\bar x}\)</span><br />
<span class="math inline">\(Z_{\bar x} = (42.4 - 50)/2.236 = -3.4\)</span></p>
<p>We can now do the same null hypothesis calculation procedures we carried out earlier, but for the <em>sampling distribution of the (appropriate) z-score</em>:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="NHST.html#cb181-1" aria-hidden="true" tabindex="-1"></a>gages <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">39</span>, <span class="dv">44</span>, <span class="dv">38</span>, <span class="dv">40</span>, <span class="dv">51</span>)</span>
<span id="cb181-2"><a href="NHST.html#cb181-2" aria-hidden="true" tabindex="-1"></a>H0.mu <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb181-3"><a href="NHST.html#cb181-3" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb181-4"><a href="NHST.html#cb181-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb181-5"><a href="NHST.html#cb181-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-6"><a href="NHST.html#cb181-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate sample statistics: mean, and n</span></span>
<span id="cb181-7"><a href="NHST.html#cb181-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(gages)</span>
<span id="cb181-8"><a href="NHST.html#cb181-8" aria-hidden="true" tabindex="-1"></a>x.bar <span class="ot">=</span> <span class="fu">mean</span>(gages)</span>
<span id="cb181-9"><a href="NHST.html#cb181-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-10"><a href="NHST.html#cb181-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean and sd of sampling distribution of the sample mean under null</span></span>
<span id="cb181-11"><a href="NHST.html#cb181-11" aria-hidden="true" tabindex="-1"></a>H0.xbar.mu <span class="ot">=</span> H0.mu</span>
<span id="cb181-12"><a href="NHST.html#cb181-12" aria-hidden="true" tabindex="-1"></a>H0.xbar.sd <span class="ot">=</span> H0.sd<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb181-13"><a href="NHST.html#cb181-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-14"><a href="NHST.html#cb181-14" aria-hidden="true" tabindex="-1"></a>Z.xbar <span class="ot">=</span> (x.bar <span class="sc">-</span> H0.xbar.mu)<span class="sc">/</span>H0.xbar.sd</span>
<span id="cb181-15"><a href="NHST.html#cb181-15" aria-hidden="true" tabindex="-1"></a>H0.z.mu <span class="ot">=</span> <span class="dv">0</span>   <span class="co"># we worked this out earlier: z scores ~ Normal(0,1)</span></span>
<span id="cb181-16"><a href="NHST.html#cb181-16" aria-hidden="true" tabindex="-1"></a>H0.z.sd <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb181-17"><a href="NHST.html#cb181-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-18"><a href="NHST.html#cb181-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate a critical score such that p(score &lt;= crit.score | H0) = alpha</span></span>
<span id="cb181-19"><a href="NHST.html#cb181-19" aria-hidden="true" tabindex="-1"></a>low.crit <span class="ot">=</span> <span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>, H0.z.mu, H0.z.sd)</span>
<span id="cb181-20"><a href="NHST.html#cb181-20" aria-hidden="true" tabindex="-1"></a>high.crit <span class="ot">=</span> <span class="fu">qnorm</span>((<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>), H0.z.mu, H0.z.sd)</span>
<span id="cb181-21"><a href="NHST.html#cb181-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-22"><a href="NHST.html#cb181-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plot things.</span></span>
<span id="cb181-23"><a href="NHST.html#cb181-23" aria-hidden="true" tabindex="-1"></a>zs <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb181-24"><a href="NHST.html#cb181-24" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">z =</span> zs, </span>
<span id="cb181-25"><a href="NHST.html#cb181-25" aria-hidden="true" tabindex="-1"></a>                  <span class="at">dens =</span> <span class="fu">dnorm</span>(zs, H0.z.mu, H0.z.sd), </span>
<span id="cb181-26"><a href="NHST.html#cb181-26" aria-hidden="true" tabindex="-1"></a>                  <span class="at">reject=</span><span class="fu">ifelse</span>(zs<span class="sc">&lt;=</span>low.crit, <span class="st">&quot;reject low&quot;</span>, <span class="fu">ifelse</span>(zs<span class="sc">&gt;=</span>high.crit, <span class="st">&quot;reject high&quot;</span>, <span class="st">&quot;do not reject&quot;</span>))),</span>
<span id="cb181-27"><a href="NHST.html#cb181-27" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>z, <span class="at">y=</span>dens, <span class="at">fill=</span>reject, <span class="at">group=</span>reject))<span class="sc">+</span><span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin=</span><span class="dv">0</span>, <span class="at">ymax=</span>dens))<span class="sc">+</span></span>
<span id="cb181-28"><a href="NHST.html#cb181-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>Z.xbar, <span class="at">size=</span><span class="dv">2</span>, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span></span>
<span id="cb181-29"><a href="NHST.html#cb181-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>low.crit, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)<span class="sc">+</span></span>
<span id="cb181-30"><a href="NHST.html#cb181-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>high.crit, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-97-1.png" width="3000" /></p>
<p>Since here we are using the sampling distribution of the z-score, the null distribution is Normal with mean 0 and standard deviation of 1. Otherwise, all the other calculations of p-values, critical statistic values, etc. is exactly the same, but we use the z-score of the sample mean as a test statistic:</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="NHST.html#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see if z-score of mean score of gages is lower than the low critical z-score, or higher than the high critical z-score.</span></span>
<span id="cb182-2"><a href="NHST.html#cb182-2" aria-hidden="true" tabindex="-1"></a>Z.xbar <span class="sc">&lt;=</span> low.crit <span class="sc">|</span> x.bar <span class="sc">&gt;=</span> high.crit</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="NHST.html#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculte p-value for z-score of mean score of gages by taking the minimum of the lower and upper tails, and multiplying by 2 (to get symmetric 2-tailed p-value)</span></span>
<span id="cb184-2"><a href="NHST.html#cb184-2" aria-hidden="true" tabindex="-1"></a>p.value.low <span class="ot">=</span> <span class="fu">pnorm</span>(Z.xbar, H0.z.mu, H0.z.sd)</span>
<span id="cb184-3"><a href="NHST.html#cb184-3" aria-hidden="true" tabindex="-1"></a>p.value.high <span class="ot">=</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(Z.xbar, H0.z.mu, H0.z.sd)</span>
<span id="cb184-4"><a href="NHST.html#cb184-4" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">min</span>(p.value.low, p.value.high)</span>
<span id="cb184-5"><a href="NHST.html#cb184-5" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>## [1] 0.0006767642</code></pre>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="NHST.html#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare p.value to alpha</span></span>
<span id="cb186-2"><a href="NHST.html#cb186-2" aria-hidden="true" tabindex="-1"></a>p.value <span class="sc">&lt;=</span> alpha</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Note that if we do this z-transformation, and consider a two-tailed test, we can take a number of shortcuts:</p>
<p>Because the z-score distribution is symmetric around 0, the low.crit score is the high.crit score multiplied by negative 1, so we can consider just the absolute critical score. Similarly, we need only consider the absolute value of our calculated z-scores to calculate two-tailed p-values. Moreover, we need not refer to the sampling distribution mean and standard deviation explicitly, since all the <code>*norm</code> functions in R assume that the default is the standard normal (z-score) distribution with mean=0 and sd=1.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="NHST.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for significance by comparing absolute z score to critical z-score</span></span>
<span id="cb188-2"><a href="NHST.html#cb188-2" aria-hidden="true" tabindex="-1"></a>abs.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb188-3"><a href="NHST.html#cb188-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abs</span>(Z.xbar) <span class="sc">&gt;=</span> abs.crit </span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="NHST.html#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate (2-tailed) p-value using absolute z-score</span></span>
<span id="cb190-2"><a href="NHST.html#cb190-2" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(<span class="fu">abs</span>(Z.xbar)))</span>
<span id="cb190-3"><a href="NHST.html#cb190-3" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>## [1] 0.0006767642</code></pre>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="NHST.html#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for significance by comparing p.value to alpha</span></span>
<span id="cb192-2"><a href="NHST.html#cb192-2" aria-hidden="true" tabindex="-1"></a>p.value <span class="sc">&lt;=</span> alpha</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Hopefully, it is clear at this point that these procedures are all doing the same thing, just with slight mathematical transformations that make some things less transparent, but other things more convenient. They all yield the same answer.</p>
</div>
<div id="normal-confidence-intervals-on-the-sample-mean" class="section level3">
<h3>(Normal) Confidence intervals on the sample mean</h3>
<p>Recall that the sampling distribution of the <em>deviation of the sample mean from the population mean</em> – the sampling distribution of the error of our sample mean – is given by:</p>
<p><span class="math inline">\((\bar x^{(n)}-\mu_X) \sim \operatorname{Normal}\left({0, \sigma_{\bar x^{(n)}}}\right)\)</span></p>
<p>It is helpful to calculate this as a z-score:</p>
<p><span class="math inline">\(\frac{\bar x^{(n)}-\mu_X}{\sigma_{\bar x^{(n)}}} \sim \operatorname{Normal}\left({0, 1}\right)\)</span></p>
<p>So the difference between our sample mean, and the population mean, <em>in units of standard errors of the mean</em>, will have a standard Normal distribution.</p>
<p>So if we want to define an interval around our sample mean, such that an interval defined this way will contain the true population mean 95% of the time, we can do so by finding a z-score interval that contains 95% of the z-scores, and then transforming these z-scores back into the units of sample means. One z-score range that will include 95% of the means can be constructed based on the z-score such that 2.5% of the z-scores are smaller than it, and the z-score such that 2.5% are larger than it (thus 95% of z-scores are between them):</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="NHST.html#cb194-1" aria-hidden="true" tabindex="-1"></a>low.z.crit <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="fl">0.025</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb194-2"><a href="NHST.html#cb194-2" aria-hidden="true" tabindex="-1"></a>high.z.crit <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="dv">1</span><span class="fl">-0.025</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb194-3"><a href="NHST.html#cb194-3" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(low.z.crit, high.z.crit)</span></code></pre></div>
<pre><code>## [1] -1.959964  1.959964</code></pre>
<p>We can convert these z-scores back to their original units by multiplying by the standard deviation, and adding the mean (reversing the calculation that yields z scores for specific x values):</p>
<p><span class="math inline">\(x = z*\sigma + \mu\)</span></p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="NHST.html#cb196-1" aria-hidden="true" tabindex="-1"></a>x.bar <span class="ot">=</span> <span class="fu">mean</span>(gages)</span>
<span id="cb196-2"><a href="NHST.html#cb196-2" aria-hidden="true" tabindex="-1"></a>sd.xbar <span class="ot">=</span> H0.sd<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(gages))</span>
<span id="cb196-3"><a href="NHST.html#cb196-3" aria-hidden="true" tabindex="-1"></a>xbar.range <span class="ot">=</span> <span class="fu">c</span>(low.z.crit, high.z.crit)<span class="sc">*</span>sd.xbar <span class="sc">+</span> x.bar</span>
<span id="cb196-4"><a href="NHST.html#cb196-4" aria-hidden="true" tabindex="-1"></a>xbar.range</span></code></pre></div>
<pre><code>## [1] 38.01739 46.78261</code></pre>
<p>Here we called it “sd.xbar” because it is the standard deviation of the sampling distribution of the sample mean, but typically we will just refer to it as the standard error of the mean, or “se.xbar.”</p>
<p>Because the z-score distribution is symmetric, and by convention we chose to define a symmetric confidence interval, the low and high z scores are symmetric, and we typically calculate just the absolute value:</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="NHST.html#cb198-1" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="fl">0.95</span> <span class="co"># desired confidence interval</span></span>
<span id="cb198-2"><a href="NHST.html#cb198-2" aria-hidden="true" tabindex="-1"></a>z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>((<span class="dv">1</span><span class="sc">-</span>q)<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb198-3"><a href="NHST.html#cb198-3" aria-hidden="true" tabindex="-1"></a>xbar.range <span class="ot">=</span> x.bar <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)<span class="sc">*</span>z.crit<span class="sc">*</span>sd.xbar</span></code></pre></div>
<p>In short, we can define the (z-score) confidence interval on the sample mean as:</p>
<p><span class="math display">\[\bar x \pm Z_{\alpha/2}^*\sigma_{\bar x}\]</span></p>
<p>Where <span class="math inline">\(\alpha = 1-q\)</span>, and <span class="math inline">\(q\)</span> is the confidence interval percentile; <span class="math inline">\(Z_{\alpha/2}^*=\)</span> <code>abs(qnorm(alpha/2,0,1))</code>. So for a 99% confidence interval, <span class="math inline">\(q=0.99\)</span>, <span class="math inline">\(\alpha=0.01\)</span>, and <span class="math inline">\(Z_{\alpha/2}^*=\)</span> <code>abs(qnorm(0.01/2,0,1))</code> = 2.5758293.</p>
<div id="relationship-between-confidence-intervals-and-null-hypothesis-tests." class="section level4">
<h4>Relationship between confidence intervals and null hypothesis tests.</h4>
<p>The use of <span class="math inline">\(\alpha\)</span> as the critical value in a null hypothesis test, and as the interim calculation in confidence intervals is no accident.</p>
<p>We declare a (2-tailed) z-test as significant when the p-value is lower than <span class="math inline">\(\alpha\)</span>, in other words, when (the absolute value of) the difference between the sample mean and the null mean, in units of standard errors of the mean, is greater than <span class="math inline">\(Z_{\alpha/2}^*\)</span>:</p>
<p><span class="math inline">\(\lvert\frac{\bar x - \mu_X^{H0}}{\sigma_{\bar x}}\rvert \geq Z_{\alpha/2}^*\)</span></p>
<p>With a bit of algebra, we can show that this means that we declare something as significant when:</p>
<p><span class="math inline">\(\mu_X^{H0} \geq \bar x + \sigma_{\bar x} Z_{\alpha/2}^*\)</span> OR<br />
<span class="math inline">\(\mu_X^{H0} \leq \bar x - \sigma_{\bar x} Z_{\alpha/2}^*\)</span></p>
<p>And as we recall, the limits of a q% confidence interval are given by:</p>
<p><span class="math inline">\(\bar x \pm \sigma_{\bar x} Z_{\alpha/2}^*\)</span></p>
<p>Thus, we see that if the null hypothesis mean does not fall in the <span class="math inline">\((1-\alpha)\)</span> confidence interval, then we can reject that null hypothesis mean with a two-tailed significance test with a Type I error rate of <span class="math inline">\(\alpha\)</span>. So, checking whether the p-value for a null hypothesis z-test is less than <span class="math inline">\(\alpha=0.05\)</span> is equivalent to checking whether the null hypothesis mean falls outside of the 95% confidence interval.</p>
</div>
<div id="special-critical-z-values" class="section level4">
<h4>Special critical Z-values</h4>
<p>Everyone using statistics would benefit from knowing a few special Z-scores, since they make back-of-the-envelope calculations easy when you want to evaluate some results in a talk or a poster.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P(Z \leq -1.96) = P(Z \geq 1.96) = 0.025\)</span>. In other words, 95% of the normal distribution is less than 1.96 standard deviations away from the mean. This means that a 95% confidence interval on a mean is <span class="math inline">\(\bar x \pm 1.96 \sigma_{\bar x}\)</span>. This also means that to pass a two-tailed Z-test with <span class="math inline">\(\alpha = 0.05\)</span>, the sample mean has to be more than 1.96 standard errors away from the null hypothesis mean.</p></li>
<li><p><span class="math inline">\(P(Z \leq -0.6745) = P(Z \geq 0.6745) = 0.25\)</span>. 50% of the normal distribution is less than 0.67 standard deviations away from the mean; the first and third quartiles are the mean plus/minus 0.67 standard deviations. The interquartile range of a normal distribution will be 1.35 standard deviations.</p></li>
<li><p><span class="math inline">\(P(Z \leq -1.645) = P(Z \geq 1.645) = 0.05\)</span>. This defines the 90% confidence intervals, and correspond to the critical Z-value for a one-tailed test with <span class="math inline">\(\alpha = 0.05\)</span>.</p></li>
<li><p><span class="math inline">\(P(Z \leq 1) = P(Z \geq 1) = 0.15866\)</span>. About 16% of a normal distribution is more than 1 standard deviation away from the mean in either direction, meaning that 68.3% of the normal distribution is less than 1 standard deviation away from the mean.</p></li>
</ol>
<p>These numbers can all be easily obtained via <code>pnorm()</code> and <code>qnorm()</code> in R, but often you might benefit from having them in your head.</p>
</div>
<div id="rarity-of-z-tests-and-z--confidence-intervals" class="section level4">
<h4>Rarity of Z-tests and Z- confidence intervals</h4>
<p>R doesn’t have a z-test function built in (although a few libraries offer one). This is because z-tests are so rarely done in practice, because carrying out a z-test requires that we <strong>know the population standard deviation</strong>. Consequently, when we reject the null hypothesis in a z-test, we reject the null of <em>a particular population mean <strong>and</strong> and a particular population standard deviation</em>. This is very rarely what we want, so we use t-tests instead (which assume that we estimate the standard deviation from the sample).</p>
<p>Furthermore, using z-tests to define confidence intervals is even more rare, because when we define a confidence interval, we do not want to assume particular parameters of the population distribution (like its mean, and standard deviation). In the vast majority of cases, we will use the t-distribution, rather than the Normal Z-distribution for our null hypothesis tests and confidence intervals on the mean.</p>
</div>
</div>
<div id="what-are-these-percents-and-probabilities" class="section level3">
<h3>What are these percents and probabilities?</h3>
<p>It is important to consider what these percents and probabilities are. This interpretation of confidence intervals and probabilities will be the same for every single confidence interval and p value we calculate. So we will keep reiterating it.</p>
<div id="so-what-is-a-p-value" class="section level4">
<h4>So what is a p-value?</h4>
<p>We obtained the p value by calculating the probability with the following logic: we calculated the sampling distribution of the test statistic if we were to take many samples (of the same size as ours) from the null hypothesis population, calculate the test statistic on each of those samples, then look at the histogram of those samples. The proportion of those samples that are more extreme than the test-statistic we saw in our <em>actual</em> sample, is the p-value.</p>
<p>Let’s do this explicitly:</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="NHST.html#cb199-1" aria-hidden="true" tabindex="-1"></a>gages <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">39</span>, <span class="dv">44</span>, <span class="dv">38</span>, <span class="dv">40</span>, <span class="dv">51</span>)</span>
<span id="cb199-2"><a href="NHST.html#cb199-2" aria-hidden="true" tabindex="-1"></a>H0.mu <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb199-3"><a href="NHST.html#cb199-3" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb199-4"><a href="NHST.html#cb199-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-5"><a href="NHST.html#cb199-5" aria-hidden="true" tabindex="-1"></a><span class="co"># A function to calculate the z statistic for a sample mean given H0 mean and sd</span></span>
<span id="cb199-6"><a href="NHST.html#cb199-6" aria-hidden="true" tabindex="-1"></a>z.stat <span class="ot">=</span> <span class="cf">function</span>(sample){(<span class="fu">mean</span>(sample)<span class="sc">-</span>H0.mu)<span class="sc">/</span>(H0.sd<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(sample)))}</span>
<span id="cb199-7"><a href="NHST.html#cb199-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-8"><a href="NHST.html#cb199-8" aria-hidden="true" tabindex="-1"></a><span class="co"># our z statistic.</span></span>
<span id="cb199-9"><a href="NHST.html#cb199-9" aria-hidden="true" tabindex="-1"></a>(<span class="at">our.z.stat =</span> <span class="fu">z.stat</span>(gages))</span></code></pre></div>
<pre><code>## [1] -3.398823</code></pre>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="NHST.html#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to sample n data points from the H0 distribution</span></span>
<span id="cb201-2"><a href="NHST.html#cb201-2" aria-hidden="true" tabindex="-1"></a>sample.from.H0 <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">rnorm</span>(n,H0.mu,H0.sd)}</span>
<span id="cb201-3"><a href="NHST.html#cb201-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb201-4"><a href="NHST.html#cb201-4" aria-hidden="true" tabindex="-1"></a><span class="co"># one sample of the same size as ours from the H0 distribution</span></span>
<span id="cb201-5"><a href="NHST.html#cb201-5" aria-hidden="true" tabindex="-1"></a>(<span class="at">one.H0.sample =</span> <span class="fu">sample.from.H0</span>(<span class="fu">length</span>(gages)))</span></code></pre></div>
<pre><code>## [1] 53.05510 50.03932 48.13520 49.41828
## [5] 54.26727</code></pre>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="NHST.html#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the z-statistic for the H0 sample.</span></span>
<span id="cb203-2"><a href="NHST.html#cb203-2" aria-hidden="true" tabindex="-1"></a>(<span class="at">one.H0.z.stat =</span> <span class="fu">z.stat</span>(one.H0.sample))</span></code></pre></div>
<pre><code>## [1] 0.4396273</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="NHST.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sample lots of z statistics from the H0 distribution</span></span>
<span id="cb205-2"><a href="NHST.html#cb205-2" aria-hidden="true" tabindex="-1"></a>many.H0.z.stats <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">z.stat</span>(<span class="fu">sample.from.H0</span>(<span class="fu">length</span>(gages))))</span>
<span id="cb205-3"><a href="NHST.html#cb205-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-4"><a href="NHST.html#cb205-4" aria-hidden="true" tabindex="-1"></a><span class="co"># show a histogram of these H0-sampled z statistics.</span></span>
<span id="cb205-5"><a href="NHST.html#cb205-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">z =</span> many.H0.z.stats, </span>
<span id="cb205-6"><a href="NHST.html#cb205-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">z.vs.ours=</span><span class="fu">ifelse</span>(<span class="fu">abs</span>(many.H0.z.stats)<span class="sc">&gt;=</span><span class="fu">abs</span>(our.z.stat),</span>
<span id="cb205-7"><a href="NHST.html#cb205-7" aria-hidden="true" tabindex="-1"></a>                                   <span class="st">&quot;more extreme&quot;</span>, </span>
<span id="cb205-8"><a href="NHST.html#cb205-8" aria-hidden="true" tabindex="-1"></a>                                   <span class="st">&quot;less extreme&quot;</span>)), </span>
<span id="cb205-9"><a href="NHST.html#cb205-9" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>z, <span class="at">fill=</span>z.vs.ours))<span class="sc">+</span></span>
<span id="cb205-10"><a href="NHST.html#cb205-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>()<span class="sc">+</span></span>
<span id="cb205-11"><a href="NHST.html#cb205-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>our.z.stat, <span class="at">color=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb205-12"><a href="NHST.html#cb205-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-13"><a href="NHST.html#cb205-13" aria-hidden="true" tabindex="-1"></a><span class="co"># our p value calculated by asking what fraction of H0-sampled z-statistics </span></span>
<span id="cb205-14"><a href="NHST.html#cb205-14" aria-hidden="true" tabindex="-1"></a><span class="co"># are larger than ours (in absolute value)</span></span>
<span id="cb205-15"><a href="NHST.html#cb205-15" aria-hidden="true" tabindex="-1"></a>(<span class="at">p.value =</span> <span class="fu">sum</span>(<span class="fu">abs</span>(many.H0.z.stats)<span class="sc">&gt;=</span><span class="fu">abs</span>(our.z.stat))<span class="sc">/</span><span class="fu">length</span>(many.H0.z.stats))</span></code></pre></div>
<pre><code>## [1] 4e-04</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-103-1.png" width="3000" /></p>
<p>Of course, when we do a z-test, we generally do this analytically, rather than numerically as we have here (by literally simulating a bunch of possible samples from the null hypothesis), which eliminates the need for sluggish computation. However, the logic of what we did is the same: we calculated the p-value as the proportion of samples from the null hypothesis that would be at least as extreme as the one we saw.</p>
<p>So what does the p-value mean? It tells us what fraction of null hypothesis samples would be at least as extreme as ours, given our test statistic. It is a calculation based on <span class="math inline">\(P(\mbox{data} \mid \mbox{H0})\)</span>. More generally, it tells us something about how this procedure is expected to behave when applied to the null hypothesis: if this procedure were applied to samples from some null hypothesis model, this is what we expect to see. Similarly, <span class="math inline">\(\alpha\)</span> tells us: if we reject the null based on this significance procedure, we expect the procedure to reject samples from the null hypothesis <span class="math inline">\((100*\alpha)\)</span>% of the time.</p>
</div>
<div id="what-does-the-percent-in-a-confidence-interval-mean" class="section level4">
<h4>What does the “percent” in a confidence interval mean?</h4>
<p>We calculated a confidence interval based on the sampling distribution of the error of the mean from the population mean.</p>
<p>Let’s simulate this procedure by picking some random <em>true</em> population mean, <em>true</em> population sd, and some sample size. Then we simulate a sample from that population, and calculate a confidence interval from that sample.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="NHST.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to get the critical z value for a 100q% interval</span></span>
<span id="cb207-2"><a href="NHST.html#cb207-2" aria-hidden="true" tabindex="-1"></a>z.crit <span class="ot">=</span> <span class="cf">function</span>(q){<span class="fu">abs</span>(<span class="fu">qnorm</span>((<span class="dv">1</span><span class="sc">-</span>q)<span class="sc">/</span><span class="dv">2</span>))}</span>
<span id="cb207-3"><a href="NHST.html#cb207-3" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to calculate the standard error of the mean for a given</span></span>
<span id="cb207-4"><a href="NHST.html#cb207-4" aria-hidden="true" tabindex="-1"></a><span class="co"># sample and null hypothesis sd.</span></span>
<span id="cb207-5"><a href="NHST.html#cb207-5" aria-hidden="true" tabindex="-1"></a>sem <span class="ot">=</span> <span class="cf">function</span>(x,H0.sd){H0.sd<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(x))}</span>
<span id="cb207-6"><a href="NHST.html#cb207-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-7"><a href="NHST.html#cb207-7" aria-hidden="true" tabindex="-1"></a><span class="co"># a function to get the mean and confidence interval (min, max)</span></span>
<span id="cb207-8"><a href="NHST.html#cb207-8" aria-hidden="true" tabindex="-1"></a>get.CI <span class="ot">=</span> <span class="cf">function</span>(x, q, H0.sd){</span>
<span id="cb207-9"><a href="NHST.html#cb207-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(</span>
<span id="cb207-10"><a href="NHST.html#cb207-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">&quot;min&quot;</span><span class="ot">=</span>(<span class="fu">mean</span>(x)<span class="sc">-</span><span class="fu">z.crit</span>(q)<span class="sc">*</span><span class="fu">sem</span>(x,H0.sd)),</span>
<span id="cb207-11"><a href="NHST.html#cb207-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;mean&quot;</span><span class="ot">=</span><span class="fu">mean</span>(x),</span>
<span id="cb207-12"><a href="NHST.html#cb207-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;max&quot;</span><span class="ot">=</span>(<span class="fu">mean</span>(x)<span class="sc">+</span><span class="fu">z.crit</span>(q)<span class="sc">*</span><span class="fu">sem</span>(x,H0.sd)))</span>
<span id="cb207-13"><a href="NHST.html#cb207-13" aria-hidden="true" tabindex="-1"></a>  )}</span>
<span id="cb207-14"><a href="NHST.html#cb207-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-15"><a href="NHST.html#cb207-15" aria-hidden="true" tabindex="-1"></a><span class="co"># a hypothetical true mean.</span></span>
<span id="cb207-16"><a href="NHST.html#cb207-16" aria-hidden="true" tabindex="-1"></a>true.mean <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">5</span>)</span>
<span id="cb207-17"><a href="NHST.html#cb207-17" aria-hidden="true" tabindex="-1"></a><span class="co"># a hypothetical true standard deviation</span></span>
<span id="cb207-18"><a href="NHST.html#cb207-18" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="fu">exp</span>(<span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="fl">0.5</span>))</span>
<span id="cb207-19"><a href="NHST.html#cb207-19" aria-hidden="true" tabindex="-1"></a><span class="co"># a hypothetical sample size:</span></span>
<span id="cb207-20"><a href="NHST.html#cb207-20" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">rgeom</span>(<span class="dv">1</span>,<span class="fl">0.2</span>)<span class="sc">+</span><span class="dv">2</span></span>
<span id="cb207-21"><a href="NHST.html#cb207-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-22"><a href="NHST.html#cb207-22" aria-hidden="true" tabindex="-1"></a><span class="co"># a hypothetical sample of size n from this &quot;true distribution&quot;</span></span>
<span id="cb207-23"><a href="NHST.html#cb207-23" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(n,true.mean,H0.sd)</span>
<span id="cb207-24"><a href="NHST.html#cb207-24" aria-hidden="true" tabindex="-1"></a><span class="co"># the sample mean and 90% confidence interval for this sample</span></span>
<span id="cb207-25"><a href="NHST.html#cb207-25" aria-hidden="true" tabindex="-1"></a>(<span class="at">ci =</span> <span class="fu">get.CI</span>(x, <span class="fl">0.9</span>, H0.sd))</span></code></pre></div>
<pre><code>##      min     mean      max 
## 1.329743 2.259383 3.189023</code></pre>
<p>So this is one such randomly generated sample mean and resulting confidence interval.</p>
<p>Now, we can ask whether the <em>true</em> population mean was contained within that confidence interval:</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="NHST.html#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s define a function to tell us whether the true mean is inside the confidence interval</span></span>
<span id="cb209-2"><a href="NHST.html#cb209-2" aria-hidden="true" tabindex="-1"></a>mean.in.ci <span class="ot">=</span> <span class="cf">function</span>(true.mean, ci){</span>
<span id="cb209-3"><a href="NHST.html#cb209-3" aria-hidden="true" tabindex="-1"></a>  ci[<span class="st">&#39;min&#39;</span>] <span class="sc">&lt;=</span> true.mean <span class="sc">&amp;</span> true.mean <span class="sc">&lt;=</span> ci[<span class="st">&#39;max&#39;</span>]</span>
<span id="cb209-4"><a href="NHST.html#cb209-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb209-5"><a href="NHST.html#cb209-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-6"><a href="NHST.html#cb209-6" aria-hidden="true" tabindex="-1"></a><span class="co"># is the true mean inside the confidence interval? (ignore the vector name &quot;min&quot; carryover)</span></span>
<span id="cb209-7"><a href="NHST.html#cb209-7" aria-hidden="true" tabindex="-1"></a>(<span class="fu">mean.in.ci</span>(true.mean, ci))</span></code></pre></div>
<pre><code>##  min 
## TRUE</code></pre>
<p>So any one confidence interval either includes, or does not include, the true population mean.</p>
<p>So what does the percent in a confidence interval mean? It’s a statement not about the current data, or the current population mean in question. It is a statement about the confidence interval <em>procedure</em>. Specifically, it tells us what fraction of all confidence intervals generated this way, in all experiments, will contain their respective population mean. Let’s simulate this.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="NHST.html#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let&#39;s consider many different true populations, samples from them, and the resulting CI</span></span>
<span id="cb211-2"><a href="NHST.html#cb211-2" aria-hidden="true" tabindex="-1"></a>results <span class="ot">=</span> <span class="fu">data.frame</span>()</span>
<span id="cb211-3"><a href="NHST.html#cb211-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>){</span>
<span id="cb211-4"><a href="NHST.html#cb211-4" aria-hidden="true" tabindex="-1"></a>  true.mean <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">5</span>)</span>
<span id="cb211-5"><a href="NHST.html#cb211-5" aria-hidden="true" tabindex="-1"></a>  H0.sd <span class="ot">=</span> <span class="fu">exp</span>(<span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="fl">0.5</span>))</span>
<span id="cb211-6"><a href="NHST.html#cb211-6" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">rgeom</span>(<span class="dv">1</span>,<span class="fl">0.2</span>)<span class="sc">+</span><span class="dv">2</span></span>
<span id="cb211-7"><a href="NHST.html#cb211-7" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">rnorm</span>(n,true.mean,H0.sd)</span>
<span id="cb211-8"><a href="NHST.html#cb211-8" aria-hidden="true" tabindex="-1"></a>  ci <span class="ot">=</span> <span class="fu">get.CI</span>(x, <span class="fl">0.9</span>, H0.sd)</span>
<span id="cb211-9"><a href="NHST.html#cb211-9" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">=</span> <span class="fu">rbind</span>(results, </span>
<span id="cb211-10"><a href="NHST.html#cb211-10" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">data.frame</span>(<span class="st">&quot;experiment&quot;</span> <span class="ot">=</span> i,</span>
<span id="cb211-11"><a href="NHST.html#cb211-11" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;mean&quot;</span><span class="ot">=</span>ci[<span class="st">&#39;mean&#39;</span>],</span>
<span id="cb211-12"><a href="NHST.html#cb211-12" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;ci.min&quot;</span><span class="ot">=</span>ci[<span class="st">&#39;min&#39;</span>],</span>
<span id="cb211-13"><a href="NHST.html#cb211-13" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;ci.max&quot;</span><span class="ot">=</span>ci[<span class="st">&#39;max&#39;</span>],</span>
<span id="cb211-14"><a href="NHST.html#cb211-14" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;true.mean&quot;</span><span class="ot">=</span>true.mean,</span>
<span id="cb211-15"><a href="NHST.html#cb211-15" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;mean.in.ci&quot;</span><span class="ot">=</span><span class="fu">mean.in.ci</span>(true.mean, ci)))</span>
<span id="cb211-16"><a href="NHST.html#cb211-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb211-17"><a href="NHST.html#cb211-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-18"><a href="NHST.html#cb211-18" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(results, <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">as.factor</span>(experiment), </span>
<span id="cb211-19"><a href="NHST.html#cb211-19" aria-hidden="true" tabindex="-1"></a>                    <span class="at">y=</span>mean, </span>
<span id="cb211-20"><a href="NHST.html#cb211-20" aria-hidden="true" tabindex="-1"></a>                    <span class="at">ymin=</span>ci.min, </span>
<span id="cb211-21"><a href="NHST.html#cb211-21" aria-hidden="true" tabindex="-1"></a>                    <span class="at">ymax=</span>ci.max, </span>
<span id="cb211-22"><a href="NHST.html#cb211-22" aria-hidden="true" tabindex="-1"></a>                    <span class="at">color=</span>mean.in.ci))<span class="sc">+</span></span>
<span id="cb211-23"><a href="NHST.html#cb211-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_pointrange</span>()<span class="sc">+</span></span>
<span id="cb211-24"><a href="NHST.html#cb211-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y=</span>true.mean), <span class="at">size=</span><span class="dv">2</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)<span class="sc">+</span></span>
<span id="cb211-25"><a href="NHST.html#cb211-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-106-1.png" width="1800" /></p>
<p>Here, each horizontal line represents a particular experiment, with a particular true population mean (black dot), some H0 standard deviation, and some random sample. That random sample is used to define a mean and a 90% confidence interval (point+range). If the confidence interval contains the true mean, it is blue, otherwise it is red.</p>
<p>So what fraction of these intervals contained the true population mean?</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="NHST.html#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fraction of confidence intervals that include the true mean</span></span>
<span id="cb212-2"><a href="NHST.html#cb212-2" aria-hidden="true" tabindex="-1"></a>(<span class="fu">sum</span>(results<span class="sc">$</span>mean.in.ci)<span class="sc">/</span><span class="fu">nrow</span>(results))</span></code></pre></div>
<pre><code>## [1] 0.89</code></pre>
<p>Note that the result of “how many of our 100 sampled intervals include their true mean” will be subject to sampling variation (binomial with n=100, p=q=0.9, here). If we increase the number of sampled experiments and intervals, we will be less likely to deviate much from 0.9.</p>
<p>So, the percent in a confidence interval describes the probability that an interval constructed using this method will include the corresponding value (provided the distribution assumptions are met).</p>
<p>All frequentist probabilities and percents have this sort of interpretation: they are statements about the procedure – how often will the procedure reject a sample from the null, how often will a confidence interval calculated in this way contain the true mean, etc.</p>

</div>
</div>
</div>
<div id="nhst-theory-normal" class="section level2">
<h2>Null hypothesis significance testing</h2>
<p>In <a href="#nhst-basics-normal">statistics via the Normal</a> we covered the basic logic and application of the <a href="#nhst-sampling-distribution.html">sampling distribution of the sample mean</a> to the problem of testing a null hypothesis about the population.</p>
<p>Null hypothesis testing follows this procedure:</p>
<ol style="list-style-type: decimal">
<li>We have some structure we are interested in (the “effect”).<br />
</li>
<li>We define a “statistic” to measure this structure.<br />
</li>
<li>We define a “null” model of the data: a statistical model that generates data like ours, but <em>lacking</em> the effect we are interested in.<br />
</li>
<li>We figure out the sampling distribution of our statistic under the null hypothesis.<br />
</li>
<li>We compare the statistic value from our data, to its null hypothesis sampling distribution, to see if our statistic is sufficiently extreme under the null, for us to say that we “reject the null.”</li>
</ol>
<div id="type-1-error-rate-alpha-alpha" class="section level3">
<h3>Type 1 error rate: alpha (<span class="math inline">\(\alpha\)</span>)</h3>
<p>To be more specific, in step 5 we calculate the <em>probability that a statistic at least as extreme as ours would be obtained from samples from the null hypothesis</em> – we call this the <strong>p-value</strong>. We decide on a <em>significance level</em>, usually called <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>): this corresponds to the largest p-value we are willing to declare <strong>significant</strong> (and thus reject the null). Consequently, the chosen alpha value (typically 0.05), corresponds to the probability that we would <em>reject the null hypothesis for a sample from the null hypothesis</em>. Thus, the alpha value corresponds to the rate at which we are willing to falsely reject the null hypothesis (reject it, when it is true); This is known as the rate of <strong>Type I error</strong>.</p>
<p>We can get a sense for this via simulation. We will use the z-test statistic comparing a <em>sample from the null</em> to the null mean (and standard error). We will calculate its p-value, and see if it would be rejected. Since all of these samples are, by definition, sampled from the null, any sample which we declare significantly different form the null is a false rejection of the null: a type 1 error.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="NHST.html#cb214-1" aria-hidden="true" tabindex="-1"></a>H0.mean <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb214-2"><a href="NHST.html#cb214-2" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="dv">15</span></span>
<span id="cb214-3"><a href="NHST.html#cb214-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb214-4"><a href="NHST.html#cb214-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-5"><a href="NHST.html#cb214-5" aria-hidden="true" tabindex="-1"></a>z.stat <span class="ot">=</span> <span class="cf">function</span>(x){(<span class="fu">mean</span>(x)<span class="sc">-</span>H0.mean)<span class="sc">/</span>(H0.sd<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(x)))}</span>
<span id="cb214-6"><a href="NHST.html#cb214-6" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="cf">function</span>(z){<span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(<span class="sc">-</span><span class="fu">abs</span>(z))}</span>
<span id="cb214-7"><a href="NHST.html#cb214-7" aria-hidden="true" tabindex="-1"></a>is.significant <span class="ot">=</span> <span class="cf">function</span>(p.val, alpha){p.val <span class="sc">&lt;=</span> alpha}</span>
<span id="cb214-8"><a href="NHST.html#cb214-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-9"><a href="NHST.html#cb214-9" aria-hidden="true" tabindex="-1"></a>sample.from.null <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">rnorm</span>(n,H0.mean,H0.sd)}</span>
<span id="cb214-10"><a href="NHST.html#cb214-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-11"><a href="NHST.html#cb214-11" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">zs =</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">z.stat</span>(<span class="fu">sample.from.null</span>(<span class="dv">10</span>))))</span>
<span id="cb214-12"><a href="NHST.html#cb214-12" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>p.value <span class="ot">=</span> <span class="fu">p.value</span>(df<span class="sc">$</span>zs)</span>
<span id="cb214-13"><a href="NHST.html#cb214-13" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>significant <span class="ot">=</span> <span class="fu">is.significant</span>(df<span class="sc">$</span>p.value, alpha)</span>
<span id="cb214-14"><a href="NHST.html#cb214-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-15"><a href="NHST.html#cb214-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb214-16"><a href="NHST.html#cb214-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>zs, <span class="at">fill=</span>significant))<span class="sc">+</span><span class="fu">geom_histogram</span>()</span>
<span id="cb214-17"><a href="NHST.html#cb214-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-18"><a href="NHST.html#cb214-18" aria-hidden="true" tabindex="-1"></a>(<span class="at">type.I.error.rate =</span> <span class="fu">sum</span>(df<span class="sc">$</span>significant)<span class="sc">/</span><span class="fu">nrow</span>(df))</span></code></pre></div>
<pre><code>## [1] 0.054</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-108-1.png" width="3000" /></p>
<p>Of course, nothing about this simulation should reveal anything new, but perhaps it illustrates this basic point adequately well.</p>
</div>
<div id="the-alternate-model" class="section level3">
<h3>The “alternate model”</h3>
<p>So far, we have only considered one statistical model: the null model with no effect. This is sufficient to obtain a p-value and test the null hypothesis. However, this only tells us the probability of rejecting (or not) the null hypothesis given data from the null hypothesis. It does not tell us what might happen if the null hypothesis is false.</p>
<p>To calculate the probability of rejecting the null hypothesis when the null hypothesis is <em>false</em> (called <strong>power</strong>) we need a statistical model of the data in the case of a false null. This is the <em>alternate hypothesis</em> model. From this we can calculate power, as well as the <strong>Type II error</strong> rate (the probability of <em>not rejecting the null hypothesis, when it is indeed false</em>).</p>
<p>To set up an alternate model, we will simply set up something like the null model, but with some key difference – an effect size. For simplicity, let’s say that our alternate model is a normal distribution with the same standard deviation, and with a mean that is 8 points higher than the null mean:</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="NHST.html#cb216-1" aria-hidden="true" tabindex="-1"></a>H0.mean <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb216-2"><a href="NHST.html#cb216-2" aria-hidden="true" tabindex="-1"></a>H0.sd <span class="ot">=</span> <span class="dv">15</span></span>
<span id="cb216-3"><a href="NHST.html#cb216-3" aria-hidden="true" tabindex="-1"></a>H1.mean <span class="ot">=</span> <span class="dv">108</span></span>
<span id="cb216-4"><a href="NHST.html#cb216-4" aria-hidden="true" tabindex="-1"></a>H1.sd <span class="ot">=</span> <span class="dv">15</span></span>
<span id="cb216-5"><a href="NHST.html#cb216-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb216-6"><a href="NHST.html#cb216-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-7"><a href="NHST.html#cb216-7" aria-hidden="true" tabindex="-1"></a><span class="co"># we will still calculate the z score, and p-value *relative to the null hypothesis*!</span></span>
<span id="cb216-8"><a href="NHST.html#cb216-8" aria-hidden="true" tabindex="-1"></a>z.stat <span class="ot">=</span> <span class="cf">function</span>(x){(<span class="fu">mean</span>(x)<span class="sc">-</span>H0.mean)<span class="sc">/</span>(H0.sd<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(x)))}</span>
<span id="cb216-9"><a href="NHST.html#cb216-9" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="cf">function</span>(z){<span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(<span class="sc">-</span><span class="fu">abs</span>(z))}</span>
<span id="cb216-10"><a href="NHST.html#cb216-10" aria-hidden="true" tabindex="-1"></a>is.significant <span class="ot">=</span> <span class="cf">function</span>(p.val, alpha){p.val <span class="sc">&lt;=</span> alpha}</span>
<span id="cb216-11"><a href="NHST.html#cb216-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-12"><a href="NHST.html#cb216-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: here we are using the H1 mean and sd!</span></span>
<span id="cb216-13"><a href="NHST.html#cb216-13" aria-hidden="true" tabindex="-1"></a>sample.from.alt <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">rnorm</span>(n,H1.mean,H1.sd)}</span>
<span id="cb216-14"><a href="NHST.html#cb216-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-15"><a href="NHST.html#cb216-15" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">zs =</span> <span class="fu">replicate</span>(<span class="dv">1000</span>, <span class="fu">z.stat</span>(<span class="fu">sample.from.alt</span>(<span class="dv">10</span>))))</span>
<span id="cb216-16"><a href="NHST.html#cb216-16" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>p.value <span class="ot">=</span> <span class="fu">p.value</span>(df<span class="sc">$</span>zs)</span>
<span id="cb216-17"><a href="NHST.html#cb216-17" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>significant <span class="ot">=</span> <span class="fu">is.significant</span>(df<span class="sc">$</span>p.value, alpha)</span>
<span id="cb216-18"><a href="NHST.html#cb216-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-19"><a href="NHST.html#cb216-19" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb216-20"><a href="NHST.html#cb216-20" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>zs, <span class="at">fill=</span>significant))<span class="sc">+</span><span class="fu">geom_histogram</span>()</span>
<span id="cb216-21"><a href="NHST.html#cb216-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-22"><a href="NHST.html#cb216-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Power: the probability that we will reject a sample from the alternate model.</span></span>
<span id="cb216-23"><a href="NHST.html#cb216-23" aria-hidden="true" tabindex="-1"></a>(<span class="at">power=</span> <span class="fu">sum</span>(df<span class="sc">$</span>significant)<span class="sc">/</span><span class="fu">nrow</span>(df))</span></code></pre></div>
<pre><code>## [1] 0.388</code></pre>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="NHST.html#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Type II error rate: the probability that we *fail* to reject the null for a sample from the alternate model</span></span>
<span id="cb218-2"><a href="NHST.html#cb218-2" aria-hidden="true" tabindex="-1"></a>(<span class="at">type.II.error.rate =</span> (<span class="dv">1</span><span class="sc">-</span>power))</span></code></pre></div>
<pre><code>## [1] 0.612</code></pre>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="NHST.html#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="co"># or alternatively:</span></span>
<span id="cb220-2"><a href="NHST.html#cb220-2" aria-hidden="true" tabindex="-1"></a>(<span class="at">type.II.error.rate =</span> <span class="fu">sum</span>(df<span class="sc">$</span>significant<span class="sc">==</span><span class="cn">FALSE</span>)<span class="sc">/</span><span class="fu">nrow</span>(df))</span></code></pre></div>
<pre><code>## [1] 0.612</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-109-1.png" width="3000" /></p>
</div>
<div id="effect-size" class="section level3">
<h3>Effect size</h3>
<p>The effect size is the magnitude of the deviation of the alternate model from the null model. While we can talk about the effect size in our case as the raw difference in means (100 for null, 108 for alternate, so 8 points), it is generally more convenient to talk about the effect in <em>standardized units</em>. This way, we get similar effect size estimates regardless of the units we are considering (e.g., centimeters, inches, etc).</p>
<p>For comparing differences in means, we generally use “Cohen’s d”: the difference in means in units of standard deviation. (alternate mean minus null mean) divided by standard deviation:</p>
<p><span class="math display">\[d&#39; = (\mu_X^{H_1} - \mu_X^{H_0})/\sigma_X\]</span></p>
<p>In our case, we said that the H1 mean was 108, the H0 mean was 100, and the standard deviation was 15, consequently the effect size is (108-100)/15 = 8/15.</p>
</div>
<div id="calculating-power-from-effect-size" class="section level3">
<h3>Calculating power from effect size</h3>
<p>With this definition, we can calculate power (using a normal z-test), simply by knowing the size of the sample, and the size of the true effect size (Cohen’s d). The power, or the probability that we will reject the null hypothesis, is the probability that a z-statistic obtained for a sample from the null hypothesis will exceed the critical z value.</p>
<p><span class="math inline">\(P(\mbox{significant} | H_1) = P(\lvert z^{H_1} \rvert \geq \lvert z^*_{\alpha/2} \rvert)\)</span></p>
<p>We must now go on a somewhat long-winded, algebraic exercise to calculate the sampling distribution of the z-statistic (relative to the null hypothesis) for samples of size n from the alternate hypothesis.</p>
<p><span class="math inline">\(\bar x_{(n)} \mid H_1 \sim \operatorname{Normal}(\mu_X^{H_1}, \sigma_X^{H_1}/\sqrt{n})\)</span></p>
<p>We know that:</p>
<p><span class="math inline">\(\sigma_X = \sigma_X^{H_1} = \sigma_X^{H_0}\)</span> (by virtue of the assumption that the alternate model has the same standard deviation as the null model!), and</p>
<p><span class="math inline">\(\mu_X^{H_1} = \mu_X^{H_0} + d*\sigma_X\)</span> (this is what the effect size – Cohen’s d – tells us). Consequently:</p>
<p><span class="math inline">\(\bar x_{(n)} \mid H_1 \sim \operatorname{Normal}(\mu_X^{H_0} + d*\sigma_X, \sigma_X/\sqrt{n})\)</span></p>
<p>If we calculate the z-score of a sample mean from the alternate model, relative to the sampling distribution of the sample mean <em>from the null model</em> (as we do when we do significance testing), we get (by virtue of our rules about how to linearly transform normally distributed variables):</p>
<p><span class="math inline">\(z_{\bar x_{(n)}}^{H_1} = \frac{\bar x_{(n)}-\mu_X^{H_0}}{\sigma_X/\sqrt{n}} \mid H_1 \sim \operatorname{Normal}(d*\sqrt{n}, 1)\)</span></p>
<p>We can then compare this to the critical z value.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="NHST.html#cb222-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb222-2"><a href="NHST.html#cb222-2" aria-hidden="true" tabindex="-1"></a>z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb222-3"><a href="NHST.html#cb222-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb222-4"><a href="NHST.html#cb222-4" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="dv">8</span><span class="sc">/</span><span class="dv">15</span> <span class="co"># the effect size we built into the alternate model in the previous section</span></span>
<span id="cb222-5"><a href="NHST.html#cb222-5" aria-hidden="true" tabindex="-1"></a>(<span class="at">p.reject.H0.low =</span> <span class="fu">pnorm</span>(<span class="sc">-</span>z.crit, d<span class="sc">*</span><span class="fu">sqrt</span>(n),<span class="dv">1</span>))    <span class="co"># probability we would reject on the low end</span></span></code></pre></div>
<pre><code>## [1] 0.000132912</code></pre>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="NHST.html#cb224-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">p.reject.H0.high =</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(z.crit, d<span class="sc">*</span><span class="fu">sqrt</span>(n), <span class="dv">1</span>)) <span class="co"># probability we would reject on the high end.</span></span></code></pre></div>
<pre><code>## [1] 0.3922668</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="NHST.html#cb226-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">p.reject.H0 =</span> p.reject.H0.low <span class="sc">+</span> p.reject.H0.high) <span class="co"># this is the power</span></span></code></pre></div>
<pre><code>## [1] 0.3923997</code></pre>
<p>Notice that in this case (with some considerable effect size, and the standard deviation of the alternate equal to the standard deviation of the null model), there is a negligible probability that we would reject the null for alternate model samples on the other side of the null (in this case, rejecting alternate samples for being too low). Consequently, we can often ignore that lower tail, and simply calculate power from the tail that the effect size is on (by using the absolute value of the effect size).</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="NHST.html#cb228-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">p.reject.H0.high =</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(<span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>)), <span class="fu">abs</span>(d)<span class="sc">*</span><span class="fu">sqrt</span>(n), <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.3922668</code></pre>
</div>
<div id="visualizing-alpha-and-power" class="section level3">
<h3>Visualizing alpha and power</h3>
<p>Our example so far can be shown in one graph (with a bit of ggplot tinkering):</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="NHST.html#cb230-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb230-2"><a href="NHST.html#cb230-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb230-3"><a href="NHST.html#cb230-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="dv">8</span><span class="sc">/</span><span class="dv">15</span></span>
<span id="cb230-4"><a href="NHST.html#cb230-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-5"><a href="NHST.html#cb230-5" aria-hidden="true" tabindex="-1"></a>z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb230-6"><a href="NHST.html#cb230-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">8</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb230-7"><a href="NHST.html#cb230-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb230-8"><a href="NHST.html#cb230-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z),</span>
<span id="cb230-9"><a href="NHST.html#cb230-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H0 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H0 sig. low&quot;</span>, <span class="st">&quot;H0 not sig&quot;</span>)),</span>
<span id="cb230-10"><a href="NHST.html#cb230-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H0&quot;</span>),</span>
<span id="cb230-11"><a href="NHST.html#cb230-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb230-12"><a href="NHST.html#cb230-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z,d<span class="sc">*</span><span class="fu">sqrt</span>(n),<span class="dv">1</span>),</span>
<span id="cb230-13"><a href="NHST.html#cb230-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H1 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H1 sig. low&quot;</span>, <span class="st">&quot;H1 not sig&quot;</span>)),</span>
<span id="cb230-14"><a href="NHST.html#cb230-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H1&quot;</span>))</span>
<span id="cb230-15"><a href="NHST.html#cb230-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-16"><a href="NHST.html#cb230-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H0&quot;</span>), </span>
<span id="cb230-17"><a href="NHST.html#cb230-17" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>z, <span class="at">y=</span>p, <span class="at">fill=</span>reject, <span class="at">color=</span>distribution))<span class="sc">+</span></span>
<span id="cb230-18"><a href="NHST.html#cb230-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb230-19"><a href="NHST.html#cb230-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data =</span> <span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H1&quot;</span>), <span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb230-20"><a href="NHST.html#cb230-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span>z.crit)<span class="sc">+</span></span>
<span id="cb230-21"><a href="NHST.html#cb230-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;#008888&quot;</span>, <span class="st">&quot;#008888&quot;</span>))<span class="sc">+</span></span>
<span id="cb230-22"><a href="NHST.html#cb230-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;#880000&quot;</span>, <span class="st">&quot;#008800&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-112-1.png" width="3000" /></p>
<p>Here, the distribution outlined in red is the sampling distribution of z scores from the null hypothesis; the distribution outlined in green is the sampling distribution of z scores (relative to the null sampling distribution) sampled from the alternate hypothesis. The red area corresponds to the probability of a type I error (alpha): rejecting a sample from the null hypothesis. The grey are corresponds to the probability of correctly failing to reject the null (for a sample from the null). The yellow area is the Type II error (beta): the probability of incorrectly failing to reject the null (for a sample from the alternate), and the teal area is the power – the probability of correctly rejecting the null (for a sample from the alternate).</p>
</div>
<div id="how-power-changes." class="section level3">
<h3>How power changes.</h3>
<p>Here we will considering how changes to effect size (<span class="math inline">\(d\)</span>), sample size (<span class="math inline">\(n\)</span>), and alpha (<span class="math inline">\(\alpha\)</span>) influence power.</p>
<p>First, let’s define a few functions that will be helpful to us.</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="NHST.html#cb231-1" aria-hidden="true" tabindex="-1"></a>getPower <span class="ot">=</span> <span class="cf">function</span>(alpha, n, d){</span>
<span id="cb231-2"><a href="NHST.html#cb231-2" aria-hidden="true" tabindex="-1"></a>  z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb231-3"><a href="NHST.html#cb231-3" aria-hidden="true" tabindex="-1"></a>  p.reject.H0.low <span class="ot">=</span> <span class="fu">pnorm</span>(<span class="sc">-</span>z.crit, d<span class="sc">*</span><span class="fu">sqrt</span>(n),<span class="dv">1</span>)</span>
<span id="cb231-4"><a href="NHST.html#cb231-4" aria-hidden="true" tabindex="-1"></a>  p.reject.H0.high <span class="ot">=</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(z.crit, d<span class="sc">*</span><span class="fu">sqrt</span>(n), <span class="dv">1</span>)</span>
<span id="cb231-5"><a href="NHST.html#cb231-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(p.reject.H0.low <span class="sc">+</span> p.reject.H0.high)</span>
<span id="cb231-6"><a href="NHST.html#cb231-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb231-7"><a href="NHST.html#cb231-7" aria-hidden="true" tabindex="-1"></a>showAreas <span class="ot">=</span> <span class="cf">function</span>(alpha, n, d){</span>
<span id="cb231-8"><a href="NHST.html#cb231-8" aria-hidden="true" tabindex="-1"></a>  z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb231-9"><a href="NHST.html#cb231-9" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">8</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb231-10"><a href="NHST.html#cb231-10" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb231-11"><a href="NHST.html#cb231-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">p=</span><span class="fu">dnorm</span>(z),</span>
<span id="cb231-12"><a href="NHST.html#cb231-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H0 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H0 sig. low&quot;</span>, <span class="st">&quot;H0 not sig&quot;</span>)),</span>
<span id="cb231-13"><a href="NHST.html#cb231-13" aria-hidden="true" tabindex="-1"></a>                        <span class="at">distribution=</span><span class="st">&quot;H0&quot;</span>),</span>
<span id="cb231-14"><a href="NHST.html#cb231-14" aria-hidden="true" tabindex="-1"></a>             <span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb231-15"><a href="NHST.html#cb231-15" aria-hidden="true" tabindex="-1"></a>                        <span class="at">p=</span><span class="fu">dnorm</span>(z,d<span class="sc">*</span><span class="fu">sqrt</span>(n),<span class="dv">1</span>),</span>
<span id="cb231-16"><a href="NHST.html#cb231-16" aria-hidden="true" tabindex="-1"></a>                        <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H1 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H1 sig. low&quot;</span>, <span class="st">&quot;H1 not sig&quot;</span>)),</span>
<span id="cb231-17"><a href="NHST.html#cb231-17" aria-hidden="true" tabindex="-1"></a>                        <span class="at">distribution=</span><span class="st">&quot;H1&quot;</span>))</span>
<span id="cb231-18"><a href="NHST.html#cb231-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb231-19"><a href="NHST.html#cb231-19" aria-hidden="true" tabindex="-1"></a>  g <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H0&quot;</span>), </span>
<span id="cb231-20"><a href="NHST.html#cb231-20" aria-hidden="true" tabindex="-1"></a>         <span class="fu">aes</span>(<span class="at">x=</span>z, <span class="at">y=</span>p, <span class="at">fill=</span>reject, <span class="at">color=</span>distribution))<span class="sc">+</span></span>
<span id="cb231-21"><a href="NHST.html#cb231-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_area</span>(<span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb231-22"><a href="NHST.html#cb231-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_area</span>(<span class="at">data =</span> <span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H1&quot;</span>), <span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb231-23"><a href="NHST.html#cb231-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> z.crit)<span class="sc">+</span></span>
<span id="cb231-24"><a href="NHST.html#cb231-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;#008888&quot;</span>, <span class="st">&quot;#008888&quot;</span>))<span class="sc">+</span></span>
<span id="cb231-25"><a href="NHST.html#cb231-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;#880000&quot;</span>, <span class="st">&quot;#008800&quot;</span>))<span class="sc">+</span></span>
<span id="cb231-26"><a href="NHST.html#cb231-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggtitle</span>(<span class="fu">sprintf</span>(<span class="st">&quot;alpha = %0.2f  power = %0.2f&quot;</span>, </span>
<span id="cb231-27"><a href="NHST.html#cb231-27" aria-hidden="true" tabindex="-1"></a>                    alpha, </span>
<span id="cb231-28"><a href="NHST.html#cb231-28" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">getPower</span>(alpha,n,d)))<span class="sc">+</span></span>
<span id="cb231-29"><a href="NHST.html#cb231-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>)</span>
<span id="cb231-30"><a href="NHST.html#cb231-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(g)</span>
<span id="cb231-31"><a href="NHST.html#cb231-31" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div id="changing-alpha-alpha" class="section level4">
<h4>Changing alpha (<span class="math inline">\(\alpha\)</span>)</h4>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="NHST.html#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb232-2"><a href="NHST.html#cb232-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb232-3"><a href="NHST.html#cb232-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fl">0.5</span></span>
<span id="cb232-4"><a href="NHST.html#cb232-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>)</span>
<span id="cb232-5"><a href="NHST.html#cb232-5" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha[<span class="dv">1</span>], n, d)</span>
<span id="cb232-6"><a href="NHST.html#cb232-6" aria-hidden="true" tabindex="-1"></a>g2 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha[<span class="dv">2</span>], n, d)</span>
<span id="cb232-7"><a href="NHST.html#cb232-7" aria-hidden="true" tabindex="-1"></a>g3 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha[<span class="dv">3</span>], n, d)</span>
<span id="cb232-8"><a href="NHST.html#cb232-8" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(g1,g2,g3,<span class="at">ncol=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-114-1.png" width="3600" /></p>
<p>So, if we are willing to increase our Type I error rate, we can increase our power (by virtue of rejecting more of everything). This is not how we want to increase power, since our goal is not to trade one kind of error for another. Clearly, we don’t just want to move our cutoff, but want to further separate the distributions.</p>
</div>
<div id="changing-effect-size-d" class="section level4">
<h4>Changing effect size (<span class="math inline">\(d\)</span>)</h4>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="NHST.html#cb233-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb233-2"><a href="NHST.html#cb233-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb233-3"><a href="NHST.html#cb233-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>)</span>
<span id="cb233-4"><a href="NHST.html#cb233-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb233-5"><a href="NHST.html#cb233-5" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha, n, d[<span class="dv">1</span>])</span>
<span id="cb233-6"><a href="NHST.html#cb233-6" aria-hidden="true" tabindex="-1"></a>g2 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha, n, d[<span class="dv">2</span>])</span>
<span id="cb233-7"><a href="NHST.html#cb233-7" aria-hidden="true" tabindex="-1"></a>g3 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha, n, d[<span class="dv">3</span>])</span>
<span id="cb233-8"><a href="NHST.html#cb233-8" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(g1,g2,g3,<span class="at">ncol=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-115-1.png" width="3600" /></p>
<p>Let’s say we can somehow increase our effect size (perhaps by finding better, less noisy measurements?, or adopting a stronger manipulation?). If this happens, we effectively increase the separation between the null and alternate distributions, and increase power without lowering alpha or changing the sample size! In practice, it’s often tricky to increase the effect size though, so while we would like to do that, it’s usually not in our power.</p>
</div>
<div id="changing-sample-size-n" class="section level4">
<h4>Changing sample size (<span class="math inline">\(n\)</span>)</h4>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="NHST.html#cb234-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb234-2"><a href="NHST.html#cb234-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>)</span>
<span id="cb234-3"><a href="NHST.html#cb234-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fl">0.5</span></span>
<span id="cb234-4"><a href="NHST.html#cb234-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb234-5"><a href="NHST.html#cb234-5" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha, n[<span class="dv">1</span>], d)</span>
<span id="cb234-6"><a href="NHST.html#cb234-6" aria-hidden="true" tabindex="-1"></a>g2 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha, n[<span class="dv">2</span>], d)</span>
<span id="cb234-7"><a href="NHST.html#cb234-7" aria-hidden="true" tabindex="-1"></a>g3 <span class="ot">=</span> <span class="fu">showAreas</span>(alpha, n[<span class="dv">3</span>], d)</span>
<span id="cb234-8"><a href="NHST.html#cb234-8" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(g1,g2,g3,<span class="at">ncol=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-116-1.png" width="3600" /></p>
<p>In practice, the easiest way to increase power is to increase the sample size. This effectively <em>also</em> separates the two distributions further, because the distance between the two sampling distributions of z-scores is <span class="math inline">\(d*\sqrt{n}\)</span>. So we can get an effective separation that scales with the square root of the sample size.</p>
</div>
</div>
<div id="calculating-n-for-a-desired-level-of-power." class="section level3">
<h3>Calculating n for a desired level of power.</h3>
<p>If we have a particular effect size, we can calculate the sample size required to achieve a particular level of power. (Here, we are using the simplified, one-tail power, which works if our assumption of equal variance in null and alternate is correct, and the effect size is not zero.)</p>
<p><code>power = 1-pnorm(abs(qnorm(alpha/2)), abs(d)*sqrt(n), 1))</code></p>
<p>With algebra, we get:</p>
<p><code>pnorm(abs(qnorm(alpha/2)), abs(d)*sqrt(n), 1)) = 1-power</code></p>
<p>Since the quantile function (<code>qnorm</code>) is the inverse of the cumulative distribution (<code>pnorm</code>)…</p>
<p><code>abs(qnorm(alpha/2)) = qnorm(1-power, abs(d)*sqrt(n), 1)</code></p>
<p>Since the normal is invariant to shifts in the mean…</p>
<p><code>abs(qnorm(alpha/2)) = qnorm(1-power) + abs(d)*sqrt(n)</code></p>
<p>We also know that the quantiles of the standard normal are symmetric around 0, so we can get rid of an absolute value…</p>
<p><code>qnorm(1-alpha/2) - qnorm(1-power)= abs(d)*sqrt(n)</code></p>
<p><code>((qnorm(1-alpha/2)-qnorm(1-power))/abs(d))^2=n</code></p>
<p>So, for a particular level of power that we might want, we can estimate the required sample size as:</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="NHST.html#cb235-1" aria-hidden="true" tabindex="-1"></a>n.for.power <span class="ot">=</span> <span class="cf">function</span>(power,alpha,d){</span>
<span id="cb235-2"><a href="NHST.html#cb235-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(((<span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">-</span><span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>power))<span class="sc">/</span><span class="fu">abs</span>(d))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb235-3"><a href="NHST.html#cb235-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb235-4"><a href="NHST.html#cb235-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb235-5"><a href="NHST.html#cb235-5" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fl">0.3</span></span>
<span id="cb235-6"><a href="NHST.html#cb235-6" aria-hidden="true" tabindex="-1"></a>power <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.05</span>,<span class="fl">0.95</span>,<span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb235-7"><a href="NHST.html#cb235-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>()</span>
<span id="cb235-8"><a href="NHST.html#cb235-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(power)){</span>
<span id="cb235-9"><a href="NHST.html#cb235-9" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">=</span> <span class="fu">rbind</span>(df, </span>
<span id="cb235-10"><a href="NHST.html#cb235-10" aria-hidden="true" tabindex="-1"></a>             <span class="fu">data.frame</span>(<span class="at">power=</span>power[i],</span>
<span id="cb235-11"><a href="NHST.html#cb235-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">d=</span>d,</span>
<span id="cb235-12"><a href="NHST.html#cb235-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">alpha=</span>alpha,</span>
<span id="cb235-13"><a href="NHST.html#cb235-13" aria-hidden="true" tabindex="-1"></a>                        <span class="at">required.n=</span><span class="fu">n.for.power</span>(power[i], alpha, d)))</span>
<span id="cb235-14"><a href="NHST.html#cb235-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb235-15"><a href="NHST.html#cb235-15" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>##    power   d alpha required.n
## 1   0.05 0.3  0.05   1.103273
## 2   0.10 0.3  0.05   5.113816
## 3   0.15 0.3  0.05   9.476764
## 4   0.20 0.3  0.05  13.896561
## 5   0.25 0.3  0.05  18.360489
## 6   0.30 0.3  0.05  22.898250
## 7   0.35 0.3  0.05  27.550025
## 8   0.40 0.3  0.05  32.361569
## 9   0.45 0.3  0.05  37.385180
## 10  0.50 0.3  0.05  42.682876
## 11  0.55 0.3  0.05  48.331478
## 12  0.60 0.3  0.05  54.430511
## 13  0.65 0.3  0.05  61.115102
## 14  0.70 0.3  0.05  68.578522
## 15  0.75 0.3  0.05  77.114961
## 16  0.80 0.3  0.05  87.209775
## 17  0.85 0.3  0.05  99.759969
## 18  0.90 0.3  0.05 116.749145
## 19  0.95 0.3  0.05 144.385667</code></pre>
<p>The noteworthy thing here is that achieving a level of power that folks conventionally recommend (0.8), requires a very large sample size for common, modest (d=0.3) effect sizes.</p>
<p>In practice, when you want to calculate power, I recommend using the <code>pwr</code> package in R, rather than undertaking this manual calculation (especially because there will not be an easy analytical solution as most tests rely on distributions whose shape varies with sample size.)</p>
</div>
<div id="sign-and-magnitude-errors." class="section level3">
<h3>Sign and magnitude errors.</h3>
<p>Instead of dividing up errors into Type I/II (falsely rejecting, and falsely failing to reject the null), it is helpful instead to consider errors in <em>sign</em> and <em>magnitude</em> of the effect we report as significant. This philosophy makes a lot of sense if you consider that very few effects are truly zero (so rejecting the null isn’t that important), but are instead small (and variable), and we need to know their size and direction.</p>
<p>A sign error amounts to getting the direction of the effect wrong. A magnitude error amounts to overestimating the effect size.</p>
<div id="magnitude-errors." class="section level4">
<h4>Magnitude errors.</h4>
<p>Consider one of our earlier plots of the rejected and retained null hypotheses for samples from the null, and samples from the alternate with a particular effect size.</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="NHST.html#cb237-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb237-2"><a href="NHST.html#cb237-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb237-3"><a href="NHST.html#cb237-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fl">0.5</span></span>
<span id="cb237-4"><a href="NHST.html#cb237-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb237-5"><a href="NHST.html#cb237-5" aria-hidden="true" tabindex="-1"></a>z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb237-6"><a href="NHST.html#cb237-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">8</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb237-7"><a href="NHST.html#cb237-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb237-8"><a href="NHST.html#cb237-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z),</span>
<span id="cb237-9"><a href="NHST.html#cb237-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H0 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H0 sig. low&quot;</span>, <span class="st">&quot;H0 not sig&quot;</span>)),</span>
<span id="cb237-10"><a href="NHST.html#cb237-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H0&quot;</span>),</span>
<span id="cb237-11"><a href="NHST.html#cb237-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb237-12"><a href="NHST.html#cb237-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z,d<span class="sc">*</span><span class="fu">sqrt</span>(n),<span class="dv">1</span>),</span>
<span id="cb237-13"><a href="NHST.html#cb237-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H1 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H1 sig. low&quot;</span>, <span class="st">&quot;H1 not sig&quot;</span>)),</span>
<span id="cb237-14"><a href="NHST.html#cb237-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H1&quot;</span>))</span>
<span id="cb237-15"><a href="NHST.html#cb237-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb237-16"><a href="NHST.html#cb237-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H0&quot;</span>), </span>
<span id="cb237-17"><a href="NHST.html#cb237-17" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>z, <span class="at">y=</span>p, <span class="at">fill=</span>reject, <span class="at">color=</span>distribution))<span class="sc">+</span></span>
<span id="cb237-18"><a href="NHST.html#cb237-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb237-19"><a href="NHST.html#cb237-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data =</span> <span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H1&quot;</span>), <span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb237-20"><a href="NHST.html#cb237-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> z.crit)<span class="sc">+</span></span>
<span id="cb237-21"><a href="NHST.html#cb237-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;#008888&quot;</span>, <span class="st">&quot;#008888&quot;</span>))<span class="sc">+</span></span>
<span id="cb237-22"><a href="NHST.html#cb237-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;#880000&quot;</span>, <span class="st">&quot;#008800&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-118-1.png" width="3000" /></p>
<p>We see that the bulk of the ‘rejected’ alternate hypothesis distribution falls on the wrong (do not reject) side of the critical z value. Thus, all the z scores we reject were abnormally high as far as samples from the alternate distribution go. Consequently, if we consider the effect size we might <em>estimate</em> from the sample (which we can get as <span class="math inline">\(z/\sqrt{n}\)</span>), we would expect an overestimate, on average. This is precisely what we see when we calculate the average estimated effect from samples from the alternate that were statistically significant.</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="NHST.html#cb238-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr, <span class="at">quietly=</span><span class="cn">TRUE</span>)</span>
<span id="cb238-2"><a href="NHST.html#cb238-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>d.est <span class="ot">=</span> df<span class="sc">$</span>z<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb238-3"><a href="NHST.html#cb238-3" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(distribution <span class="sc">==</span> <span class="st">&quot;H1&quot;</span>, reject <span class="sc">==</span> <span class="st">&quot;H1 sig. high&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb238-4"><a href="NHST.html#cb238-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">true.d =</span> d, <span class="at">avs.est.d =</span> <span class="fu">sum</span>(d.est<span class="sc">*</span>p)<span class="sc">/</span><span class="fu">sum</span>(p))</span></code></pre></div>
<pre><code>##   true.d avs.est.d
## 1    0.5  0.832083</code></pre>
<p>This basic effect is sometimes called the “statistical significance filter”: findings that are significant, are likely to overestimate the true effect size in the population. Moreover, it’s easy to convince ourselves that the lower the power, the worse the overestimation: If power=100%, then we get 0 overestimation.</p>
</div>
<div id="sign-errors" class="section level4">
<h4>Sign errors</h4>
<p>The other kind of error worth considering is the probability that we get the direction of the effect wrong. Although with a reasonable effect size, and no difference in variance between the null and alternate hypothesis, this probability is quite small, it might become intollerable with low power.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="NHST.html#cb240-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb240-2"><a href="NHST.html#cb240-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb240-3"><a href="NHST.html#cb240-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb240-4"><a href="NHST.html#cb240-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb240-5"><a href="NHST.html#cb240-5" aria-hidden="true" tabindex="-1"></a>z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb240-6"><a href="NHST.html#cb240-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">8</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb240-7"><a href="NHST.html#cb240-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb240-8"><a href="NHST.html#cb240-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z),</span>
<span id="cb240-9"><a href="NHST.html#cb240-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H0 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H0 sig. low&quot;</span>, <span class="st">&quot;H0 not sig&quot;</span>)),</span>
<span id="cb240-10"><a href="NHST.html#cb240-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H0&quot;</span>),</span>
<span id="cb240-11"><a href="NHST.html#cb240-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb240-12"><a href="NHST.html#cb240-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z,d<span class="sc">*</span><span class="fu">sqrt</span>(n),<span class="dv">1</span>),</span>
<span id="cb240-13"><a href="NHST.html#cb240-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H1 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H1 sig. low&quot;</span>, <span class="st">&quot;H1 not sig&quot;</span>)),</span>
<span id="cb240-14"><a href="NHST.html#cb240-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H1&quot;</span>))</span>
<span id="cb240-15"><a href="NHST.html#cb240-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb240-16"><a href="NHST.html#cb240-16" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H0&quot;</span>), </span>
<span id="cb240-17"><a href="NHST.html#cb240-17" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>z, <span class="at">y=</span>p, <span class="at">fill=</span>reject, <span class="at">color=</span>distribution))<span class="sc">+</span></span>
<span id="cb240-18"><a href="NHST.html#cb240-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb240-19"><a href="NHST.html#cb240-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data =</span> <span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H1&quot;</span>), <span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb240-20"><a href="NHST.html#cb240-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> z.crit)<span class="sc">+</span></span>
<span id="cb240-21"><a href="NHST.html#cb240-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="sc">-</span>z.crit)<span class="sc">+</span></span>
<span id="cb240-22"><a href="NHST.html#cb240-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;#008888&quot;</span>, <span class="st">&quot;#008888&quot;</span>))<span class="sc">+</span></span>
<span id="cb240-23"><a href="NHST.html#cb240-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;#880000&quot;</span>, <span class="st">&quot;#008800&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-120-1.png" width="3000" /></p>
<p>With low power (arising from small effect and sample sizes), we see that many of our rejections of the null hypothesis based on samples from the null are actually coming from the wrong side of the null! We can calculate their proportion as a function of power. (Note that here we are interested in power to reject on both the correct, and incorrect tail; and we don’t care about whether power comes from effect size or sample size, so we adopt a somewhat tricky equivalence, which you can ignore.)</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="NHST.html#cb241-1" aria-hidden="true" tabindex="-1"></a>pow <span class="ot">=</span> <span class="cf">function</span>(m,z.crit){<span class="fu">pnorm</span>(<span class="sc">-</span>z.crit,m,<span class="dv">1</span>)<span class="sc">+</span><span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(<span class="sc">+</span>z.crit,m,<span class="dv">1</span>)}</span>
<span id="cb241-2"><a href="NHST.html#cb241-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb241-3"><a href="NHST.html#cb241-3" aria-hidden="true" tabindex="-1"></a>p.sign.error <span class="ot">=</span> <span class="cf">function</span>(des.pow,alpha){</span>
<span id="cb241-4"><a href="NHST.html#cb241-4" aria-hidden="true" tabindex="-1"></a>  z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb241-5"><a href="NHST.html#cb241-5" aria-hidden="true" tabindex="-1"></a>  mz <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">4</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb241-6"><a href="NHST.html#cb241-6" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">=</span> mz[<span class="fu">which.min</span>((<span class="fu">pow</span>(mz,z.crit)<span class="sc">-</span>des.pow)<span class="sc">^</span><span class="dv">2</span>)]</span>
<span id="cb241-7"><a href="NHST.html#cb241-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb241-8"><a href="NHST.html#cb241-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">pnorm</span>(<span class="sc">-</span>z.crit,m,<span class="dv">1</span>)<span class="sc">/</span>(<span class="fu">pnorm</span>(<span class="sc">-</span>z.crit,m,<span class="dv">1</span>) <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">+</span>z.crit,m,<span class="dv">1</span>)))</span>
<span id="cb241-9"><a href="NHST.html#cb241-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb241-10"><a href="NHST.html#cb241-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb241-11"><a href="NHST.html#cb241-11" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb241-12"><a href="NHST.html#cb241-12" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>()</span>
<span id="cb241-13"><a href="NHST.html#cb241-13" aria-hidden="true" tabindex="-1"></a>power <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.05</span>,<span class="fl">0.15</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb241-14"><a href="NHST.html#cb241-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(power)){</span>
<span id="cb241-15"><a href="NHST.html#cb241-15" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">=</span> <span class="fu">rbind</span>(df,</span>
<span id="cb241-16"><a href="NHST.html#cb241-16" aria-hidden="true" tabindex="-1"></a>             <span class="fu">data.frame</span>(<span class="at">power=</span>power[i],</span>
<span id="cb241-17"><a href="NHST.html#cb241-17" aria-hidden="true" tabindex="-1"></a>                        <span class="at">alpha=</span>alpha,</span>
<span id="cb241-18"><a href="NHST.html#cb241-18" aria-hidden="true" tabindex="-1"></a>                        <span class="at">p.sign.err =</span> <span class="fu">p.sign.error</span>(power[i],alpha)))</span>
<span id="cb241-19"><a href="NHST.html#cb241-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb241-20"><a href="NHST.html#cb241-20" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>##    power alpha p.sign.err
## 1   0.05  0.05 0.50000000
## 2   0.06  0.05 0.20482240
## 3   0.07  0.05 0.12289824
## 4   0.08  0.05 0.08414848
## 5   0.09  0.05 0.06204462
## 6   0.10  0.05 0.04544329
## 7   0.11  0.05 0.03465503
## 8   0.12  0.05 0.02634886
## 9   0.13  0.05 0.02093070
## 10  0.14  0.05 0.01660337
## 11  0.15  0.05 0.01378320</code></pre>
<p>What this tells us is that a null hypothesis z-test, with the standard deviation correctly matched between the true alternate and the null, will declare a sample from the alternate as significant, but get the <em>direction of the effect wrong</em> frighteningly frequently when our power is very low. Hopefully, our power is rarely that low.</p>
<p>However, one problem that is likely to arise when running z-tests is that the population might have a different standard deviation than assumed under the null hypothesis, we get quite a different phenomenon:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="NHST.html#cb243-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.05</span></span>
<span id="cb243-2"><a href="NHST.html#cb243-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb243-3"><a href="NHST.html#cb243-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb243-4"><a href="NHST.html#cb243-4" aria-hidden="true" tabindex="-1"></a>sd.ratio <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb243-5"><a href="NHST.html#cb243-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb243-6"><a href="NHST.html#cb243-6" aria-hidden="true" tabindex="-1"></a>z.crit <span class="ot">=</span> <span class="fu">abs</span>(<span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb243-7"><a href="NHST.html#cb243-7" aria-hidden="true" tabindex="-1"></a>z <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb243-8"><a href="NHST.html#cb243-8" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb243-9"><a href="NHST.html#cb243-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z),</span>
<span id="cb243-10"><a href="NHST.html#cb243-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H0 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H0 sig. low&quot;</span>, <span class="st">&quot;H0 not sig&quot;</span>)),</span>
<span id="cb243-11"><a href="NHST.html#cb243-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H0&quot;</span>),</span>
<span id="cb243-12"><a href="NHST.html#cb243-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">z=</span>z,</span>
<span id="cb243-13"><a href="NHST.html#cb243-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p=</span><span class="fu">dnorm</span>(z,d<span class="sc">*</span><span class="fu">sqrt</span>(n),<span class="dv">1</span><span class="sc">*</span>sd.ratio),</span>
<span id="cb243-14"><a href="NHST.html#cb243-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">reject=</span><span class="fu">ifelse</span>(z<span class="sc">&gt;=</span>z.crit, <span class="st">&quot;H1 sig. high&quot;</span>, <span class="fu">ifelse</span>(z<span class="sc">&lt;=-</span>z.crit, <span class="st">&quot;H1 sig. low&quot;</span>, <span class="st">&quot;H1 not sig&quot;</span>)),</span>
<span id="cb243-15"><a href="NHST.html#cb243-15" aria-hidden="true" tabindex="-1"></a>                 <span class="at">distribution=</span><span class="st">&quot;H1&quot;</span>))</span>
<span id="cb243-16"><a href="NHST.html#cb243-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb243-17"><a href="NHST.html#cb243-17" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H0&quot;</span>), </span>
<span id="cb243-18"><a href="NHST.html#cb243-18" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>z, <span class="at">y=</span>p, <span class="at">fill=</span>reject, <span class="at">color=</span>distribution))<span class="sc">+</span></span>
<span id="cb243-19"><a href="NHST.html#cb243-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb243-20"><a href="NHST.html#cb243-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data =</span> <span class="fu">subset</span>(df, df<span class="sc">$</span>distribution<span class="sc">==</span><span class="st">&quot;H1&quot;</span>), <span class="at">alpha=</span><span class="fl">0.3</span>)<span class="sc">+</span></span>
<span id="cb243-21"><a href="NHST.html#cb243-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> z.crit)<span class="sc">+</span></span>
<span id="cb243-22"><a href="NHST.html#cb243-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="sc">-</span>z.crit)<span class="sc">+</span></span>
<span id="cb243-23"><a href="NHST.html#cb243-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;#008888&quot;</span>, <span class="st">&quot;#008888&quot;</span>))<span class="sc">+</span></span>
<span id="cb243-24"><a href="NHST.html#cb243-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;#880000&quot;</span>, <span class="st">&quot;#008800&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-122-1.png" width="3000" /></p>
<p>As you can see, even with no difference in means, but a difference in standard deviations, we might get considerable power (we will reject the null for lots of samples from the alternate). But not for the reason we think (not because there is a difference in means, but because there is a difference in standard deviations). This is the reason we generally don’t use z-tests (which postulate a null hypothesis with a specific mean and a specific standard deviation), and use t-tests instead (which postulate a null hypothesis with a specific mean, but are agnostic as to the standard deviation).</p>

</div>
</div>
</div>
<div id="theory-binomial" class="section level2">
<h2>Binomial: Probability to statistics</h2>
<p>These notes are (a) very mathy, (b) very optional, and (c) cover a very broad range of material that we will not directly address this term. I hope that for especially advanced and ambitious folks, this kind of theoretical exposition of what is done, and why, in various statistical methods will be helpful. For folks who are less comfortable with math, this will be a whole lot of difficult material that is likely to increase, rather than reduce, confusion.</p>
<p>Let’s say our data are the outcomes of 10 coin flips obtained by flipping a coin 10 times:<br />
<span class="math inline">\(X = \{x_1, x_2, ..., x_10\} = \{H,H,H,H,H,H,H,H,T,T\}\)</span>.</p>
<div id="data-description-summary" class="section level3">
<h3>Data description / summary</h3>
<p>The first thing we might do is summarize the data. Whenever we summarize data we inevitably make some assumptions about which aspects of our data are important, and which are not. Often, these assumptions reflect our hypotheses about how the data were generated, and which aspects of the data ought to be relatively stable across samples. “Descriptive statistics” is usually used as the opposite of “inferential statistics,” suggesting that when we choose some way to summarize the data, we are in no way drawing inferences or attempting to generalize from the sample. This is true only technically, but not practically, since our data descriptions often tend to coincide with estimators of model parameters. Hopefully this will become more clear as this section continues.</p>
<p>Let’s start with our data: 10 outcomes of a coin flip: H,H,H,H,H,H,H,H,T,T.</p>
<p>How should we summarize the data? Well, we can first tabulate the frequencies of different outcomes, and if we are so inclined, we can plot these frequencies as a histogram.</p>
<table>
<thead>
<tr class="header">
<th align="left">outcome</th>
<th align="right">frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">H</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="left">T</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p>By summarizing the frequencies of heads and tails, we are throwing out information about the order in which those data points came. This is a useful summary insofar as we believe that the order doesn’t matter. The order doesn’t matter if we believe that those particular H/T outcomes are independent (if the flip of a coin does not depend on the previous outcome of the coin flip, or any other coin flip at all).</p>
<p>However, if we believed that our data came from a coin flipping process where the coin-flipper tended to flip the coin an even number of times, that means that the second outcome will tend to be like the first outcome, and the third like the second, etc. Thus, the individual coin flips are not independent, because they were generated by a process that tends to reproduce the same outcome in succession. If we thought that such a process was underlying out data, then instead of tabulating the raw frequencies of heads and tails we might choose to tabulate how often the outcomes were different in consecutive pairs of flips (note that while we have 10 flips, we only have 9 consecutive pairs). By choosing to tabulate repetitions/swaps, we throw out information about whether the coin was heads or tails, and which came up more often:</p>
<table>
<thead>
<tr class="header">
<th align="left">outcome</th>
<th align="right">frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">same</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="left">different</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>Which of these histograms is a more useful summary depends on our beliefs about how the data were generated, and which features of the data are more or less important. It is important to note that both summaries throw out some information while highlighting some other information.</p>
<p>In addition to tabulating frequencies, often data are summarized with some <a href="../introduction/descriptive.html">“statistics”</a>. Any function of the data that returns some simple number (not always, but usually just one number), is called a <strong>statistic</strong>. Some of these statistics are more useful than others because they have useful properties and serve as estimators for parameters of common models. All of these summarize some aspect of the data, while ignoring other aspects; thus in choosing a particular summary statistic you effectively presume which aspects of your data are important, and which can be disregarded.</p>
<p>For instance, for our data we might consider a number of statistics:</p>
<p><span class="math inline">\(\frac{\text{# heads}}{\text{total}} = 0.8\)</span>, or</p>
<p><span class="math inline">\(\frac{\text{# heads}}{\text{# tails}} = 4.0\)</span>, or</p>
<p>“position of first occurrence of tails” <span class="math inline">\(= 9\)</span>, or</p>
<p><span class="math inline">\(\frac{\text{# repeats}}{\text{# consecutive pairs}} = 0.\bar8\)</span>, etc.</p>
<p>In short, the choice of which aspects of the data to summarize necessarily reflects some implicit beliefs about the structure that ought to be present in the data. Similarly, when we decide to present some <a href="../visualization/visualization.html">graphical summary</a> of the data, we will also need to choose which aspects of the data to emphasize, and which can be obscured. Therefore the choice of which descriptive statistics to report and consider should be done with ample thought and care (like everything else in scientific research)</p>
</div>
<div id="estimation" class="section level3">
<h3>Estimation</h3>
<p>Descriptive statistics are somewhat motivated by an (implicitly) assumed model of the data, if we make this process explicit, we will postulate a particular <a href="../introduction/uses-of-models.html">statistical model</a> of the data. We usually call this the <em>population</em> model, which will have some unknown <strong>parameters</strong>. For instance, we might suppose that our sequence of coin flips reflect independent, identically distributed outcomes from the flip of a bent coin that comes up heads with probability <span class="math inline">\(\theta\)</span> (theta; note we are now switching to using greek letters for population parameters), and tails with probability <span class="math inline">\(1-\theta\)</span>. Thus our model of the data is that the number of heads, <span class="math inline">\(k\)</span>, out of <span class="math inline">\(n\)</span> flips is a sample from a Binomial: <span class="math inline">\(k \sim \operatorname{Binomial}(k \mid n, \theta)\)</span>, and we need to use our sample of coin flips to estimate the population parameter (obtaining the estimate <span class="math inline">\(\hat\theta\)</span>).</p>
<div id="point-estimates" class="section level4">
<h4>Point Estimates</h4>
<p>Point estimates are a single value, corresponding to our best guess about the latent parameter; these are contrasted with <em>interval estimates</em> which provide a range of plausible values that the parameter might be. We will consider two approaches to estimating parameters, in our case just <span class="math inline">\(\theta\)</span>: <strong>Maximum Likelihood</strong> and <strong>Maximum A Posteriori</strong>.</p>
<p>There are others methods for estimation, but explaining how they work, and the logic behind them, is rather convoluted. Moreover, there is rarely any reason to prefer them. Popular alternative approaches include “Method of Moments” and “Minimum squared error” (a.k.a. “Least squares”). Later in the course we will talk about least squares estimates, but this will be in the context of models where least squares error estimates are equivalent to the maximum likelihood estimates, so while we will use the least squares estimation procedure, we can think of it as a convenient way to obtain a maximum likelihood estimate. (One notable exception is estimators for the variance.)</p>
<div id="classical-maximum-likelihood-estimate" class="section level5">
<h5>Classical (Maximum Likelihood) estimate</h5>
<p>The Maximum Likelihood estimate (ML estimate) aims to find the parameter value <span class="math inline">\(\theta\)</span> which makes the data most likely, thus maximizing the likelihood function.</p>
<p>What is the <strong>likelihood function</strong>?</p>
<p>Consider the conditional probability <span class="math inline">\(P(X|\theta)\)</span>, this assigns probabilities to different data outcomes for a given <span class="math inline">\(\theta\)</span>. For instance:</p>
<p><span class="math inline">\(P(3 \text{ heads out of } 8|\theta=0.8) = \operatorname{Binomial}(3 \mid 8, 0.8) = {8 \choose 3} 0.8^3(1-0.8)^7\)</span> = 0.009175<br />
<span class="math inline">\(P(7 \text{ heads out of } 8|\theta=0.8) = \operatorname{Binomial}(7 \mid 8, 0.8) = {8 \choose 7} 0.8^7(1-0.8)^1\)</span> = 0.3355443</p>
<p>If we consider <span class="math inline">\(P(X|\theta,n)\)</span> as a function over possible <span class="math inline">\(X\)</span>s, assigning probability to different data outcomes while <span class="math inline">\(\theta\)</span> remains fixed, then <span class="math inline">\(P(X|\theta,n)\)</span> is a conditional probability distribution over possible data sets (so if you sum <span class="math inline">\(P(X|\theta,n)\)</span> over all possible <span class="math inline">\(X\)</span>s, you will get 1.)</p>
<p>In contrast, the likelihood function, usually written as <span class="math inline">\(\mathcal{L}(X|\theta)\)</span>, is not a probability distribution. It is a collection of <span class="math inline">\(P(X|\theta)\)</span> values for different values of <span class="math inline">\(\theta\)</span> while <span class="math inline">\(X\)</span> is fixed.</p>
<p>The likelihood function does not sum to 1 when you sum over all <span class="math inline">\(\theta\)</span>s; however, it does indicate how likely the data are under each possible <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(\mathcal{L}(3 \text{ heads out of } 8|\theta=0.2) = \operatorname{Binomial}(3 \mid 8, 0.2) = {8 \choose 3} 0.2^3(1-0.2)^7\)</span> = 0.1468006<br />
<span class="math inline">\(\mathcal{L}(3 \text{ heads out of } 8|\theta=0.4) = \operatorname{Binomial}(3 \mid 8, 0.4) = {8 \choose 3} 0.4^3(1-0.4)^7\)</span> = 0.2786918<br />
<span class="math inline">\(\mathcal{L}(3 \text{ heads out of } 8|\theta=0.6) = \operatorname{Binomial}(3 \mid 8, 0.6) = {8 \choose 3} 0.6^3(1-0.6)^7\)</span> = 0.123863<br />
<span class="math inline">\(\mathcal{L}(3 \text{ heads out of } 8|\theta=0.8) = \operatorname{Binomial}(3 \mid 8, 0.8) = {8 \choose 3} 0.8^3(1-0.8)^7\)</span> = 0.009175</p>
<p>The likelihood function effectively describes how well different values of our parameter <span class="math inline">\(\theta\)</span> describe the data <span class="math inline">\(X\)</span>. If we choose the maximum point of this function, we choose the parameter value that maximizes the likelihood of the data.</p>
<p>We can plot the likelihood function for our data (<span class="math inline">\(X= \{H,H,H,H,H,H,H,H,T,T\}\)</span>) below:</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="NHST.html#cb244-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&#39;H&#39;</span>,<span class="st">&#39;H&#39;</span>,<span class="st">&#39;H&#39;</span>,<span class="st">&#39;H&#39;</span>,<span class="st">&#39;H&#39;</span>,<span class="st">&#39;H&#39;</span>,<span class="st">&#39;H&#39;</span>,<span class="st">&#39;H&#39;</span>,<span class="st">&#39;T&#39;</span>,<span class="st">&#39;T&#39;</span>)</span>
<span id="cb244-2"><a href="NHST.html#cb244-2" aria-hidden="true" tabindex="-1"></a>n.heads <span class="ot">=</span> <span class="fu">sum</span>(data<span class="sc">==</span><span class="st">&#39;H&#39;</span>)</span>
<span id="cb244-3"><a href="NHST.html#cb244-3" aria-hidden="true" tabindex="-1"></a>n.total <span class="ot">=</span> <span class="fu">length</span>(data)</span>
<span id="cb244-4"><a href="NHST.html#cb244-4" aria-hidden="true" tabindex="-1"></a>thetas <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.001</span>)</span>
<span id="cb244-5"><a href="NHST.html#cb244-5" aria-hidden="true" tabindex="-1"></a>lik.theta <span class="ot">=</span> <span class="fu">dbinom</span>(n.heads, n.total, thetas)</span>
<span id="cb244-6"><a href="NHST.html#cb244-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb244-7"><a href="NHST.html#cb244-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">theta=</span>thetas, <span class="at">likelihood=</span>lik.theta), <span class="fu">aes</span>(theta, likelihood))<span class="sc">+</span><span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-123-1.png" width="3000" /></p>
<p>We can find the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood with:</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="NHST.html#cb245-1" aria-hidden="true" tabindex="-1"></a>thetas[<span class="fu">which</span>(lik.theta<span class="sc">==</span><span class="fu">max</span>(lik.theta))]</span></code></pre></div>
<pre><code>## [1] 0.8</code></pre>
<p>The <span class="math inline">\(\theta\)</span> for which the likelihood is greatest is 0.8. Thus, our maximum likelihood estimate is <span class="math inline">\(\hat \theta_{ML} = 0.8\)</span>. The hat operator (<span class="math inline">\(\hat \cdot\)</span>) is convention for indicating an estimate of a parameter value, the subscript usually denotes the type of estimator used to obtain the estimate (here “ML” for “maximum likelihood”). Note that for many statistical models, the maximum likelihood estimate need not be calculated numerically, but has an analytical solution; we will go over these when appropriate in the future, but for now, I want to convey what the maximum likelihood estimator means.</p>
</div>
<div id="bayesian-maximum-a-posteriori-estimate" class="section level5">
<h5>Bayesian (Maximum a posteriori) estimate</h5>
<p>The maximum a posteriori estimate (MAP estimate) uses Bayes’ rule to invert the probability, to obtain <span class="math inline">\(P(\theta|X)\)</span> from <span class="math inline">\(P(X|\theta)\)</span>. Of course, to do so one must specify some sort of prior distribution over the parameter <span class="math inline">\(\theta\)</span> (<span class="math inline">\(P(\theta)\)</span>; this need to choose a prior is one reason why some are uneasy about Bayesian estimation). Let’s say we believe that a coin is unlikely to be biased too far away from 0.5, so our prior will be centered on 0.5. We can express such a prior with a <a href="https://en.wikipedia.org/wiki/Beta_distribution">beta distribution</a>.</p>
<p><span class="math inline">\(P(\theta)= \operatorname{Beta}(2,2)\)</span> This is the prior on <span class="math inline">\(\theta\)</span>.</p>
<p>So now, we can calculate the posterior as:</p>
<p><span class="math inline">\(P(\theta|X) = \frac{\mathcal{L}(X|\theta)P(\theta)}{\int\limits_0^1 \mathcal{L}(X|\theta)P(\theta)d\theta}\)</span><br />
(Note that the denominator comes from the law of total probability.)</p>
<p>In practice we could use an analytical solution for this posterior distribution (the Beta distribution, by virtue of it being a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> for the Binomial); however, here we can just do it via a crude numerical estimate.</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="NHST.html#cb247-1" aria-hidden="true" tabindex="-1"></a>p.theta <span class="ot">=</span> <span class="fu">dbeta</span>(thetas, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb247-2"><a href="NHST.html#cb247-2" aria-hidden="true" tabindex="-1"></a>p.theta <span class="ot">=</span> p.theta<span class="sc">/</span><span class="fu">sum</span>(p.theta)</span>
<span id="cb247-3"><a href="NHST.html#cb247-3" aria-hidden="true" tabindex="-1"></a>p.data <span class="ot">=</span> <span class="fu">sum</span>(lik.theta<span class="sc">*</span>p.theta)</span>
<span id="cb247-4"><a href="NHST.html#cb247-4" aria-hidden="true" tabindex="-1"></a>p.theta.data <span class="ot">=</span> lik.theta<span class="sc">*</span>p.theta <span class="sc">/</span> p.data</span>
<span id="cb247-5"><a href="NHST.html#cb247-5" aria-hidden="true" tabindex="-1"></a>p.theta.data.uni <span class="ot">=</span> lik.theta<span class="sc">/</span><span class="fu">sum</span>(lik.theta)</span>
<span id="cb247-6"><a href="NHST.html#cb247-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb247-7"><a href="NHST.html#cb247-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">theta=</span>thetas, <span class="at">probability=</span>p.theta, <span class="at">whichone=</span><span class="st">&quot;beta prior&quot;</span>),</span>
<span id="cb247-8"><a href="NHST.html#cb247-8" aria-hidden="true" tabindex="-1"></a>              <span class="fu">data.frame</span>(<span class="at">theta=</span>thetas, <span class="at">probability=</span>p.theta.data, <span class="at">whichone=</span><span class="st">&quot;posterior (beta)&quot;</span>),</span>
<span id="cb247-9"><a href="NHST.html#cb247-9" aria-hidden="true" tabindex="-1"></a>              <span class="fu">data.frame</span>(<span class="at">theta=</span>thetas, <span class="at">probability=</span>p.theta.data.uni, <span class="at">whichone=</span><span class="st">&quot;posterior (uniform)&quot;</span>)), </span>
<span id="cb247-10"><a href="NHST.html#cb247-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">aes</span>(theta, probability, <span class="at">color=</span>whichone))<span class="sc">+</span><span class="fu">geom_line</span>()</span>
<span id="cb247-11"><a href="NHST.html#cb247-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb247-12"><a href="NHST.html#cb247-12" aria-hidden="true" tabindex="-1"></a>thetas[<span class="fu">which</span>(p.theta.data <span class="sc">==</span> <span class="fu">max</span>(p.theta.data))] <span class="co"># Maximum a posteriori</span></span></code></pre></div>
<pre><code>## [1] 0.75</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-125-1.png" width="3000" /></p>
<p>Note that<br />
(a) the posterior distribution under a uniform prior (which is just the normalized likelihood), has the same shape and peak as just the likelihood function,<br />
(b) the posterior under a non-uniform prior gives us an estimate somewhere between the likelihood and the prior.</p>
<p>Here the the maximum a posteriori (MAP) estimate is 0.75; we would write this as <span class="math inline">\(\hat \theta_{MAP} = 0.75\)</span>. (the MAP estimate under a uniform prior gives us the same estimate as the maximum likelihood).</p>
</div>
<div id="estimators" class="section level5">
<h5>Estimators</h5>
<p>While the two approaches we have described are general, all-purpose methods to obtain ML and MAP estimates, typically, we will not work through all of this. Instead we will use special types of statistics (again, statistics meaning “functions of the data”) which someone else has proven to yield the appropriate estimates: these are called <strong>estimators</strong>. For instance, <span class="math inline">\(k/n\)</span> is a maximum likelihood estimator for the Bernoulli/Binomial <span class="math inline">\(\theta\)</span>, in the sense that it will always give the <span class="math inline">\(\theta\)</span> value which maximizes <span class="math inline">\(\mathcal{L}(D|\theta)\)</span>. The sample mean (which we will get to later) <span class="math inline">\(\bar x = \sum_{i=1}^n x_i / n\)</span> is the maximum likelihood estimator for the mean (<span class="math inline">\(\mu\)</span>) when we assume the data come from a Gaussian, because, again, it will always maximize the Gaussian likelihood. So, estimators are functions of the data that yield useful estimates.</p>
<p>Not all estimators map neatly on to the ML estimate, and sometimes this is quite desirable. For instance, the maximum likelihood estimate of the variance is biased because of the skewed shape of the likelihood function for variance. In this case, a different estimator is used (which can be thought of as a correction to the ML estimate).</p>
</div>
</div>
<div id="interval-estimates" class="section level4">
<h4>Interval Estimates</h4>
<p>Our point estimates, <span class="math inline">\(\hat \theta_{ML}\)</span> and <span class="math inline">\(\hat \theta_{MAP}\)</span>, are not particularly useful on their own, because they are bound to have some error/uncertainty due to sampling variability. In other words: if we flip the same coin 10 more times, we will get different outcomes, and the number of heads will not be exactly 8; thus, the 8/10 estimate from our data arises from the idiosyncrasies of our specific sample. Consequently, instead of providing a single best guess (a point) as an estimate, it is usually preferable to provide a range (interval) of guesses around our best estimate, to indicate the expected precision of our estimate. Usually these are expressed as a percentage (e.g., a 95% interval). And for the general case we will often refer to a <span class="math inline">\(100*q\%\)</span> interval (where <span class="math inline">\(q\)</span> is between 0 and 1).</p>
<p>However, when providing such interval estimate, the difference between frequentist and Bayesian approaches to statistics becomes quite important.</p>
<div id="bayesian-credible-intervals" class="section level5">
<h5>Bayesian Credible Intervals</h5>
<p>I will start by describing Bayesian interval estimates, called “credible intervals,” because their definition, interpretation, and procedure for calculation is intuitive – it is what most people think of when they read “95% interval estimate of men’s shoe size is 7 to 11,” or other such statements.</p>
<p><span class="math inline">\([a,b]\)</span> is a <span class="math inline">\(100*q\%\)</span> credible interval for our parameter <span class="math inline">\(\theta\)</span> if <span class="math inline">\(P(a \leq \theta \leq b | X) = q\)</span>. Which should be intuitive: <span class="math inline">\([a,b]\)</span> is a <span class="math inline">\(100q\%\)</span> credible interval for our parameter <span class="math inline">\(\theta\)</span> if the posterior probability that <span class="math inline">\(\theta\)</span> is between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is <span class="math inline">\(q\)</span>.</p>
<p>In the previous section we calculated the posterior distribution of <span class="math inline">\(\theta\)</span>: <span class="math inline">\(P(\theta|X)\)</span>. From this distribution we can compute the cumulative distribution function: <span class="math inline">\(F_{\theta}(\theta&#39;) = P(\theta \leq \theta&#39; | X)\)</span>. This cdf has an intuitive interpretation: what is the probability that the parameter value <span class="math inline">\(\theta\)</span> is less than or equal to <span class="math inline">\(\theta&#39;\)</span>?</p>
<p>Thus, if we aim to obtain a <span class="math inline">\(100q\%\)</span> interval, we need to find values <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(F_{\theta}(b) - F_{\theta}(a) = q\)</span>. There are many pairs of values for which this will be true, but some of them are more useful than another. It is generally most convenient to take an interval that leaves equal amounts of probability on either side (thus guaranteeing to include the median, so that <span class="math inline">\(F_{\theta}(a) = 0.5 - q/2\)</span> and <span class="math inline">\(F_{\theta}(b) = 0.5 + q/2\)</span>. (Note that this procedure does not guarantee that the MAP estimate – the mode of the posterior distribution– will be included in the interval, but it is easiest to define an interval around the median; moreover, when dealing with wide intervals [when <span class="math inline">\(q\)</span> is quite large], it doesn’t much matter.)</p>
<p>For instance, if we use our numerical estimates from before, we can calculate an approximation of the cdf, and estimate quantiles from it numerically.</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="NHST.html#cb249-1" aria-hidden="true" tabindex="-1"></a>cum.probability <span class="ot">=</span> <span class="fu">cumsum</span>(p.theta.data)</span>
<span id="cb249-2"><a href="NHST.html#cb249-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">theta=</span>thetas, <span class="at">cumulative.prob=</span>cum.probability), <span class="fu">aes</span>(theta, cumulative.prob))<span class="sc">+</span><span class="fu">geom_line</span>()</span>
<span id="cb249-3"><a href="NHST.html#cb249-3" aria-hidden="true" tabindex="-1"></a><span class="fu">range</span>(thetas[cum.probability<span class="sc">&gt;=</span><span class="fl">0.025</span> <span class="sc">&amp;</span> cum.probability<span class="sc">&lt;=</span><span class="fl">0.975</span>])</span></code></pre></div>
<pre><code>## [1] 0.462 0.908</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-126-1.png" width="3000" /></p>
<p>Obviously, the validity of our estimates here is contingent on the granularity of our numerical approximation: if we consider only a small number of thetas, we cannot estimate an interval estimate very precisely.</p>
<p>These credible intervals are constructed from the posterior distribution of the parameter given the data; thus we can make statements like “given the data, (and our prior), the probability that <span class="math inline">\(\theta\)</span> is between 0.46 and 0.91 is 95%.”</p>
</div>
<div id="frequentist-confidence-intervals" class="section level5">
<h5>Frequentist Confidence Intervals</h5>
<p>Frequentists are not as lucky as Bayesians when it comes to constructing interval estimates. According to the philosophy of frequentist statistics, there exists a true parameter value in the world. This value is fixed, and it is meaningless to assign probabilities to different possible parameter values (because probabilities are relative frequencies, and the parameter value will always be the same – it is fixed). Consequently, a given interval estimate either includes, or does not include, the true parameter value, and again, we cannot assign a probability to the statement that “the true parameter value is within this interval” (that statement is either true or false, and it will always be either true or false, so we can’t assign it a relative frequency). However, while we cannot make frequentist probability statements about a particular estimate, we can make probability statements about our <em>procedure</em>, so we might say, “the procedure that I used for constructing this 95% confidence interval, if repeated many many times in different experiments, will contain the respective true parameter values 95% of the time.”</p>
<p>The Wikipedia confidence interval page provides the rather unwieldy formal definition of a confidence interval which I will simplify for our case. We can define two functions of the data (statistics) <span class="math inline">\(L(X)\)</span> and <span class="math inline">\(U(X)\)</span>. If the interval <span class="math inline">\([L(X), U(X)]\)</span> is a <span class="math inline">\(100q\%\)</span> confidence interval then if we generate many data sets from a Bernoulli distribution with a parameter <span class="math inline">\(\theta\)</span>, and we construct a lower and upper bound using the functions <span class="math inline">\(L(\cdot)\)</span> and <span class="math inline">\(U(\cdot)\)</span> for each data set, then the parameter <span class="math inline">\(\theta\)</span> will be within those bounds <span class="math inline">\(q\)</span> proportion of the time.</p>
<p>Thus, the <span class="math inline">\(100q\%\)</span> associated with a confidence interval statement is not a statement about the parameter value, but a statement about the functions <span class="math inline">\(L()\)</span> and <span class="math inline">\(U()\)</span>: those functions have the property that 95% of the intervals constructed using these functions will contain the true parameter value for that experiment. This is why frequentist confidence intervals are confusing.</p>
<p>That’s all well and good, but how do we actually obtain a confidence interval in our case of 8 heads and 2 tails? <a href="http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">A number of approaches</a> to constructing intervals are available that have (approximately) the required property described above. The most common relies on a normal approximation (which works reasonably provided you have enough data and <span class="math inline">\(\theta\)</span> is not too close to 0 or 1). This confidence interval with <span class="math inline">\(q=1-\alpha\)</span>, is defined as:<br />
<span class="math inline">\(\hat\theta \pm Z_{\alpha/2} \sqrt{\frac{\hat\theta(1-\hat\theta)}{n}}\)</span>,<br />
where <span class="math inline">\(Z_{\alpha/2}\)</span> is the z-score for <span class="math inline">\((\alpha/2)\)</span> – the <span class="math inline">\((\alpha/2)\)</span> quantile of the standard normal distribution (we will discuss this later). Since we haven’t yet talked about the normal distribution, we will not discus this approximate confidence interval further until we are really interested in categorical data; plus, that interval estimate will be inappropriate because of our small sample size).</p>
</div>
<div id="confidence-intervals-from-null-hypothesis-tests" class="section level5">
<h5>Confidence intervals from null hypothesis tests</h5>
<p>Instead of using a confidence interval from the normal approximation, we will consider the “exact” frequentist confidence interval, purely for pedagogical (not practical) purposes. Under this approach, we construct a confidence interval via significance testing. This yields the second interpretation of a frequentist confidence interval: “A <span class="math inline">\(100q\%\)</span> confidence interval for a parameter <span class="math inline">\(\theta\)</span> is the range of values that <span class="math inline">\(\theta_0\)</span> (the null hypothesis value of <span class="math inline">\(\theta\)</span>) could take on without you being able to reject the null hypothesis with your data at significance level <span class="math inline">\(\alpha = 1-q\)</span>.” We will now construct a confidence interval for our Bernoulli <span class="math inline">\(\theta\)</span> in this manner (Although normally this could done analytically, here we will go through the exercise of calculating it numerically).</p>
<p>We will use the <code>binom.test</code> function to get a p.value for the null hypothesis binomial test, and obtain this p-value for every theta we can consider, then keep the thetas we could not reject, and define our interval based on the range of the theta values we could not reject.</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="NHST.html#cb251-1" aria-hidden="true" tabindex="-1"></a>p.val <span class="ot">=</span> <span class="fu">sapply</span>(thetas, <span class="cf">function</span>(theta){<span class="fu">min</span>(<span class="fu">binom.test</span>(n.heads, n.total, theta, <span class="at">alternative =</span> <span class="st">&quot;greater&quot;</span>)<span class="sc">$</span>p.value,</span>
<span id="cb251-2"><a href="NHST.html#cb251-2" aria-hidden="true" tabindex="-1"></a>                                           <span class="fu">binom.test</span>(n.heads, n.total, theta, <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>)<span class="sc">$</span>p.value)})</span>
<span id="cb251-3"><a href="NHST.html#cb251-3" aria-hidden="true" tabindex="-1"></a>not.rejected.thetas <span class="ot">=</span> thetas[p.val<span class="sc">&gt;</span>(<span class="dv">1</span><span class="fl">-0.95</span>)<span class="sc">/</span><span class="dv">2</span>]</span>
<span id="cb251-4"><a href="NHST.html#cb251-4" aria-hidden="true" tabindex="-1"></a><span class="fu">range</span>(not.rejected.thetas)</span></code></pre></div>
<pre><code>## [1] 0.444 0.974</code></pre>
<p>So the interval we get out is [0.45 0.96].</p>
</div>
</div>
</div>
<div id="null-hypothesis-significance-testing-nhst" class="section level3">
<h3>Null Hypothesis Significance testing (NHST)</h3>
<p>Hypothesis testing typically follows these steps:</p>
<ol style="list-style-type: decimal">
<li><p>First, you define a null hypothesis, usually written as <span class="math inline">\(\mathcal{H}_0\)</span>. This null hypothesis should capture some belief about the population (or data distribution), which is the default/standard, or the dull “no discovery here” current belief. For instance, if comparing salaries between men and women, you might want the null hypothesis to be “they have the same average.” Or if you are Kepler measuring planetary orbits, you might pose the null hypothesis that orbits are circular (rather than elliptical). Or if you are measuring coin flips, as we are, your null hypothesis could sensibly be “they are independent, identically distributed samples from a fair coin (<span class="math inline">\(X\sim \text{Bernoulli}(\theta=0.5)\)</span>).” Note that our null hypothesis here is “parametric” in the sense that it makes a claim about the distribution of the data by specifying a parametric distribution of the data.</p></li>
<li><p>Second, you define a “test statistic.” This is like like any other statistic (a function of the data which returns a value), but it will serve as the basis for your significance test. Let’s call the value you obtain for your test statistic <span class="math inline">\(v_x\)</span> (for “value (from the data <span class="math inline">\(x\)</span>)”). For us, a sensible choice of test statistic would be “the number of heads” observed: <span class="math inline">\(v_x = 8\)</span>.</p></li>
<li><p>Third, you obtain (look up somewhere, compute numerically, or calculate analytically) the “sampling distribution” for this test statistic under the null hypothesis. In other words, if the null hypothesis were true, what values of the test statistic would you expect to see, with what frequency. We can refer to this distribution as <span class="math inline">\(P_0(V)\)</span>. This is the “sampling distribution of V” where V is the test statistic of interest under the null hypothesis. This is often simply called the “null hypothesis distribution” (beware that this creates some ambiguity, since the null hypothesis distribution also refers to the distribution of possible data under the null hypothesis.) Fortunately, we have already calculated the sampling distribution for our “number of heads” test statistic: the Binomial distribution.</p></li>
<li><p>Fourth, you compare where your observed test statistic (<span class="math inline">\(v_x\)</span>) falls with respect to the null-hypothesis sampling distribution of the test statistic (<span class="math inline">\(P_0(V)\)</span>). Your goal is to ascertain the probability under the null hypothesis of observing a test-statistic value at least as “extreme” as the one you calculated of your data. <span class="math inline">\(P_0(V \geq v_x) = \text{p-value}\)</span>. (We are doing a one-tailed test here – not because it is the right thing to do, but because it is easier to describe as a start.) This is the “p value” – the probability that a test statistic at least as extreme as the one you observed would arise from the null hypothesis distribution. You will then reject the null hypothesis if this probability is small enough, and if the probability is not sufficiently small, you will “fail to reject” the null hypothesis. (You will never “accept” the null hypothesis, since NHST follows Karl Popper’s “falsificationist” approach to the philosophy of science: meaning that science can only falsify theories, but never validate them.)</p></li>
</ol>
<p>There are a number of variations on hypothesis testing, but the essence remains the same.</p>
<p>The traditional procedure is to choose a “significance level” some time before step 4. A significance level, usually denoted as <span class="math inline">\(\alpha\)</span> (“alpha”), indicates the rate at which you are willing to falsely reject the null hypothesis. The traditional procedure suggests that you use <span class="math inline">\(\alpha\)</span> to compute a critical value <span class="math inline">\(v_{crit}\)</span>, defined (for our one tailed test) as <span class="math inline">\(P_0(V \geq v_{crit})=\alpha\)</span>. The null hypothesis is rejected if your test statistic is more extreme than the critical value (<span class="math inline">\(v_x \geq v_{crit}\)</span>). Note that if <span class="math inline">\(v_x \geq v_{crit}\)</span>, then, necessarily <span class="math inline">\(\text{p-value} \leq \alpha\)</span>, these are exactly the same condition for rejecting the null hypothesis. In the past the typical approach was to choose one of a few standard <span class="math inline">\(\alpha\)</span> values, then find the critical statistic; this was the case because one could not calculate a p-values due to lack of computers, and folks relied on pre-calculated tables to find specific critical values. Nowadays, it is much more common to simply report the calculated p-value, because people can.</p>
<p>Choosing <span class="math inline">\(\alpha\)</span> is quite arbitrary, but according to tradition in psychology and social science, <span class="math inline">\(\alpha=0.05\)</span>. This tradition arose because Ronald Fisher said (paraphrasing), “Being wrong 1 out of 20 times seems fine.” Note that the standards in other fields are considerably higher, for instance, in physics, the standard for a discovery is “six-sigma” – an observation that is 6 standard deviations away from the average of the noise – which amounts to <span class="math inline">\(\alpha=10^{-9}=0.000000001\)</span>. There is hardly ever any reason to prefer an <span class="math inline">\(\alpha\)</span> value that is higher than 0.05 (that is, to decide that you are content with a higher than 1 in 20 chance of false positives), but there is often reason to choose a lower alpha value. When p-values are between 0.1 and 0.05, they are often called “marginally significant” and a number of other <a href="https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/">awkward phrases</a>.</p>
<p>There are other variations (involving the directionality of the test, but we will cover those later).</p>
<p>Ok, so let’s say we want to test the null hypothesis <span class="math inline">\(\mathcal{H}_0: D \sim \text{Bernoulli}(\theta_0 = 0.5)\)</span>, via the test statistic “number of heads” (<span class="math inline">\(v\)</span>). We have previously introduced the Binomial distribution, which, conveniently, gives us the sampling distribution of our test statistic under our null hypothesis. Recall that the <span class="math inline">\(\text{Binomial}(k|n,\theta)\)</span> distribution distributes probability over the number of successes <span class="math inline">\(k\)</span> out of <span class="math inline">\(n\)</span> attempts, with a probability of success <span class="math inline">\(\theta\)</span>.</p>
<p>So in our case:<br />
<span class="math inline">\(P_0(V=v) = P(K=k|n=10,\theta_0=0.5)=\text{Binomial}(k|n=10,\theta_0=0.5)\)</span>.</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="NHST.html#cb253-1" aria-hidden="true" tabindex="-1"></a>n.total <span class="ot">=</span> <span class="fu">length</span>(data)</span>
<span id="cb253-2"><a href="NHST.html#cb253-2" aria-hidden="true" tabindex="-1"></a>n.heads <span class="ot">=</span> <span class="fu">sum</span>(data<span class="sc">==</span><span class="st">&#39;H&#39;</span>)</span>
<span id="cb253-3"><a href="NHST.html#cb253-3" aria-hidden="true" tabindex="-1"></a>ks <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,n.total)</span>
<span id="cb253-4"><a href="NHST.html#cb253-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">n.heads =</span> ks, <span class="at">p.H0=</span><span class="fu">dbinom</span>(ks, n.total, <span class="fl">0.5</span>), <span class="at">geq=</span>ks<span class="sc">&gt;=</span>n.heads), </span>
<span id="cb253-5"><a href="NHST.html#cb253-5" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>n.heads, <span class="at">y=</span>p.H0, <span class="at">fill=</span>geq))<span class="sc">+</span><span class="fu">geom_bar</span>(<span class="at">stat=</span><span class="st">&quot;identity&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-128-1.png" width="3000" /></p>
<p>Let’s work through this graph. The x axis shows <span class="math inline">\(k\)</span>: the number of heads one might get from <span class="math inline">\(n\)</span> flips. The y axis shows the the probability of obtaining <span class="math inline">\(k\)</span> heads out of <span class="math inline">\(n=10\)</span> flips where each flip is heads with probability <span class="math inline">\(\theta_0=0.5\)</span>; in other words, this is our null hypothesis distribution for the test statistic <span class="math inline">\(k\)</span>. The colors of the bars indicate whether or not this value of <span class="math inline">\(k\)</span> is at least as large as the number we saw in our sample. The total probability of the bars where <span class="math inline">\(k\)</span> is at least as large as n.heads is the p-value for the one (upper) tailed null hypothesis test.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="NHST.html#cb254-1" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">dbinom</span>(ks[ks<span class="sc">&gt;=</span>n.heads],n.total,<span class="fl">0.5</span>))</span>
<span id="cb254-2"><a href="NHST.html#cb254-2" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>## [1] 0.0546875</code></pre>
<p>We can then compare the p.value to <span class="math inline">\(\alpha=0.05\)</span> to see if it is significant.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="NHST.html#cb256-1" aria-hidden="true" tabindex="-1"></a>p.value <span class="sc">&lt;=</span> <span class="fl">0.05</span></span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>Note that we did a one-tailed test, meaning we only looked for extreme outcomes on one side of the distribution (8 or more). A two-tailed test would also consider equally extreme outcomes on the other side (2 or fewer heads out of 10). A two tail test distributes the probability <span class="math inline">\(\alpha\)</span> to both tails (but this is somewhat tricky to do with potentially asymmetric binomial distributions, so let’s skip it for now).</p>
<p>The statistical test we just described is often called the <a href="https://en.wikipedia.org/wiki/Binomial_test">Binomial test</a> (obviously because it is based on the binomial distribution), which might be run with <code>binomial.test()</code> in R. While it is useful for testing null hypotheses about the probability underlying a bunch of data that comes from Bernoulli trials, it is used more often to test null hypotheses about the <a href="https://en.wikipedia.org/wiki/Sign_test">median or sign</a>.</p>
</div>
<div id="model-selection" class="section level3">
<h3>Model selection</h3>
<p>Thus far every individual analysis we have done has assumed a particular model for our data: for instance, we assumed that the coin flips are independent, but perhaps biased; or the converse: the coin flips are not independent, but unbiased with respect to heads/tails). How do we decide between these two models? This is the domain of model selection, and all model selection approaches have a similar structure: they find a tradeoff between how well a given model can fit the data and how complicated the model is.</p>
<p>At the start of the estimation section we proposed two different models of our data (<span class="math inline">\(\{H,H,H,H,H,H,H,H,T,T\}\)</span>); we now add a third:</p>
<p><span class="math inline">\(M_0\)</span>: The data are independent samples from a Bernoulli distribution with some unknown parameter <span class="math inline">\(\theta\)</span> (this is the intuitive model of coin-flips, so each one is independent of the others)</p>
<p><span class="math inline">\(M_1\)</span>: The data are sequentially dependent samples, the first one is H or T with probability 0.5, and <span class="math inline">\(x_i\)</span> is the same as <span class="math inline">\(x_{i-1}\)</span> with probability <span class="math inline">\(\theta\)</span>, and different with probability <span class="math inline">\((1-\theta)\)</span>. (This model makes sense if you suppose that the person flipping the coin is able to flip the coin an even number of times more often than an odd number of times – or vice versa).</p>
<p><span class="math inline">\(M_2\)</span>: The data are sequentially dependent samples, the first one is H or with probability <span class="math inline">\(\theta_1\)</span>, and <span class="math inline">\(x_i\)</span> is the same as <span class="math inline">\(x_{i-1}\)</span> with probability <span class="math inline">\(\theta_2\)</span>, and different with probability <span class="math inline">\((1-\theta_2)\)</span>.</p>
<p>Which model is a better description of our data?</p>
<div id="penalized-maximum-likelihood-selection-criteria" class="section level4">
<h4>Penalized (maximum) likelihood selection criteria</h4>
<p>First, let’s consider the <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">Akaike information criterion (AIC)</a> to choose a model. AIC is defined as:</p>
<p><span class="math inline">\(\operatorname{AIC} = 2m - 2 \log\left({\mathcal{L}(D \mid \hat \theta_{ML})}\right)\)</span></p>
<p>Where <span class="math inline">\(m\)</span> is the number of parameters (1 in <span class="math inline">\(M_0\)</span> and <span class="math inline">\(M_1\)</span>, and 2 in <span class="math inline">\(M_2\)</span>), <span class="math inline">\(\hat \theta_{ML}\)</span> is the maximum likelihood estimate of those parameters; therefore, <span class="math inline">\(\mathcal{L}(D \mid \hat \theta_{ML})\)</span> is the maximized likelihood of the data <span class="math inline">\(D\)</span> under the given model. The better model is the one with the lowest AIC value.</p>
<p>This model selection “criterion” has all the usual properties of model selection methods: it trades off model fit to the data (<span class="math inline">\(- 2 \log\left({\mathcal{L}(D \mid \hat \theta_{ML})}\right)\)</span>) while penalizing models with more parameters (<span class="math inline">\(2m\)</span>).</p>
<p>We will write out the likelihood functions for the three models.</p>
<p>For <span class="math inline">\(M_0\)</span>:</p>
<p><span class="math inline">\(\mathcal{L}_0(X \mid \theta) = \prod\limits_{i=1}^n P(x_i \mid \theta) = \theta^k(1-\theta)^{n-k}\)</span>, where <span class="math inline">\(k\)</span> is the number of heads, and <span class="math inline">\(n\)</span> is the length of the sequence.</p>
<p>For <span class="math inline">\(M_1\)</span>:</p>
<p><span class="math inline">\(\mathcal{L}_1(X \mid \theta) = 0.5\prod\limits_{i=2}^n P(x_i \mid \theta,x_{i-1}) = 0.5\theta^k(1-\theta)^{n-k-1}\)</span>, where <span class="math inline">\(k\)</span> is the number of repetitions, and <span class="math inline">\(n\)</span> is the length of the sequence.</p>
<p>For <span class="math inline">\(M_2\)</span>:</p>
<p><span class="math inline">\(\mathcal{L}_2(X \mid \theta_1,\theta_2) = P(x_1|\theta_1)\prod\limits_{i=2}^n P(x_i \mid \theta_2,x_{i-1}) = \theta_1^a(1-\theta_1)^{1-a}\theta_2^k(1-\theta_2)^{n-k-1}\)</span>, where <span class="math inline">\(a=1\)</span> is the first coin of the sequence is heads, and 0 otherwise; <span class="math inline">\(k\)</span> is the number of repetitions; and <span class="math inline">\(n\)</span> is the length of the sequence.</p>
<p>In the interest of space, I will not write out the exercise of finding the maximum likelihood parameters for these models, but they are:</p>
<p><span class="math inline">\(M_0: \hat \theta_{ML} = 0.8\)</span>;<br />
<span class="math inline">\(M_1: \hat \theta_{ML} = 8/9 = 0.\bar8\)</span>;<br />
<span class="math inline">\(M_2: \hat \theta_{ML} = [1.0, 0.\bar8]\)</span></p>
<p>We can compute the AIC values for each model:\
<span class="math inline">\(\operatorname{AIC}_0 = 2(1)-2\log \mathcal{L}_0(D \mid \hat \theta_{ML}) = 2-2 \log (0.00671) = 12\)</span><br />
<span class="math inline">\(\operatorname{AIC}_1 = 2(1)-2\log \mathcal{L}_1(D \mid \hat \theta_{ML}) = 2-2 \log (0.02165) = 9.7\)</span><br />
<span class="math inline">\(\operatorname{AIC}_2 = 2(2)-2\log \mathcal{L}_2(D \mid \hat \theta_{ML}) = 4-2 \log (0.0433) = 10.3\)</span></p>
<p>So of the three models proposed, <span class="math inline">\(M_1\)</span> wins, in the sense that it has the lowest AIC value. So, our data are best described as sequentially dependent; however, the increase in maximized likelihood obtained from the more complicated <span class="math inline">\(M_2\)</span> does not sufficiently offset the extra parameter. This is generally how penalized maximum likelihood model selection works (e.g., <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a> differs only in that it has a slightly different magnitude of penalty for each additional parameter).</p>
</div>
<div id="bayesian-model-selection-and-bayes-factors" class="section level4">
<h4>Bayesian model selection (and Bayes Factors)</h4>
<p>Bayesian model selection works a bit differently than penalized maximum likelihood criteria. Instead of adopting an ad-hoc penalty for extra parameters, we will calculate the <em>marginal likelihood</em> of the model, by marginalizing over parameters. This requires specifying a <em>prior</em> about the parameters for each model.</p>
<p>Specifically, we wish to calculate:</p>
<p><span class="math inline">\(P(M_i \mid D) = \frac{P(D \mid M_i)P(M_i)}{P(D)}\)</span></p>
<p>To do so, we need to calculate the <em>marginal likelihood</em> of the data, marginalizing over parameters:</p>
<p><span class="math inline">\(P(D \mid M_i) = \int \mathcal{L}_i(D \mid \theta,M_i) P(\theta \mid M_i) d\theta\)</span></p>
<p>And to calculate this, we need to specify the prior distribution over model parameters:</p>
<p><span class="math inline">\(P(\theta \mid M_i)\)</span></p>
<p>So let’s specify some very vague priors (uniform on all <span class="math inline">\(\theta\)</span>s):</p>
<p><span class="math inline">\(P(\theta \mid M_i) = \operatorname{Uniform}(0,1)\)</span></p>
<p>While we can calculate the marginal likelihood analytically, let’s do it numerically instead:</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="NHST.html#cb258-1" aria-hidden="true" tabindex="-1"></a>p.theta <span class="ot">=</span> <span class="cf">function</span>(theta){<span class="fu">dunif</span>(theta,<span class="dv">0</span>,<span class="dv">1</span>)}</span>
<span id="cb258-2"><a href="NHST.html#cb258-2" aria-hidden="true" tabindex="-1"></a>d.theta <span class="ot">=</span> <span class="fl">0.001</span></span>
<span id="cb258-3"><a href="NHST.html#cb258-3" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span>d.theta)</span>
<span id="cb258-4"><a href="NHST.html#cb258-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb258-5"><a href="NHST.html#cb258-5" aria-hidden="true" tabindex="-1"></a>p.M <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;M0&quot;</span><span class="ot">=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>, <span class="st">&quot;M1&quot;</span><span class="ot">=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>, <span class="st">&quot;M2&quot;</span><span class="ot">=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>)</span>
<span id="cb258-6"><a href="NHST.html#cb258-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb258-7"><a href="NHST.html#cb258-7" aria-hidden="true" tabindex="-1"></a>p.theta.M0 <span class="ot">=</span> <span class="fu">p.theta</span>(theta)</span>
<span id="cb258-8"><a href="NHST.html#cb258-8" aria-hidden="true" tabindex="-1"></a>likelihood.M0 <span class="ot">=</span> <span class="cf">function</span>(theta){theta<span class="sc">^</span><span class="dv">8</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">2</span>}</span>
<span id="cb258-9"><a href="NHST.html#cb258-9" aria-hidden="true" tabindex="-1"></a>p.data.theta_M0 <span class="ot">=</span> <span class="fu">vapply</span>(theta, likelihood.M0, <span class="fu">c</span>(<span class="st">&quot;lik&quot;</span> <span class="ot">=</span> <span class="dv">0</span>))</span>
<span id="cb258-10"><a href="NHST.html#cb258-10" aria-hidden="true" tabindex="-1"></a>p.data.M0 <span class="ot">=</span> <span class="fu">sum</span>(p.theta.M0<span class="sc">*</span>p.data.theta_M0<span class="sc">*</span>d.theta)</span>
<span id="cb258-11"><a href="NHST.html#cb258-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb258-12"><a href="NHST.html#cb258-12" aria-hidden="true" tabindex="-1"></a>p.theta.M1 <span class="ot">=</span> <span class="fu">p.theta</span>(theta)</span>
<span id="cb258-13"><a href="NHST.html#cb258-13" aria-hidden="true" tabindex="-1"></a>likelihood.M1 <span class="ot">=</span> <span class="cf">function</span>(theta){<span class="fl">0.5</span><span class="sc">*</span>theta<span class="sc">^</span><span class="dv">8</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">1</span>}</span>
<span id="cb258-14"><a href="NHST.html#cb258-14" aria-hidden="true" tabindex="-1"></a>p.data.theta_M1 <span class="ot">=</span> <span class="fu">vapply</span>(theta, likelihood.M1, <span class="fu">c</span>(<span class="st">&quot;lik&quot;</span> <span class="ot">=</span> <span class="dv">0</span>))</span>
<span id="cb258-15"><a href="NHST.html#cb258-15" aria-hidden="true" tabindex="-1"></a>p.data.M1 <span class="ot">=</span> <span class="fu">sum</span>(p.theta.M1<span class="sc">*</span>p.data.theta_M1<span class="sc">*</span>d.theta)</span>
<span id="cb258-16"><a href="NHST.html#cb258-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb258-17"><a href="NHST.html#cb258-17" aria-hidden="true" tabindex="-1"></a>p.theta_1.M2 <span class="ot">=</span> <span class="fu">p.theta</span>(theta)</span>
<span id="cb258-18"><a href="NHST.html#cb258-18" aria-hidden="true" tabindex="-1"></a>p.theta_2.M2 <span class="ot">=</span> <span class="fu">p.theta</span>(theta)</span>
<span id="cb258-19"><a href="NHST.html#cb258-19" aria-hidden="true" tabindex="-1"></a>p.theta.M2 <span class="ot">=</span> <span class="fu">outer</span>(p.theta_1.M2, p.theta_2.M2, <span class="cf">function</span>(a,b){a<span class="sc">*</span>b})</span>
<span id="cb258-20"><a href="NHST.html#cb258-20" aria-hidden="true" tabindex="-1"></a>likelihood.M2 <span class="ot">=</span> <span class="cf">function</span>(theta_1, theta_2){theta_1<span class="sc">^</span><span class="dv">1</span><span class="sc">*</span>theta_2<span class="sc">^</span><span class="dv">8</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta_2)<span class="sc">^</span><span class="dv">1</span>}</span>
<span id="cb258-21"><a href="NHST.html#cb258-21" aria-hidden="true" tabindex="-1"></a>p.data.theta_M2 <span class="ot">=</span> <span class="fu">outer</span>(theta, theta, likelihood.M2)</span>
<span id="cb258-22"><a href="NHST.html#cb258-22" aria-hidden="true" tabindex="-1"></a>p.data.M2 <span class="ot">=</span> <span class="fu">sum</span>(p.theta.M2<span class="sc">*</span>p.data.theta_M2<span class="sc">*</span>d.theta<span class="sc">*</span>d.theta)</span>
<span id="cb258-23"><a href="NHST.html#cb258-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb258-24"><a href="NHST.html#cb258-24" aria-hidden="true" tabindex="-1"></a>p.data.M <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;M0&quot;</span><span class="ot">=</span>p.data.M0, <span class="st">&quot;M1&quot;</span><span class="ot">=</span>p.data.M1, <span class="st">&quot;M2&quot;</span><span class="ot">=</span>p.data.M2)</span>
<span id="cb258-25"><a href="NHST.html#cb258-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb258-26"><a href="NHST.html#cb258-26" aria-hidden="true" tabindex="-1"></a>p.data <span class="ot">=</span> <span class="fu">sum</span>(p.M<span class="sc">*</span>p.data.M)</span>
<span id="cb258-27"><a href="NHST.html#cb258-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb258-28"><a href="NHST.html#cb258-28" aria-hidden="true" tabindex="-1"></a>(<span class="at">p.M.data =</span> p.M<span class="sc">*</span>p.data.M<span class="sc">/</span>p.data)</span></code></pre></div>
<pre><code>##        M0        M1        M2 
## 0.1537821 0.4228975 0.4233204</code></pre>
<p>This kinds of Bayesian model selection is hard because calculating <span class="math inline">\(P(D|M)\)</span> – the likelihood of the data while marginalizing over possible parameter values under that model – is hard.</p>
<div id="bayes-factors" class="section level5">
<h5>Bayes Factors</h5>
<p>Often Bayesian model selection is used to compare just two hypotheses: some null <span class="math inline">\(H_0\)</span>, which has an assumption of an effect of 0, and some alternative <span class="math inline">\(H_1\)</span> in which the effect size is a parameter.</p>
<p>For instance, we might say that the two models are:</p>
<p><span class="math inline">\(H_0\)</span>: The data are independent samples from a Bernoulli distribution with <span class="math inline">\(\theta=0.5\)</span> (this is the null hypothesis postulating that the coin has zero bias).</p>
<p><span class="math inline">\(H_1\)</span>: The data are independent samples from a Bernoulli distribution with <span class="math inline">\(\theta\)</span> free to vary (this is the alternate hypothesis in which the coin <em>can</em> have some bias).</p>
<p>The logic of Bayes factors is that if we consider the ratio of the posterior probability of <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_0\)</span>, the calculation can be broken up into the ratio of the priors, and the ratio of the (marginal) likelihoods:</p>
<p><span class="math display">\[\frac{P(H_1 \mid D)}{P(H_0 \mid D)} = \frac{P(D  \mid H_1)}{P(D \mid H_0)} \frac{P(H_1)}{P(H_0)}\]</span></p>
<p>Since we want to know what kind of evidence in favor of <span class="math inline">\(H_1\)</span> (or <span class="math inline">\(H_0\)</span>) our data offer, we are only interested in the ratio of the marginal likelihoods. This ratio is called the Bayes factor:</p>
<p><span class="math display">\[\operatorname{BF} = \frac{P(D  \mid H_1)}{P(D \mid H_0)} = \frac{\int \mathcal{L}(D \mid \theta,H_1) P(\theta \mid H_1)d\theta}{\int \mathcal{L}(D \mid \theta,H_0) P(\theta \mid H_0)d\theta}\]</span></p>
<p>Again, this is generally hard to calculate, because it is hard to calculate the marginal likelihood. However, here we can do so numerically:</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="NHST.html#cb260-1" aria-hidden="true" tabindex="-1"></a>d.theta <span class="ot">=</span> <span class="fl">0.001</span></span>
<span id="cb260-2"><a href="NHST.html#cb260-2" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span>d.theta)</span>
<span id="cb260-3"><a href="NHST.html#cb260-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-4"><a href="NHST.html#cb260-4" aria-hidden="true" tabindex="-1"></a>p.data.H0 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">^</span><span class="dv">8</span><span class="sc">*</span><span class="fl">0.5</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb260-5"><a href="NHST.html#cb260-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-6"><a href="NHST.html#cb260-6" aria-hidden="true" tabindex="-1"></a>p.theta.H1 <span class="ot">=</span> <span class="fu">dunif</span>(theta,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb260-7"><a href="NHST.html#cb260-7" aria-hidden="true" tabindex="-1"></a>likelihood.H1 <span class="ot">=</span> <span class="cf">function</span>(theta){theta<span class="sc">^</span><span class="dv">8</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">2</span>}</span>
<span id="cb260-8"><a href="NHST.html#cb260-8" aria-hidden="true" tabindex="-1"></a>p.data.theta_H1 <span class="ot">=</span> <span class="fu">vapply</span>(theta, likelihood.H1, <span class="fu">c</span>(<span class="st">&quot;lik&quot;</span> <span class="ot">=</span> <span class="dv">0</span>))</span>
<span id="cb260-9"><a href="NHST.html#cb260-9" aria-hidden="true" tabindex="-1"></a>p.data.H1 <span class="ot">=</span> <span class="fu">sum</span>(p.theta.H1<span class="sc">*</span>p.data.theta_H1<span class="sc">*</span>d.theta)</span>
<span id="cb260-10"><a href="NHST.html#cb260-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-11"><a href="NHST.html#cb260-11" aria-hidden="true" tabindex="-1"></a>(<span class="at">bayesfactor =</span> p.data.H1<span class="sc">/</span>p.data.H0)</span></code></pre></div>
<pre><code>## [1] 2.068687</code></pre>
<p>This bayes factor (about 2), is generally not considered to be sufficiently strong evidence in favor of the alternate hypothesis (the prescribed cutoffs can be found <a href="https://en.wikipedia.org/wiki/Bayes_factor">here</a>)</p>

</div>
</div>
</div>
</div>
<div id="t-distribution" class="section level2">
<h2>t-distribution</h2>
<div id="tl-dr.-1" class="section level3">
<h3>TL; DR.</h3>
<ul>
<li><p>When using the sample standard deviation to define a test statistic, the sampling variability of the sample sd. will influence the distribution of the test statistic under the null.</p></li>
<li><p>Consequently, we can’t use the normal distribution if we use the sample sd. Instead we use the t distribution, (<code>dt</code>, <code>pt</code>, <code>qt</code>, with the degrees of freedom used to estimate the sample sd)</p></li>
</ul>
</div>
<div id="sampling-distribution-of-sample-variance-and-t-statistic" class="section level3">
<h3>Sampling distribution of sample variance, and t-statistic</h3>
<p>Ok, so suppose we no longer know what the population standard deviation ought to be under the null hypothesis. This is nearly always the case in practice.</p>
<p>For instance, we might measure the math GRE scores of folks in our class, and aim to test whether or not those GRE scores are distributed with a mean different from 500. We don’t know what the standard deviation ought to be, so we cannot use a Z test. Instead we must use a T-test.</p>
</div>
<div id="sample-variance" class="section level3">
<h3>Sample variance</h3>
<p>Recall that the sample variance is defined as:</p>
<p><span class="math inline">\(s^2_x = \frac{1}{n-1}\sum\limits_{i=1}^n{(x_i-\bar x)^2}\)</span></p>
<p>You would reasonably ask: <strong>why are we dividing by <span class="math inline">\((n-1)\)</span>?</strong></p>
<p><em>The short answer</em>: because if you used <span class="math inline">\(n\)</span>, your sample variance would tend to underestimate the population variance; however, with the <span class="math inline">\((n-1)\)</span> correction, ensures that the sample variance is not biased.</p>
<p><em>The longer answer</em>: The sample variance is defined as the variance around the sample mean; however, the sample mean is calculated so as to <em>minimize</em> the sum of squared deviations of the data points from it; in other words, to minimize the sample variance. Therefore, without the <span class="math inline">\((n-1)\)</span> correction, the sample variance would be biased by our calculation of sample mean. Since we compute the sample mean based on the data, and then use the same data to calculate the sample variance, our calculation of sample variance has fewer <strong>degrees of freedom</strong> than our calculation of the sample mean. The degrees of freedom refers to how much we have constrained our calculation. When we calculate the sample mean, we have not constrained any of the parameters of our calculation based on the data, therefore all the data points are “free to vary.” So for the calculation of sample mean, our degrees of freedom is the number of data points <span class="math inline">\(df = n\)</span>. However, when we calculate the sample variance, we have already calculated the sample mean, and the sample variance calculation is <em>conditioned</em> on the value of the sample mean (since we calculate squared deviations from the sample mean). Consequently, we have constrained the calculation by one parameter. This means that conditioned on our estimated sample mean, all but one of the data points are free to vary (in other words, the nth data point can be calculated from the first n-1 data points and the sample mean, so it is not free to vary) – thus we have <span class="math inline">\(n-1\)</span> degrees of freedom, and using that in the calculation of the sample variance appropriately corrects the bias.</p>
<p><em>The mathematical answer</em>: If this text-based explanation seems convoluted and confusing, that is because it is. For those that are comfortable with math, the wikipedia page on Variance works through the calculation of the expected bias of the sample variance calculated using <span class="math inline">\(n\)</span>, and shows that using <span class="math inline">\(n-1\)</span> <a href="https://en.wikipedia.org/wiki/Variance#Sample_variance">yields an unbiased estimate of variance</a>.</p>
</div>
<div id="sampling-distribution-of-the-sample-variance" class="section level3">
<h3>Sampling distribution of the sample variance</h3>
<p>If we take multiple samples of size <span class="math inline">\(n\)</span> from some population, and compute the sample mean of each, the sample mean will vary from one sample to the next. The variation of the sample mean across samples (when they all come from the same distribution) is known as the sampling distribution of the sample mean, and it follows a normal distribution with a standard deviation known as the “standard error of the mean” (see <a href="nhst-basics-normal.html">notes on normal statistics</a> ). Similarly, the sample variance will vary from one sample to another. We need not worry now about the details of the sampling distribution of the sample variance, but it suffices to say: it will vary around the population variance.</p>
<p>We can simulate the sampling distribution of the sample standard deviation:</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="NHST.html#cb262-1" aria-hidden="true" tabindex="-1"></a>sample.n <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">rnorm</span>(n, <span class="dv">100</span>, <span class="dv">15</span>)}</span>
<span id="cb262-2"><a href="NHST.html#cb262-2" aria-hidden="true" tabindex="-1"></a>df.sample.var <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">sample.variance=</span><span class="fu">replicate</span>(<span class="dv">10000</span>,<span class="fu">sd</span>(<span class="fu">sample.n</span>(<span class="dv">10</span>))), <span class="at">n=</span><span class="st">&quot;10&quot;</span>),</span>
<span id="cb262-3"><a href="NHST.html#cb262-3" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">data.frame</span>(<span class="at">sample.variance=</span><span class="fu">replicate</span>(<span class="dv">10000</span>,<span class="fu">sd</span>(<span class="fu">sample.n</span>(<span class="dv">50</span>))), <span class="at">n=</span><span class="st">&quot;50&quot;</span>),</span>
<span id="cb262-4"><a href="NHST.html#cb262-4" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">data.frame</span>(<span class="at">sample.variance=</span><span class="fu">replicate</span>(<span class="dv">10000</span>,<span class="fu">sd</span>(<span class="fu">sample.n</span>(<span class="dv">250</span>))), <span class="at">n=</span><span class="st">&quot;250&quot;</span>))</span>
<span id="cb262-5"><a href="NHST.html#cb262-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df.sample.var, <span class="fu">aes</span>(<span class="at">x=</span>sample.variance))<span class="sc">+</span><span class="fu">facet_grid</span>(.<span class="sc">~</span>n)<span class="sc">+</span></span>
<span id="cb262-6"><a href="NHST.html#cb262-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill=</span><span class="st">&quot;darkblue&quot;</span>)<span class="sc">+</span></span>
<span id="cb262-7"><a href="NHST.html#cb262-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">15</span>, <span class="at">size=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;red&quot;</span>)<span class="sc">+</span> <span class="co"># true population variance.</span></span>
<span id="cb262-8"><a href="NHST.html#cb262-8" aria-hidden="true" tabindex="-1"></a>  my_theme <span class="co"># consult visualization notes on how to make a plot theme.</span></span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-133-1.png" width="3000" /></p>
<p>In other words, sometimes the sample standard deviation (<span class="math inline">\(s_x\)</span>) will be bigger than the population standard deviation (<span class="math inline">\(\sigma_x\)</span>), and sometimes smaller. The smaller our sample size, the more variable the sample standard deviation will be around the population standard deviation.</p>
</div>
<div id="sampling-distribution-of-t-statistic" class="section level3">
<h3>Sampling distribution of t-statistic</h3>
<p>Because the sample standard deviation is subject to sampling variability, if we calculate the equivalent of a Z-statistic, but using the sample standard deviation instead of the population standard deviation:</p>
<p><span class="math inline">\(t_{\bar x} = \frac{(\bar x-mu)}{s_x / \sqrt n}\)</span></p>
<p>this statistic will <em>not</em> follow the standard normal (Z) distribution. This is entirely because our sample standard deviation also tends to vary from sample to sample.</p>
<p>The sampling distribution of this <strong>“t”</strong> statistic reflects the variation of both the sample mean as well as the sample variance. The sampling distribution of the t statistic is effectively a weighted mixture of many gaussian distributions, each with a different standard deviation (reflecting the sampling distribution of the sample variance). This is known as the <strong>(Student’s) T distribution</strong>. And this is the distribution we will be using to calculate null hypothesis tests and confidence intervals in situations when we must estimated the population standard deviation from the sample.</p>
<p>We can simulate the distribution of a t-statistic, and compare it to the standard normal (Z) distribution.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="NHST.html#cb263-1" aria-hidden="true" tabindex="-1"></a>mu.x <span class="ot">=</span> <span class="dv">100</span>  <span class="co"># population value</span></span>
<span id="cb263-2"><a href="NHST.html#cb263-2" aria-hidden="true" tabindex="-1"></a>sd.x <span class="ot">=</span> <span class="dv">15</span>   <span class="co"># population value</span></span>
<span id="cb263-3"><a href="NHST.html#cb263-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">3</span>       <span class="co"># small sample size to highlight t distribution tails</span></span>
<span id="cb263-4"><a href="NHST.html#cb263-4" aria-hidden="true" tabindex="-1"></a>sample.n <span class="ot">=</span> <span class="cf">function</span>(n){<span class="fu">rnorm</span>(n, mu.x, sd.x)}</span>
<span id="cb263-5"><a href="NHST.html#cb263-5" aria-hidden="true" tabindex="-1"></a>calculate.t <span class="ot">=</span> <span class="cf">function</span>(x){(<span class="fu">mean</span>(x) <span class="sc">-</span> mu.x)<span class="sc">/</span>(<span class="fu">sd</span>(x)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(x)))} <span class="co"># uses sample sd</span></span>
<span id="cb263-6"><a href="NHST.html#cb263-6" aria-hidden="true" tabindex="-1"></a>calculate.z <span class="ot">=</span> <span class="cf">function</span>(x){(<span class="fu">mean</span>(x) <span class="sc">-</span> mu.x)<span class="sc">/</span>(sd.x <span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(x)))} <span class="co"># uses population sd</span></span>
<span id="cb263-7"><a href="NHST.html#cb263-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb263-8"><a href="NHST.html#cb263-8" aria-hidden="true" tabindex="-1"></a>sample.ts <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">calculate.t</span>(<span class="fu">sample.n</span>(n)))</span>
<span id="cb263-9"><a href="NHST.html#cb263-9" aria-hidden="true" tabindex="-1"></a>sample.zs <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">calculate.z</span>(<span class="fu">sample.n</span>(n)))</span>
<span id="cb263-10"><a href="NHST.html#cb263-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb263-11"><a href="NHST.html#cb263-11" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">value=</span>sample.ts, <span class="at">statistic=</span><span class="st">&quot;T&quot;</span>),</span>
<span id="cb263-12"><a href="NHST.html#cb263-12" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">value=</span>sample.zs, <span class="at">statistic=</span><span class="st">&quot;Z&quot;</span>))</span>
<span id="cb263-13"><a href="NHST.html#cb263-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb263-14"><a href="NHST.html#cb263-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>value, <span class="at">fill=</span>statistic))<span class="sc">+</span></span>
<span id="cb263-15"><a href="NHST.html#cb263-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position=</span><span class="st">&quot;identity&quot;</span>, <span class="at">binwidth=</span><span class="fl">0.2</span>, <span class="at">alpha=</span><span class="fl">0.5</span>)<span class="sc">+</span></span>
<span id="cb263-16"><a href="NHST.html#cb263-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>))<span class="sc">+</span></span>
<span id="cb263-17"><a href="NHST.html#cb263-17" aria-hidden="true" tabindex="-1"></a>  my_theme <span class="co"># consult visualization notes on how to make a plot theme.</span></span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-134-1.png" width="3000" /></p>
<p>Notice that with our small sample size (3), the tails of the t-distribution are considerably “heavier” than those of the standard normal distribution. With small ns, the standard deviation of the t distribution is larger than 1, and it has a kurtosis much greater than that of a normal.</p>
<p>We can see the deviation of the T-distribution from a standard normal distribution much more clearly using <a href="qq-plots.html">QQ plots</a>:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="NHST.html#cb264-1" aria-hidden="true" tabindex="-1"></a>qs <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb264-2"><a href="NHST.html#cb264-2" aria-hidden="true" tabindex="-1"></a>df.qs <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">quantile.P =</span> qs,</span>
<span id="cb264-3"><a href="NHST.html#cb264-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">q.val.Normal =</span> <span class="fu">qnorm</span>(qs),</span>
<span id="cb264-4"><a href="NHST.html#cb264-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">q.val.t =</span> <span class="fu">quantile</span>(sample.ts,qs),</span>
<span id="cb264-5"><a href="NHST.html#cb264-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">q.val.z =</span> <span class="fu">quantile</span>(sample.zs,qs))</span>
<span id="cb264-6"><a href="NHST.html#cb264-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df.qs, <span class="fu">aes</span>(q.val.Normal, q.val.z))<span class="sc">+</span></span>
<span id="cb264-7"><a href="NHST.html#cb264-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">cex=</span><span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb264-8"><a href="NHST.html#cb264-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&#39;blue&#39;</span>, <span class="at">size=</span><span class="fl">0.75</span>)<span class="sc">+</span></span>
<span id="cb264-9"><a href="NHST.html#cb264-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y=</span>q.val.t), <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">cex=</span><span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb264-10"><a href="NHST.html#cb264-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>q.val.t), <span class="at">col=</span><span class="st">&#39;red&#39;</span>, <span class="at">size=</span><span class="fl">0.75</span>)<span class="sc">+</span></span>
<span id="cb264-11"><a href="NHST.html#cb264-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">position=</span><span class="st">&quot;identity&quot;</span>)<span class="sc">+</span></span>
<span id="cb264-12"><a href="NHST.html#cb264-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Standard normal quantile&quot;</span>)<span class="sc">+</span></span>
<span id="cb264-13"><a href="NHST.html#cb264-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Sampling dist of statistic quantile&quot;</span>)<span class="sc">+</span></span>
<span id="cb264-14"><a href="NHST.html#cb264-14" aria-hidden="true" tabindex="-1"></a>  my_theme</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-135-1.png" width="1500" /></p>
<p>The blue line corresponds to the qq-plot for sample z statistics, and the red line for sampled t statistics, compared to the standard normal distribution. This display highlights that the heavy tails of the sampling distribution of the t statistic have a huge impact on extreme quantiles: the z statistic corresponding to the 98th percentile is about <code>qnorm(0.98)</code>=2, but the corresponding 0.98 quantile for a t distribution with n=3 is about <code>qt(0.98,2)</code>=5. Likewise, while a statistic of 2 corresponds to the <code>pnorm(2)</code>=98th percentile of the z distribution, it is the <code>pt(2,2)</code>=91st percentile of the t distribution. Consequently, if we define a 95% confidence interval as <span class="math inline">\(\pm 1.96 s_{\bar x}\)</span> (as we would using the z distribution), we only get a <code>1-2*pt(-1.96,2)</code>=81% confidence interval out. Similarly, if we reject a t-statistic based on a critical z value chosen for <span class="math inline">\(\alpha=0.05\)</span>, our false-alarm rate will actually be 19%.</p>
</div>
<div id="t-distribution-1" class="section level3">
<h3>T-distribution</h3>
<p>Consequently, the sampling distribution of the t-statistic is not a standard normal, but is a T distribution with a “degrees of freedom” parameter:</p>
<p><span class="math inline">\(df = \nu = n-1\)</span> (this is pronounced “nu,” but normally we will just refer to it as degrees of freedom)</p>
<p>We can show that the sampled t statistics follow this t-distribution by looking at the qq-plot comparing sampled t statistics to the theoretical t distribution:</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="NHST.html#cb265-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">value=</span>sample.ts, <span class="at">what=</span><span class="st">&quot;sample t stats&quot;</span>),</span>
<span id="cb265-2"><a href="NHST.html#cb265-2" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">value=</span><span class="fu">rt</span>(<span class="fu">length</span>(sample.ts), n<span class="dv">-1</span>), <span class="at">what=</span><span class="st">&quot;t distribution samples&quot;</span>))</span>
<span id="cb265-3"><a href="NHST.html#cb265-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb265-4"><a href="NHST.html#cb265-4" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">=</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>value, <span class="at">fill=</span>what))<span class="sc">+</span></span>
<span id="cb265-5"><a href="NHST.html#cb265-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position=</span><span class="st">&quot;identity&quot;</span>, <span class="at">binwidth=</span><span class="fl">0.2</span>, <span class="at">alpha=</span><span class="fl">0.5</span>)<span class="sc">+</span></span>
<span id="cb265-6"><a href="NHST.html#cb265-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>))<span class="sc">+</span></span>
<span id="cb265-7"><a href="NHST.html#cb265-7" aria-hidden="true" tabindex="-1"></a>  my_theme <span class="co"># consult visualization notes on how to make a plot theme.</span></span>
<span id="cb265-8"><a href="NHST.html#cb265-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb265-9"><a href="NHST.html#cb265-9" aria-hidden="true" tabindex="-1"></a>qs <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb265-10"><a href="NHST.html#cb265-10" aria-hidden="true" tabindex="-1"></a>df.qs <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">quantile.P =</span> qs,</span>
<span id="cb265-11"><a href="NHST.html#cb265-11" aria-hidden="true" tabindex="-1"></a>                   <span class="at">q.val.t.theoretical =</span> <span class="fu">qt</span>(qs,n<span class="dv">-1</span>),</span>
<span id="cb265-12"><a href="NHST.html#cb265-12" aria-hidden="true" tabindex="-1"></a>                   <span class="at">q.val.t.samples =</span> <span class="fu">quantile</span>(sample.ts,qs))</span>
<span id="cb265-13"><a href="NHST.html#cb265-13" aria-hidden="true" tabindex="-1"></a>g2 <span class="ot">=</span> <span class="fu">ggplot</span>(df.qs, <span class="fu">aes</span>(q.val.t.theoretical, q.val.t.samples))<span class="sc">+</span></span>
<span id="cb265-14"><a href="NHST.html#cb265-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">cex=</span><span class="dv">2</span>)<span class="sc">+</span></span>
<span id="cb265-15"><a href="NHST.html#cb265-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&#39;red&#39;</span>, <span class="at">size=</span><span class="fl">0.75</span>)<span class="sc">+</span></span>
<span id="cb265-16"><a href="NHST.html#cb265-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">position=</span><span class="st">&quot;identity&quot;</span>)<span class="sc">+</span></span>
<span id="cb265-17"><a href="NHST.html#cb265-17" aria-hidden="true" tabindex="-1"></a>  my_theme</span>
<span id="cb265-18"><a href="NHST.html#cb265-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb265-19"><a href="NHST.html#cb265-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb265-20"><a href="NHST.html#cb265-20" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(g1,g2,<span class="at">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-136-1.png" width="3000" /></p>
</div>
<div id="degrees-of-freedom." class="section level3">
<h3>Degrees of freedom.</h3>
<p>As our sample size increases, our degrees of freedom increase. This means that the sampling distribution of the sample variance has less variability, and as a consequence, the t-distribution looks more normal.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="NHST.html#cb266-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb266-2"><a href="NHST.html#cb266-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">data.frame</span>(<span class="at">t=</span>t, <span class="at">pt=</span><span class="fu">dnorm</span>(t), <span class="at">df=</span><span class="st">&quot;Normal&quot;</span>),</span>
<span id="cb266-3"><a href="NHST.html#cb266-3" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">t=</span>t, <span class="at">pt=</span><span class="fu">dt</span>(t,<span class="dv">100</span>), <span class="at">df=</span><span class="st">&quot;100&quot;</span>),</span>
<span id="cb266-4"><a href="NHST.html#cb266-4" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">t=</span>t, <span class="at">pt=</span><span class="fu">dt</span>(t,<span class="dv">30</span>), <span class="at">df=</span><span class="st">&quot;30&quot;</span>),</span>
<span id="cb266-5"><a href="NHST.html#cb266-5" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">t=</span>t, <span class="at">pt=</span><span class="fu">dt</span>(t,<span class="dv">10</span>), <span class="at">df=</span><span class="st">&quot;10&quot;</span>),</span>
<span id="cb266-6"><a href="NHST.html#cb266-6" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">t=</span>t, <span class="at">pt=</span><span class="fu">dt</span>(t,<span class="dv">3</span>), <span class="at">df=</span><span class="st">&quot;5&quot;</span>),</span>
<span id="cb266-7"><a href="NHST.html#cb266-7" aria-hidden="true" tabindex="-1"></a>           <span class="fu">data.frame</span>(<span class="at">t=</span>t, <span class="at">pt=</span><span class="fu">dt</span>(t,<span class="dv">1</span>), <span class="at">df=</span><span class="st">&quot;1&quot;</span>))</span>
<span id="cb266-8"><a href="NHST.html#cb266-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x=</span>t,<span class="at">y=</span>pt,<span class="at">color=</span><span class="fu">as.factor</span>(df)))<span class="sc">+</span></span>
<span id="cb266-9"><a href="NHST.html#cb266-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">position=</span><span class="st">&quot;identity&quot;</span>, <span class="at">size=</span><span class="dv">1</span>)<span class="sc">+</span></span>
<span id="cb266-10"><a href="NHST.html#cb266-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>))<span class="sc">+</span></span>
<span id="cb266-11"><a href="NHST.html#cb266-11" aria-hidden="true" tabindex="-1"></a>  my_theme</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-137-1.png" width="3000" /></p>
<p>As we see, when n is larger than about 30, the t distribution is pretty close to the normal distribution, so its deviation won’t matter for all but the most extreme quantiles.</p>
</div>
<div id="summary." class="section level3">
<h3>Summary.</h3>
<p>In short, when we use the <em>sample</em> standard deviation to calculate a statistic, (as we usually will), we use the <em>t distribution</em> with <span class="math inline">\(n-1\)</span> degrees of freedom rather than the standard normal distribution.</p>
<p>In R, we can get the T distribution’s density, cumulative probability, quantile, and random samples with <code>dt(x,df)</code>, <code>pt(x,df)</code>, <code>qt(q,df)</code>, and <code>rt(n,df)</code>, respectively.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="t-tests.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/notes/nhst-simulation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
