---
output: 
    tufterhandout::html_tufte_handout
---

# Experiments

Experiments aim to test or constrain theories by manipulating some variables in the world (typically called "independent variables"), and seeing how these manipulations change the outcome measurements ("dependent variables").  

## Causal inference and the logic of experiments.

Let's consider the relationship between height and annual income.  

Most data analyses / experiments / surveys include some response/outcome variables (that is -- the measurements that you care about; here: annual income) and some explanatory variables, that we think might be related to the response variable (here, height).  Which variable we treat as explanatory, and which as response, will determine our choice of data analysis, but this choice should also depend on where the data came from.  

**Observational and survey data:**    
The easiest way to get data is though passive observation.  One might conduct an *observational* study in which the researcher sits back and observes some process in the world unfold, and records those observations (e.g., recording the heights and incomes of people I may observe as a bank-teller).  In a *survey* the researcher has to elicit observations somehow (e.g., by sending surveys out to a bunch of people).  Crucially, in neither of these methods does the researcher (intentionally) intervene on the data generating process: both the response variables and explanatory variables are free to vary naturally. 

**Causal routes:**    
Generally, one cannot infer causal relationships from passive data collection methods.  A relationship between variables A (say height) and B (say income) identified from passive data collection might have arisen from a number of possible causal routes.  Perhaps A causes B directly (e.g., if employees give raises based on height).  Perhaps B causes A directly (e.g., being rich makes you stand up straight).  Perhaps A and B have a **common cause** C (e.g., "good genes" make you tall, and make you smart enough to get a good job).  Perhaps A causes B through some **mediating variables** (or vice versa; e.g., being rich allows you to buy good food, which makes you grow up big and tall).   These potential alternative explanations of an observed relationship are often called **confounds**. Figuring out which of these causal routes play a role requires experiments in which we intervene on some variables and measure the outcome on others. 

**Experiments and the logic of random assignment:**     
In an *experiment* the researcher tries to control and manipulate some possible explanatory variables.  By controlling and manipulating these potential influential variables, they become **independent**, in that their values are controlled by the experimenter, and thus independent of other naturally occurring variables.  After manipulating the independent variables, the experimenter then measures some response variables, which might depend on the manipulation, hence these resopnses are called **dependent** variables.  Pierce (1877) pointed out the virtues of *random assignment* of independent variables (e.g., randomly assigning subjects to groups, randomly assigning trials to within-subject conditions, etc.).  Random assignment ensures that the independent variables are indeed independent of systematic external influences; thus eliminating causal influences on the independent variables.  By so doing, one can isolate, and test for, the causal relationship from the independent variable to the dependent variable, ruling out the possibility of common causes (although this does not rule out the possibility of mediating variables, nor does it rule out the possibility that the experiment inadvertently introduces confounds by manipulates something else along with the independent variables).  

Since most statistical procedures are tailored to analyzing experiments to ask whether the independently manipulated explanatory variables influence the response variables, we often call the variables playing an explanatory role in an analysis *independent*, and the response/outcome variables *dependent*.

## Designing experiments

Designing experiments is hard.  It is hard to ensure the *validity* of manipulations and measurements.  It is hard to control *extraneous variability*.  It is hard to be sure that the data analysis will be feasible and the results interpretable.  When designing an experiment you will have to make a plethora of seemingly arbitrary, but likely influential, decisions, and that's not even considering the choices you took for granted.

The general principles I would advocate when designing experiments:  

1. keep it simple: it is much more effective to ask and answer one precise question with one experiment, than to try to answer many experiment at once.  

2. think hard about validity of manipulations and measurements.  

3. choose measurements and manipulations with high resolution and reliability.  

4. use random assignment, double-blinding, and random selection.   

5. favor within-subject designs, and try to hold nuisance variables constant.  

6. add checks for manipulation, reliability, and blindness.  

7. think through your analysis while designing the experiment.  

8. play devil's advocate to find faults in your design.

### Validity
The tricky part of experiment design in the social sciences is in coming up with manipulations and measurements that cleanly operationalize the theoretical constructs you aim to test -- that is, your primary concern when designing an experiment should be the "validity" of the manipulated and measured variables.  The primary cause of the difficulty is that our theoritical constructs are sufficiently vague that connecting them to concrete manipulations and measurements usually requires a series of assumptions predicated on auxilliariry theories (see Meehl, 1990; *Psychological Reports* for some discussion).   It's worth taking some time to carefully think through this chain linking your concrete experiment to your theory.

### How to measure
Holding validity constant, you want measurements that have higher fidelity/resolution, higher reliability, and higher "levels" of measurement.  You can always make your data coarser by binning, thresholding, etc, but you can never make your data more fine-grained after it has been recorded.  See [here](measurement.html) for more.

### How to manipulate.
When coming up with a (valid!) concrete way to manipulate the theoretical variables of interest, you have a choice how to do so.  Do you choose two "levels" of the variable, this yielding a binary comparison between the two levels?  Or might you choose multiple levels?  I would advocate that, as much as possible, you strive to *parametrically* manipulate the variable of interest, thus yielding at least 3 levels.  This is effectively an appeal to increase the *resolution* of your manipulations.  Insofar as your theory predicts a monotonic effect of your manipulation on your measurement (meaning that increasing levels of the manipulated variable yield the same directional change in the measurement), then parametric manipulations provide a much more convincing test of this prediction.  Such parametric manipulations have further positive consequences: they generally demand greater precision in how you connect your theory to your manipulation during experiment design, and they allow you to provide more precise, quantitative, estimates of the effects of interest.

Insofar as your theory predicts an effect of multiple different variables, but does not predict that they will interact in any interesting way, I recommend doing multiple isolated experiments each manipulating a single variable of interest, rather than adopting a **factorial** design in which you *cross* the manipulation of multiple variables together.  Isolated experiments are generally more statistically efficient, meaning that they will be more powerful with a fixed total sample size, or will require a smaller total sample size to achieve the same level of power.  Insofar as you find the effects of interest, it might make sense to run a **factorial** design as a follow-up, but the factorial design is a bad place to start (this is a specific case of the "keep it simple" principle -- try to pose one precise question with each experiment, rather than trying to answer all the questions at once).

### Gold standard: random selection, random assignment, double-blind
The gold standard for experiments is for the sampled observations to be randomly selected based on [probability sampling](surveys.html), for different experimental units to be randomly assigned to experimental conditions (thus justifying the logic of causal inference in experiments), and for the experiment to be double-blind (meaning that neither the participants, nor the experimenters they interact with, know which conditions participants are assigned to.)

**Random assignment** is the most important criterion for an experiment, it means that which experimental units are included in which conditions should have no systematic relationship to any pre-experimental properties of that unit.  Whether we consider assigning subjects to *between-subject* conditions, or individual trials to *within-subject* conditions, this assignment should not introduce systematic confounds (such as putting young people, or early trials in a particular condition).  Without such random assignment, the experiment has confounds that jeopardize the validity of causal inference.

**Blinding** experiments with people require that the prople not know what kind of data experimenters want to see, otherwise **demand effects** will influence the results: people will want to please the experimenter by providing the desired data.  Often blinding people to the condition they are in is impossible (perhaps because the condition requires that they be explicitly given some instruction), however, at the very least participants should be blind to the expected consequences of being in that condition.  It is also important that experimenters who directly interact with participants also be blind to the participant's condition (hence "double-blind": blind subjects, and blind experimenters), lest they influence participants via some version of the [clever Hans effect](https://en.wikipedia.org/wiki/Clever_Hans), or by perturbing the data they manually code or record.  

**Random selection** is, in principle, quite important, as we want the results of our experiment to be generalizable to a broader population.  However, in practice, random selection in psychology experiments is quite rare (convenience sampling is the norm).  The extent to which this is a problem depends on how much you think the effects you are studying vary across sub-populations.  The [many labs](https://osf.io/wx7ck/) replication project suggests that such across-site variation in effects is generally small.  In general these sorts of "interactions" with demographics tend to be smaller when studying phenomena closer to sensation or motor control.  Consequently, most folks feel comfortable without random selection, but its useful to remember that such confidence is not always justified.

### Dealing with extraneous influences on measurements.
Most of the measurements you might be interested in will not only be influenced by your manipulations, but also by a number of variables you have no immediate interest in; let's call these *nuisance* variables.  For example, in an experiment assessing processing speed for different parts of speech, the measured response times will be influenced by the characteristics of the participant (their age, sex, IQ, reading proficiency), some time-varying properties of participants (how tired they are, their blood-sugar, engagement with the task), properties of the stimuli (length, frequency, concreteness of words being used), sequence-dependent properties of the stimuli (which words preceded the current one, how far into the experiment the current trial occurred), and many others.  We definitely do not want these nuisance variables to systematically relate to the experimental conditions of interest, but there are many options for how we might accomplish this.

We can **randomize** some, so that, on average different levels of that nuisance variable occur equally often in each condition; thus variation in measurements due to the nuisance variable will not yield differences between conditions. (e.g., we might randomly assign participants to different conditions, regardless of their age, sex, etc.)  This strategy prevents natural variation in the nuisance variables from influencing the effects of experimental interest, but the natural variation will manifest as additional observation "noise", thus lowering the signal-to-noise ratio of our manipulations.

We can **hold constant** some nuisance variables, so that there is no variation in the nuisance variable, and thus no variation in measurements due to the nuisance variable.  (e.g., we might pick participants only of a certain age, or sex; words of only a certain length, etc.).  Since this strategy eliminates all natural variation of the nuisance variable, we need not be worried about it confounding our manipulation, nor about it increasing observation noise; however, the downside is that if the nuisance variable interacts with our manipulation, we might get a poor picture of the real-world impact of the manipulated variables.

We can **intentionally manipulate**, some nuisance variables orthogonally to the primary manipulations of interest.  This creates a factorial design, potentially allowing us to see how the effect of our experimental manipulation varies with the nuisance variable.  (e.g., we might intentionally manipulate word length along with part of speech thus effectively creating a factorial word-length x part-of-speech design).  This strategy is useful if we think the effect of the primary manipulation might be influenced by the additional variable, but should be used sparingly, as additional, orthogonal manipualtions generally pose substantial statistical difficulties by lowering power and increasing sample size requirements.

We can **measure and statistically correct** for the influence of some nuisance variables.  (e.g., we might record the age of our participants, and then try to estimate how they influence response times and factor that out, either by adding "covariates" or "blocking" factors based on participants' age).  While, such a procedure can help correct for some "noise" and systematic biases due to the nuisance variables, the efficacy of such a strategy depends on how well the statistical model of the nuisance variable we adopt matches the real-world influence of that variable.

We can **measure and block/balance** the nuisance variables, by assigning experimental units to conditions so as to ensure that the nuisance variables are equally represented in each condition.  (e.g., we record the participants' education level, and assign them to "blocks" of matching education, which are then each split among conditions).  This strategy is sometimes the only available option, although it is difficult and may subvert the quality of random assignment.

We can **yoke** nuisance variables in one condition to those in another, if one condition is more flexible.  (e.g., if we compare self-paced learning to fixed-pace learning, we might take the exposure times from one self-paced subject as the fixed-pace presentation for another subject, thus equating the training exposure across the two conditions; or we might sample control subjects to be IQ-matched to individual members of the autistic sample; thus yoking IQ across conditions to create "blocks" of IQ-matched pairs of autistic and control participants.)  This is much like "blocking" during random assignment, but is a bit more active, such that the experimental or sampling procedure is modified in support of the desired blocking structure.

We can effectively block all participant-specific nuisance variables by adopting a **repeated measures** or **within-subject** design, such that all manipulations are carried out across the many trials/blocks each participant undertakes.  (e.g., each trial/block is assigned a condition, and every participant completes trials from all conditions).  This strategy is an effortless way to factor out all participant-specific nuisance variable: although they will still contribute noise to all measurements of a participant, we can factor our their influence by considering the within-subject differences.  Repeated measures designs tend to be considerably more powerful than across-subject designs, but unfortunately it is not always feasible to manipulate the variables of interest within-subject.

On the whole, I think a good strategy is to **strongly favor within-subject designs** to not worry about participant-specific nuisance variables; try to **hold constant** as many stimulus-specific nuisance variables as possible, and **randomize** and **measure to statistically correct** the stimulus-specific variables that cannot be held constant.

### Add checks
Regardless of how carefully you designed your experiment, something will go wrong, or at the very least, reviewers will think that something went wrong.  To make it easier to check what went wrong in your experiment, and to give you some way to address at least some concerns reviewers' might have, it is worth incorporating various explicit "checks" in your experiment.

**Attention checks** often take the form of "catch" trials -- trials where any sane, competent person paying attention to your experiment should provide a particular, predictable answer.  In experiments that tend to have high error rates, this might take the form of some very easy trials, to ascertain that subjects are actually paying some attention.  In surveys, these might take the form of simple obvious questions, such as "what is $61-27$?", to make sure folks are actually answering the questions they are posed, rather than responding with garbage.  Other forms of attention checks include measuring how much time people spent reading the instructions, etc.  Insofar as people failed the attention checks, you have reason to suspect that they were not really doing your experiment in any kind of meaningful way, so you should probably not use their data. 

**Manipulation checks** take the form of some additional measurements to ascertain the validity of your manipulation.  For instance, if you want to see how perceived wealth of a person changes your moral judgments of the person, and you manipulate perceived wealth by showing them driving a particular make of car, you might include a question asking explicitly about how wealthy participants think the person in question is, to make sure the car manipulation had the effect you wanted.  More subtly, if you want to assess the influence of verbal interference (by verbal shadowing some NPR broadcast) on an arithmetic task, you might include some conditions/trials to check that the verbal shadowing manipulation actually produces verbal interferences by assessing tasks that are known to suffer during verbal interference.  

**Data quality / reliability checks** allow you to measure the reliability of your measurements, so that you can check if your measurements are clean enough to make the experiment viable.  These might take the form of including many trials so that you can assess the split-half (items) reliability across subjects (for between-subject designs), or including the same trials for all subjects to assess the split-half (subjects) reliability across trials.  Basically, such checks give you multiple (independent) measurements of the same subject-condition combination to allow you to assess how consistent these measurements are.

**Strategy checks (debriefing):** Although your manipulation may have effectively made it quite difficult to perform the task in the same way across conditions, people might have outsmarted you and adopted a completely different strategy.  For instance, you might have made the spatial working-memory task so hard that people started marking the locations they need to remember by holding fingers in their locations on-screen.  Oops.  The debriefing (after the experiment is complete) is where you should ask subjects to see what kinds of weird strategies they may have used to make the task easier.  

**Blindness checks (debriefing):**  Since subjects often cannot be blind to the condition they are in, you often hope that they are at least blind to the experimental hypothesis about how the condition will change their behavior.  Unfortunately, participants are pretty clever, and may have figured it out, if so, you have a bit of a problem.  You should ask in the post-experiment debriefing what they thought the experiment was about.  Often this is done in several steps, first open-ended questions like (what did you think our experiment was about?), then a slightly more leading question (we made you rate pictures of puppies while doing arithmetic, what do you think we were getting at?), then an explicit question (we wanted to know how arithmetic influences perceived cuteness, do you think cuteness ratings would increase or decrease while doing arithmetic?)


### Think through the analysis to debug your experiment design.

> "To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of." **RA Fisher** (1938)

While you might not need to consult a statistician before conducting your experiment, it would be prudent to think through your data analysis before collecting data, as often problems in the design and measurements can be spotted even when simply running through the analysis procedure in your head.  I recommend asking yourself the following sequence of questions:

0) What are you trying to do?  Your goal should boil down to a simple, concrete question, with a set of candidate concrete answers that you have already entertained.  The most common problem I've seen in experiments designed by new students is to try to do too much with one experiment: they try to ask *all* the questions at once, and thus fail to answer any one of them.  Just remember **KISS**: "Keep it simple, stupid".

1) What will the "money shot" figure look like?  This is the figure that clearly illustrates the awesome result you are going after, the concrete answer to your concrete question that you most favor.  What will be plotted in this figure?  How might this figure look under the less interesting possibilities you find plausible, but want to dismiss?  What will this figure look like if you got the result you want?  These should be different!  The other common problem in experiment design is a failure to think through to the data that an experiment will generate, and how those data will answer the question posed.

2) Now that you know what kinds of patterns your impressively convincing figure might show, you should ask yourself: how are you going to go from the data you collected to the figure?  Is there a vague step that you are not sure about?  Think harder about that, often these vague, indetermined steps are vague because that step is hard.  Try to find these pitfalls ahead of time, and make sure you that you come up with solutions (perhaps by changing the experiment a bit).

3) Now that you know what the diagnostic pattern in your killer figure would look like, and how you are going to produce it from your data, ask yourself: If I'm wrong about my overall conclusion, but such a pattern of data appears anyway, what are some plausible reasons why?  This is a good way to probe for alternate explanations to your result, and try to control for them ahead of time.  Is there anything you can do in the experiment design to make those alternate explanations less likely or plausible?

4) Similarly, ask yourself: between my manipulation (if any) and measurements, what possible confounds, and mediators could I have introduced?  What possible common causes might be influencing both the explanatory and response variables?  Can you control these?

5) Ok, now you are reasonably confident in the logic of the experiment, and the decisiveness of your killer figure.  Great, let's try to figure out whether you are likely to get this result, and how you might improve your chances.  Consider the overall differences in your figure, how big are they likely to be?  Let's say you are predicting differences in accuracy, how big will these differences be?  50% to 95%?  75% to 85%?  Now consider what other factors in the world (that you are neither controlling or manipulating) are going to influence your measurement (e.g., accuracy will be influenced by overall participant attentiveness, their intelligence, etc.).  All these other sources are going to add noise.  How big is the main difference of interest going to be relative to this noise?  That's your "effect size", if it's going to be small, consider somehow controlling for, or factoring out these extraneous sources of variation (e.g., via within-subject, or even within-trial designs).  Finally, how noisy do you think your measurements will be?  If you measure the same thing twice will your measurements be very similar relative to your effect size (great!), if not, your effect size, as measured, will be smaller -- perhaps you could find a more reliable measurement procedure?

6) At this point, it's worth asking: how much data will you need for your result of interest to show up clearly in your killer graph (given error bars and such).  This is known as a "power analysis" and requires a bit more statistical knowhow than we have provided up to now, but this step if often worth doing if you are expecting small effect sizes and expensive/restricted sample sizes.  If you can't get enough power for this experiment design, perhaps it's worth redesigning it.

The study registration movement, suggest that you should not only think through the analysis ahead of time, but should actually write up your design and analysis plan, have it reviewed to be a "pre-registered study plan", and then carry out your study and analysis according to this plan.  This sort of procedure aims to eliminate the possibility of p-hacking via data-driven analysis selection.  While you might not want to pre-register your study, I encourage you to at the very least write out your analysis plan (in R code) in advance of the experiment so you can be honest with yourself about how much you altered your analysis plan in light of the data.

