---
output: 
    tufterhandout::html_tufte_handout
---

# Philosophy of science

Scientific inquiry starts when people notice some interesting phenomena in the world: flora and fauna are not uniformly distributed across the globe,  people speak different languages, objects fall to the ground when unsupported, etc.  Science is the subsequent process of measuring, explaining, and predicting these phenomena.  When science has a phenomenon mostly figured out, it becomes engineering.

Scientists start out measuring and documenting details of phenomena: what lives where? what forms do languages take? how quickly do objects fall?  They then proceed through a long process of trying to explain these phenomena via theories of increasing precision and predictive power.  Scientists advance this endless [series of approximations](http://www.jstor.org/stable/27535282) by developing new theories and measuring the world to [test](http://en.wikipedia.org/wiki/Karl_Popper#Philosophy_of_science) predictions and document new phenomena.  Often, this series undergoes discontinuous transitions (["paradigm shifts"](http://en.wikipedia.org/wiki/Paradigm_shift) a la Kuhn) as scientists reconsider the basic elements being measured or basic constructs of their theories, because different ones might be more usefully invariant, and predictable (e.g., realizing that mass and force -- rather than Aristotelian elements and causes -- are more stable and predictable in the world).  Once these approximations are so good at predicting the behavior of a given aspect of the world across many relevant circumstances, that domain of scientific inquiry becomes engineering.  For instance, we're now so good at predicting how force influences the motion of macroscopic objects that there are few scientific questions left in classical mechanics; instead, mechanical engineering uses that predictive success to make useful stuff.

There are a few interesting corollaries of this view of science:

1) There is nothing sacred about theories, yours, or anyone else's: They are all approximations of the world.  (In the words of Box: "all models are wrong, but some are useful").  Some of these approximations are better than others, but many theories that seem to compete may be good at predicting different things, or may make different tradeoffs in terms of their predictive scope and precision.  None of them are The Truth, and it is quite reasonable for many to co-exist at once, and you should expect all of them to be supplanted by better approximations in the future.  Don't get too emotionally attached to theories.

2) We should expect different approximations to exist at different "scales" of analysis, and they will be useful in different ways.  We should not expect one of them to come out a winner.  

## Scales of Analysis

What are scales of analysis?  A given phenomenon, like the apparent political polarization of US congressional representatives, has substrates at many different scales.  For instance, one can consider political polarization as an aggregate phenomenon of US society, or as a phenomenon arising in small groups of competing politicians, or as a process determining the voting patterns of a single representative, or as the interaction of large-scale neural mechanisms in the representative's brain, or as arising from decision-making circuits, or as the properties of those individual cells, or the neurotransmitters and other molecules that operate within those cells, or even the atoms and subatomic particles that make up those molecules.   Obviously, stuff is happening at *all* of these scales, but some of them seem absurd for an analysis of political polarization.  Nevertheless, deciding which scale of analysis to adopt to describe a social science phenomenon is a thorny issue.

Usually, when scientists frown upon research or theories as not describing the underlying *mechanism*, they are making a self-serving assumption that the best scale of analysis for a given phenomenon is the scale that they themselves adopt.  When a cognitive psychologist frowns upon a computational account of learning as not being "mechanistic", they are assuming that the most relevant scale of analysis is in the algorithms implementing such a computation.  Similarly, when a systems neuroscientist frowns upon an fMRI finding describing how a particular region responds to a particular task because they fail to describe "how" that region works, they do so from the assumption that the best level of description is in terms of wiring diagrams between ensembles of neurons.  In general, a "mechanism" is built from the basic theoretical building-blocks adopted at a particular scale of analysis (e.g., molecules for some cellular neuroscience, or whole neurons for some systems neuroscience).  Consequently, the basic units that make up a "mechanism" at one scale of analysis are phenomena in need of mechanistic explanation at a smaller scale of analysis.

If we back up a bit and consider that accounts and explanations at different scales are all just approximations differing in the scope of what they can predict, it seems clear that we should not expect one scale of analysis to rule them all.  It is impractical to try to explain higher-order phenomena (like congressional polarization) in terms of much smaller scale units (like neurotransmitters).  It is also impossible to predict fMRI activity from a sociological analysis of the political forces in play on congressional representatives in aggregate.  Hopefully we will connect all these levels at some point, but even once we do: no level will come out the victor.  Even in physics, where we have succeeded in reducing many large-scale phenomena to their microscopic substrates, we still rely on higher order abstractions (like temperature, force, etc.) when we need to make predictions at their scale.

## Scientific paradigms

Just as individual scientific theories are cast at a particular scale of analysis, they are also cast within a certain [paradigm](https://en.wikipedia.org/wiki/Paradigm#Scientific_paradigm).

A scientific paradigm is grander than a theory: it is a set of assumptions about how we should cut up nature at its joints.  It does not make specific predictions about how the world will behave, but it makes assumptions about what are the relevant variables in the world.  For instance, at some point, Greek philosophers thought that the physical world is made up of elements: earth, fire, water, air, aether. Within this paradigm, one can conduct experiments to try to measure the fire in some object, and make predictions about how the interactions of fire and air elements will cause different sorts of effects.  Eventually, natural philosophers started thinking in terms of mass, force, etc., bringing a whole new paradigm of thought to physics.  

Although a paradigm itself does not make predictions, theories expressed within a given paradigm can be more or less successful.  It turned out that theories of the motion of objects based on properties such as mass and force are much more effective than theories devised using Aristotelean elements and causes.  

I would venture to guess that many fields of social science could do with some paradigm improvement: coming up with new concepts and constructs that are more usefully invariant, and can thus be used to build more effective theories.  To this end I I encourage you to   

- question your field's current assumptions about the basic variables in the world: What are the aspects of the world worth measuring and predicting?  What are the building blocks (intangible constructs) of theories (attention, emotion, etc)? When you find yourself using some niche jargon to explain a phenomenon, ask yourself what abstract construct that jargon refers to, and what evidence you have for it being a "real thing".   

- consider your paradigm from a different scale or field: Perhaps the basic constructs at your scale of analysis ought to be revised in light of phenomena at smaller-scales?  Perhaps the basic building blocks at larger-scales are useful target phenomena to measure and explain at your scale of analysis?   
