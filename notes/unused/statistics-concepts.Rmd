---
output: 
    tufterhandout::html_tufte_handout
---

# Conceptual overview of statistics

## Relationship between theories, statistical models, and probability

Statistical models are a formal specification of a theory.  When we have vague theories, we resort to somewhat generic and vague statistical models (e.g., there is a linear relationship with Normal noise between our expanatory and repsonse variables).  Probability provides the formal tools to describe what data are more or less likely under a particular statistical model.  Thus, probability links theories to data by way of an explicit statistical model.

## Populations, samples, and statistical models.

It is useful to think of a population at a stochastic process in the world that generates our data (sample).  Our statistical models are approximations of this process, and we try to characterize the process by picking models that approximate it best, estimating their parameters, etc.  More on this [here](../probability-stats/population-samples.html).

## Uses of statistical models.

Since statistical models are formal descriptions of our theory of the population, and connect our data to the questions we are interested in, we use them throughout statistics.  In [descriptive statistics](descriptive.html), we choose summaries that are informative in light of some implicit statistical model.  In estimation, we explicitly try to estimate model parameters.  In null hypothesis significance testing, we compare observed data to those generated by a statistical model we aim to reject.  In model selection, we compare the likelihoods of our data under competing statistical models.  In decision making, we estimate the probability of various outcomes of our actions based on our best statistical model, and choose accordingly.  More on this [here](uses-of-models.html)

## Signal, noise, error, and variability

Our measurements are variable. Some of this variability is of primary interest to us, and we call that "signal" or "effect".  Other sources of variability are of no interest to us and obscure the signal, we call these "noise", or "error", or "subject variance", or "item effects", or "trial variation", etc.  However, **what we call signal and noise is subjective**: if I am interested in how SAT scores change with a particular training program, individual differences in SAT performance are a source of noise; in contrast, if I want to know how SAT scores vary with demographics, the individual differences now become signal, and any training effects from SAT courses are part of the noise.

## Scope of sources of variability

Most things we measure are perturbed by many sources of variability, some of which we consider signal, others we consider noise.  It is important to know the scope of different sources of variability:   

- every measurement with a miscalibrated instrument will *share* variability due to miscalibration (we often call this bias, as it will look like an offset, rather than variability, if all our measurements used the same instrument)    
- every measurement of a particular subject will share variability due to individual differences    
- every measurement of a particular item will share variability due to item effects   
- measurements adjacent in time will share variability from various time-varying effects (like attentional state)   
- trial-specific idiosyncracies will be shared by all measurements on a specific trial (generally, we get one measurement per trial, and we call this "measurement error")   

These sources of shared variability induce *correlations* in the noise of our data.  Sound inference in such data requires appropriately accounting for these noise correlations, to ascertain what to count as "independent" measurements.

## Probability interpretation of frequentist and Bayesian statistics

Both (classical) frequentist and Bayesian statistics rely on probability to do some sort of inferential statistics.  However, the probability statements by frequentists and Bayesians apply to different things.  Frequentist probability statements are statements about the long-run properties of the *procedure we used*, not the actual data or parameters we are interested in; for instance, "95%" in a 95% confidence interval means that "of all the confidence intervals we calculate via this procedure, 95% of them will contain the true value".  Bayesian probability statements are about *beliefs*; for instance, "95%" in a 95% credible interval (the Bayesian analogue of confidence intervals), says that with the priors used, after seeing the data you saw, you should believe that the parameter value is contained in this interval with probability 0.95.

## Sampling and sampling distributions of statistics.

Our data are a sample.  If we were to rerun the experiment, we would get a different sample.  The sampling distribution of something (technically a random variable) is the probability distribution describing the probability that it will take on any possible value when we sample it.  For instance, let's consider a measurement of a sampled person's IQ.  Although we actually got some measurement, from the perspective of frequentist statistics, we must consider what other measurements we could have seen -- so we say that the sampling distribution of a measurement of IQ is Normal with a mean of 100 and a standard deviation of 15 (this is how IQ is defined).  Just as it makes sense to talk about sampling distributions for measurements, or sets of measurements, it also makes sense to consider sampling distributions for [statistics](descriptive.html).  For instance, we got a particular sample of 10 people's IQs, and calculated the sample mean.  Even though we saw one particular sample mean, we must consider the probability distribution of other sample means we could have seen from this procedure of sampling 10 people and averaging their IQs (this is the sampling distribution of the sample mean).  We can mathematically derive the sampling distributions of various statistics by relying on the *statistical model* we assume underlies our data.

### Standard error

Many sampling distributions for statistics used to *estimate* parameters are well approximated as Normal, centered on the true value.  Thus, on average, the estimator is unbiased (meaning that the average error is zero), but the actual error of a particular observed estimator is distributed normally around 0.  The standard deviation of this "error" distribution (which is basically the sampling distribution of the statistic), is called the "standard error".  You are probably most familiar with this concept in the context of the "standard error of the sample mean" (when the sample mean is used to estimate the population mean), but it applies to many statistics and estimators.

### Sampling distribution of test statistic under the null hypothesis.

A test statistic (like the t value from a t-test) is also a sample statistic, and also has some sampling distribution under a particular assumed statistical model.  The sampling distribution of this test statistic under the null hypothesis describes which values of the test statistic we expect to see when drawing samples of which the null hypothesis is true.  By comparing our observed test statistic to the sampling distribution of this test statistic under the null, we can calculate the *p-value*: the probability that a test statistic sampled from the null will be at least as "extreme" as the one we observed -- lower p-values mean that our test statistic is less likely under the null, so we are more justified to "reject" the null (much more on this later).  *alpha* refers to the maximum p-value we are willing to tollerate to declare that our data "reject" the null, and are thus "significant"; this sets the probability of "Type I" error: how often this testing procedure will falsely reject the null.  The *critical value* refers to the value of the test statistic that would yield a p-value equal to alpha -- basically, the least extreme test statistic we are willing to delcare significantly different from the null.

### Sampling distribution of test statistic under the alternative

Just as we can calculate the sampling distribution of a test statistic under the null hypothesis, which is a statistical model that assumes our effect size is zero, we can also calculate this sampling distribution under a particular alternate hypothesis that assumes some specific, non-zero *effect size*.  By comparing the critical value (obtained from alpha, and the sampling distribution of the test statistic under the null) to the sampling distribution under a particular effect size, we can calculate how often a test-statistic sampled from a population with a particular effect size will fail to be significant; this is known as *beta*, or the probability of Type II error.  Similarly, we can calculate how often test statistics obtained from a population with that particular effect size will exceed our critical value for declaring them significant, thus we can estimate the probability that we will reject the null hypothesis when we get a sample from a population with that particular effect size; this is known as *power*.

