---
output: 
    html_document
---

# Conditional probability and Bayes

Let's start by remembering where we were with our unfair die roll.

````{r}
outcomes = c('1', '2', '3', '4', '5', '6')
unnormalized = c(2, 1, 1, 1, 1, 1)
p.outcomes = unnormalized/sum(unnormalized)
names(p.outcomes) <- outcomes
p.outcomes

p.event = function(subset){
    sum(p.outcomes[subset])
}
even = c('2', '4', '6')
greater = c('4', '5', '6')
````

> The **Conditional probability** of event A *given* that event B has occurred is the probability of A *and* B divided by the probability of B:   
> $P(A \mid B) = \frac{P(A \& B)}{P(B)}$

Which might be read as: of all the ways in which B might occur, how many of them co-occur with A?

For instance, the probability of "even" given "greater than 3" is asking: of all the ways the dice could come up greater than 3, which of them also yield an even roll?  
$P(\mbox{even} \mid \mbox{greater than 3}) = P(\{2,4,6\} \mid \{4,5,6\}) = \frac{P(\{4,6\})}{P(\{4,5,6\})}$

````{r}
# probability of even given greater than 3
p.event(intersect(even,greater)) / p.event(greater)
````

From this definition, we can find a general expression for the probability of a conjunction.

> The **probability of a conjunction** of two *events* (A and B):   
> $P(A \& B) = P(A \mid B) * P(B) = P(B \mid A) * P(A)$

There are two special cases worth mentioning:

> Two events are **disjoint** (mutually exclusive) if their interesction is null:   
> $A \cap B = \emptyset$;   
> consequently: $P(A \& B) = P(A \mid B) = P(B \mid A) = 0$, and $P(A \lor B) = P(A) + P(B)$


> If two events are **independent** then:   
> $P(A \& B) = P(A) * P(B)$, and $P(A \mid B) = P(A)$, and $P(B \mid A) = P(B)$

Note that often this special case (of independent events) is described as the rule for the probability of a conjunction, but in most cases, events are not independent.

## Chain rule

We can calculate complex conjunctions by repeating the same process of calculating a conjunction many times via the *chain rule*:

$P(a \& b \& c \& d) = P(a \mid b \& c \& d) * P(b \mid c \& d) * P(c \mid d) * P(d)$


## Partitions and total probability

To express Bayes rule as it is commonly used, we first need to introduce two concepts.

> A **partition** of sample space $\Omega$ is a set of events that are *disjoint*, *nonempty*, and *their union is $\Omega$*.   
> In other words, $A$ is a partition of $\Omega$ if every element of $\Omega$ is in one and only one event in A.  
> $\emptyset \not\in A$   
> $\bigcup\limits_{E \in A} E = \Omega$   
> $E_1 \cap E2 = \emptyset \mbox{ for all } E_1, E_2 \in A \mbox{ and } E_1 \not= E_2$

For instance, the events {"even" and "odd"} form a partition of the sample space of die rolls.  In contrast, {"even", "odd", and "greater than 3"} do not form a partition, as they include overlapping events (they are not disjoint).  Similarly, {"1 or 2", and "4 or 6"} are not a partition as they do not cover the full sample space (their union is not $\Omega$).

> The **law of total probability** says that if events $A_1,A_2, ..., A_n$ are a partition of $\Omega$, then    
> $P(B) = \sum\limits_{i=1}^n P(B|A_i)P(A_i) = \sum\limits_{i=1}^n P(B \& A_i)$.

We can confirm that this works in the simple case of B = "greater than 3", and the partition is "even" and "odd".   
P("greater than 3") = P("greater than 3"|even) P(even) + P("greater than 3"|odd) P(odd)   
P("greater than 3") = (2/3)(3/7) + (1/4)(4/7) = 3/7   

````{r}
greater = c('4', '5', '6')
even = c('2', '4', '6')
odd = c('1', '3', '5')
p.greater.even = p.event(intersect(even,greater)) / p.event(even)
p.greater.odd = p.event(intersect(odd,greater)) / p.event(odd)
p.even = p.event(even)
p.odd = p.event(odd)
p.greater.even * p.even + p.greater.odd * p.odd
p.event(greater)
````

## Bayes' rule

You can find an ["excruciatingly gentle" introduction to Bayes' rule](http://yudkowsky.net/rational/bayes).

Bayes rule follows directly from our definitions of conjunctions and conditional probabilities    

$P(B|A)P(A) = P(A \& B) = P(A|B)P(B)$

> **Bayes rule** provides a way to *invert conditional probabilities*: to go from P(B|A) to P(A|B)    
> $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

More often, in practice we substitute the law of total probability for the denominator of the Bayes rule expression:

$P(A|B) = \frac{P(B|A)P(A)}{\sum\limits_{A'}P(B|A')P(A')}$, where $\sum\limits_{A'}$ denotes a sum over all the events in the partition of which A is a member.

Bayes' theorem is the basis for an entire branch of statistics, as well as a number of artificial intelligence applications.  In statistics, Bayes' theorem is often written in terms of individual hypotheses $h$ from the hypothesis space $\mathcal{H}$ and data $D$, yielding:  

$P(h|D) = \frac{P(D|h)P(h)}{P(D)} = \frac{P(D|h)P(h)}{\sum\limits_{q \in \mathcal{H}} P(D|q) P(q)}$

The individual components of this equation have names:  

$P(h)$ is the **prior**: how much you believe hypothesis $h$ is true before you saw data $D$.   

$P(D|h)$ is the **likelihood**: how likely are the data $D$ to have been observed if hypothesis $h$ were true.    

$P(h|D)$ is the **posterior**: how much you ought to believe hypothesis $h$ is true given that you have seen data $D$.    

$P(D)$ is often called the **normalizing constant** because it is constant regardless of which $h$ you consider.   
Consequently, it is often dropped and Bayes' theorem is written as a proportionality: $P(h|D) \propto P(D|h)P(h)$  

We will come back to all of these at a later point. In the meantime, we can use Bayes rule to calculate the probability that a published significant finding is truly not null (H1 rather than H0).   
P('significant'|H0) = $\alpha$ = 0.05   
P('significant'|H1) = "power" = 0.6   
P(H1) = 0.3 = 1-P(H0)   

```{r}
p.sig.H0 = 0.05   # conventional alpha
p.sig.H1 = 0.6    # assumption about power.
p.H1 = 0.3        # assumption about baserate of tested effects
p.H0 = 1-p.H1
p.sig = p.sig.H0*p.H0 + p.sig.H1*p.H1
p.H1.sig = p.sig.H1*p.H1 / p.sig
p.H1.sig
```