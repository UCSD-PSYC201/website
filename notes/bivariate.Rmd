# Bivariate linear relationships {#bivariate}

When we have two variables measured per "unit" (e.g., measure height and weight for each person), we can refer to this as "bivariate" data.  We already discussed how to analyze [contingency tables](#nhst-chi-squared) when dealing with bivariate categorical data, here we are concerned with bivariate numerical data that may have a *linear relationship*.

To play with these measures, we will consider Karl Pearson's data on the heights of fathers and their sons.  (In 1895 Pearson worked out the formula for calculating what we now call the correlation coefficient.)  Generally, we will look at a bivariate numerical relationship in a scatterplot, like the one below.

```{r, fig.width=4, fig.height=4, message=FALSE, warning=FALSE}
library(tidyverse, quietly=TRUE)
heights <- read_csv('http://vulstats.ucsd.edu/data/Pearson.csv')
heights %>% 
  ggplot(aes(x=Father, y=Son))+
  geom_point()
```

## *Linear* relationships

Everything we discuss in this section is specific to measuring a *linear* relationship, meaning (informally) that the scatterplot looks like it would be meaningful to draw a line through it.  This has two implications.

(1) Lots of patterns of bivariate data have a relationship between x and y (meaning that we can learn something about y by knowing x, or vice versa), but that relationship is not linear.  The bottom row of the graph below (from wikipedia) does a great job showing many such cases.

![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg)

(2) Many different patterns of data might yield exactly the same line, and line statistics, and our simple measures of a linear relationship will not be able to distinguish among them.  The canonical example of this is Anscombe's quartet (below; [graph it yourself](#bivariate-anscombe)), which shows 4 sets of data with obviously very different relationships, but which have the same correlation, covariance, linear slope, as well as marginal means and standard deviations of x and y.

```{r, echo=FALSE, fig.width=8, fig.height=2.5}
anscombe.tidy = mutate(anscombe, observation=seq_len(n())) %>%
  gather(key, value, -observation) %>%
  separate(key, 
           c("variable", "set"), 
           1, 
           convert = TRUE) %>%
  mutate(set = c("I", "II", "III", "IV")[set]) %>%
 spread(variable, value)

ggplot(anscombe.tidy, aes(x=x,y=y))+
  facet_grid(.~set)+
  geom_smooth(method='lm', formula=y~x)+
  geom_point(color="black", fill="orange", shape=21, size=3.5)+
  theme_bw()+
  theme(text=element_text(size=16))
```

What this means, is that you should always look at a scatterplot of the data, don't just blindly rely on the numerical summaries.

## Covariance and correlation: Measuring the linear dependence.

### Covariance

The co-variance measures whether, when x is bigger than the mean of x, is y also bigger than the mean of y, and what is the overall scale of this co-variation?

$$\operatorname{covariance}(x,y) =  s_{xy} = \frac{1}{n-1}\sum\limits_{i=1}^n (x_i - \bar x)(y_i - \bar y)$$

- To get an intuition for what this is doing, consider when the expression being summed ($(x_i - \bar x)(y_i - \bar y)$) will be positive, and when it will be negative.  When x and y both deviate from their means in the same direction (either both larger, or both smaller than their mean), this product will be positive.  When they deviate in different directions (one smaller and the other larger than their mean), this product will be negative.  Thus the whole sum will be very positive if x and y tend to deviate in the same direction as their respective means, and will be very negative if they tend to deviate in opposite directions.  

- Note that this expression ($(x_i - \bar x)(y_i - \bar y)$) will give the same answer regardless of which variable we call x, and which we call y.  So the covariance of x and y is the same as the covariance of y and x.

- Also consider how this expression ($(x_i - \bar x)(y_i - \bar y)$) will change if we (a) add a constant to x, or y, or both, and (b) multiply x or y or both by a constant.  If we add a constant, it will just end up being subtracted out, since we consider only differences of a given value from its mean, thus the covariance will be the same regardless of where we "center" x and y.  However, if we multiply x (or y) by something, we will end up scaling the distance between x and its mean. Thus, if we scale by a factor larger than 1,  we will get a larger magnitude of this expression, and thus the covariance; and if we scale by a factor between 0 and 1, we will get a smaller covariance.  So while the covariance doesn't depend on the *location* it does depend on the *scale* of our variables.

In R we can calculate the covariance with the `cov` function:

```{r}
cov(heights$Father, heights$Son)
```

Here are a few more [detailed notes on the covariance](#bivariate-covariance).

### Correlation coefficient

The correlation coefficient re-scales the covariance by the standard deviations of x and y, so it yields a measure of the linear relationship that is scale invariant, and is always between -1 and +1.

$$\operatorname{correlation}(x,y) =  r_{xy} = \frac{\operatorname{covariance}(x,y)}{s_x s_y} = \frac{s_{xy}}{s_x s_y}$$

There are a bunch of ways to think about what the correlation means ([this paper lists 13](https://www.jstor.org/stable/2685263)), but there are a few that I consider to be particularly useful:  

- The **correlation is a scale-invariant covariance**.  It is equal to the covariance of the z-score of x and the z-score of y.  Thus, it disregards both the location and the scale of the variables, so we will get the same correlation regardless of how we *linearly transform* (multiply by a constant and/or add a constant) x and y (provided we don't multiply by a negative number -- that will change the sign, but not the magnitude, of the correlation).

- The **correlation is the slope of the z-scores**: meaning that if an x is 2 standard deviations above the mean of x, we would expect the corresponding y to be $r_{xy}*2$ standard deviations above the mean of y (and vice versa).

- The **correlation squared is the "coefficient of determination"**.  We will talk more about this when we consider the partitioning of variance, but this basically means: the proportion of the variance of y that we can explain by its relationship with x (and vice versa) is $r_{xy}^2$.

In R, we can calculate the correlation via `cor`, or calculate the correlation and test it's significance via `cor.test`:

```{r}
cor(heights$Father, heights$Son)
cor.test(heights$Father, heights$Son)
```

For more details on the correlation, here are some [detailed notes](#bivariate-corelation).

## (OLS) Regression: Predicting the mean of y for a given x

A linear regression of y~x ("~" here is read "as a function of") finds the "best fitting" line of the form $a \cdot x + b$ to estimate the "conditional mean" of y at a given x.  

```{r, fig.width=4, fig.height=4}
heights %>% 
  ggplot(aes(x=Father, y=Son))+
  geom_point()+
  geom_smooth(method="lm")
```

In the [detailed notes](#bivariate-ols) we go through the mathematical definitions, but here, let's just sort out what the different words in the sentence above mean.

- Regression line of y~x estimates the **conditional mean** of y for a given x.  Therefore, the y value of the line at a given x is the estimated mean of the y values with that paired x.  For instance, the Son ~ Father line shown above has y=72.4 at x=75.  This indicates that we estimate the mean height of Son, whose fathers are 75" tall, to be 72.4" tall.

- A **line** ($\hat y = a \cdot x + b$) is characterized by its **slope** ($a$) and **intercept** ($b$).   
The slope indicates how many units $\hat y$ will increase (or decrease in the case of a negative slope) every time x goes up by one unit, thus it is in units of $y/x$.  For instance if the slope of a line predicting weight (kg) as a function of height (cm) is 0.44 (kg/cm), that means that we expect someone who is 1 cm taller to weight 0.44 kg more.     
The intercept tells us the value of the line ($\hat y$), and thus our predicted mean y, when $x=0$.  So an intercept of of 2.46 on a line predicting weight (kg) as a function of height (cm) says that we predict people who are 0 cm tall to weigh 2.46 kg.     

- For some parameters (in our case, a slope and intercept of a line) to be "**best fitting**", that means that of all the values the parameters could have taken on, the "best fitting" values optimize some function that evaluates the overall fit.  In the simple regression case, we evaluate the overall fit as the sum of squared errors, the smaller the better: $\operatorname{SSE} = \sum_{i=1}^n (y_i - \hat y_i)^2 = \sum_{i=1}^n (y_i - (a\cdot x_i + b))^2$.  So the "best fitting" line is the line with a slope ($a$) and intercept ($b$) that yields the smallest sum of squared errors.  These are also the "maximum likelihood" slope/intercept because we usually specify a probability model for the data which says that the y values are Normally distributed around a mean which varies linearly with x: $y_i \sim \operatorname{Normal}(a\cdot x_i + b, \sigma_e)$.  Thus, although 'least squares' regression can be motivated simply by asserting that we want to minimize squared error, it also happens to be the correct procedure for estimating the maximum likelihood parameters under the standard regression model. 

In R, we can do all this (and a whole lot more) with a single command `lm`, which stands for "linear model".  It takes as an argument a formula, and a data frame.  A formula is written with the syntax `respons.variable ~ explanatory.variable`, so if we want to estimate the regression line predicting Son' height as a function of fathers' height, we would invoke the following incantation:

```{r}
lm(data=heights, Son ~ Father)
```

### Difference between y~x, x~y, and the principle component line

The "best fitting" line shown in the above section, doesn't really *look* like the best fitting line.  What gives?  Consider the three lines below...

```{r, fig.height=6, fig.width=6, echo=FALSE}
m.y = mean(heights$Son)
m.x = mean(heights$Father)
s.y = sd(heights$Son)
s.x = sd(heights$Father)

lm.y.x = lm(data=heights, Son ~ Father)

b1.y.x = coef(lm.y.x)['Father']
b0.y.x = coef(lm.y.x)['(Intercept)']

lm.x.y = lm(data=heights, Father ~ Son)
b1.x.y = 1/coef(lm.x.y)['Son']
b0.x.y = m.y - b1.x.y*m.x

pc.load = prcomp(heights, scale=T, retx=T)
b1.yx = pc.load$rotation[2,1]/pc.load$rotation[1,1]*s.y/s.x
b0.yx = m.y - b1.yx*m.x

ggplot(heights, aes(x=Father, y=Son))+
  geom_point(size=1, color="darkgray")+
  geom_abline(intercept = b0.y.x, slope=b1.y.x, color="blue", size=1.5)+
  geom_abline(intercept = b0.x.y, slope=b1.x.y, color="red", size=1.5)+
  geom_abline(intercept = b0.yx, slope=b1.yx, color="black", size=1.5)+
  theme_bw()
```

I bet the black line above looks like the *best* line to you, and both the red and blue lines seem off, right?  So what are these lines?   
Blue: the **y~x line**.    
Red: the **x~y line**.    
Black: the **principle component line**. 

Why is the "best looking" line not the "best fitting" y~x line?  While we're at it, why is the y~x line different from the x~y line?  The answer to all of these is that when we do a regression predicting y from x, we *only care about error in y*.  We minimize the squared deviations of each data point from the line in y, while keeping x constant.  Consider this a teaser explanation, and go read the more [detailed explanation](#bivariate-lines).

## Partitioning variance

As we move toward more complicated data, and away from simply comparing means between groups, it is useful to consider our analysis goals as *partitioning variance*.  We want to separate variability in some response variable into different sources.  In the simple linear regression case, we are just going to separate out the variability of y into the linear 'signal' (variability in y which we can explain via a linear relationship with x), and 'noise' (all other variability).  When we move on to more complicated regression setups, we will be using more explanatory variables, and thus will be partitioning variability into more sources.  It is important to keep in mind that 'noise' depends very much on the model we are considering.  Perhaps all the variability in sons' heights we can't explain with fathers' heights might be explained by 'volume of breast milk consumed in the first year of life', or some other variable we don't have access to.  Thus, the 'noise', is just *unexplained/unmodeled variance*.

We have some detailed notes on [partitioning variance](#bivariate-determination), but briefly.  When we partition variance, we mostly just consider the partitioning of the "sums of squares", which we can get in R via the `anova` (analysis of variance) command, which tells us the variability in y attributable to our explanatory variable, and that which is left unexplained (residuals).  

```{r}
lm.son.father <- lm(data=heights, Son~Father)
anova(lm.son.father)
```
 
 The proportion of variability in y that we can explain by taking into account the linear relationship with x, is the correlation squared ($r_{xy}^2$).   However, R will just give us this number if we look at the detailed summary of the linear model we fit (via the `summary` function).

```{r}
summary(lm.son.father)
```

That 'Multiple R-squared' number is "the coefficient of determination", which is the proportion of variance we can explain with the linear model, and in this simple one-variable regression case, it is also just the correlation squared.
 
## Significance of a linear relationship.

We have seen a few p values above.  One in `cor.test`, that told us the the correlation between fathers' and sons' heights is larger than expected by chance.  One in `anova(lm(...))` which told us that a linear model of sons~fathers explained more variance in sons heights than we would expect by chance.  Another in `summary(lm(...))` which told us that the father coefficient in the linear model we fit ($\mbox{son} = \hat \beta_{\mbox{father}} \cdot \mbox{father} + \hat \beta_0$) is significantly greater than zero.

For the simple linear regression case, these are all the same *by definition*!  They are all asking "is there more of a linear relationship than we expect from chance?"  In the case of *multiple regression* these will all be different, and will all be asking different questions.  
(In this father-son data, all these p-values are so small, that they are past the limit of our computer's ability to represent tiny numbers.  Thus, even if they were different, we wouldn't be able to tell.  To see a case in which they are less tiny, [look here](#bivariate-significance).

## Prediction from regression.

A regression line is our estimate of the *mean of y for a given x* (given the assumption that these conditional means fall on a line).  Since there is uncertainty inherent in all estimation, the slope and intercept are uncertain, and thus the estimated mean for a given x is also uncertain.  We can translate our standard errors of the slope and intercept into a standard error of the conditional mean of y for a given x.  This is our uncertainty about the *conditional mean of y*, and we can extract it from a given linear model using the `predict.lm` function, with `interval='coonfidence'` (Note that the syntax for the predict function is to provide the fitted model object (`m`) and a data frame of new observations that we want to make new predictions on):

```{r}
m <- lm(m <- lm(data=heights, Son ~ Father))
predict.lm(m, newdata = data.frame(Father=72), interval='confidence')
```

By default, this returns the estimated conditional mean of y, at the x values provided in `newdata`; here, the estimated mean height of sons whose fathers were 72" tall, and a 95% confidence interval on that mean.  

However, the confidence interval on the mean doesn't give us a good idea of what heights we might expect of these sons -- it tells us that on average they will be about 70.9" tall, but any given son will vary a lot from the mean.  To put a confidence interval on the height of a particular son we might see, we need not only take into account out uncertainty in estimating the mean height, but also how much variability there is in heights around the mean.  The confidence interval for that is given with the `interval = 'prediction'` option:

```{r}
predict.lm(m, newdata = data.frame(Father=72), interval='prediction')
```

This interval is much broader because, although we can estimate the best fitting line (and thus the mean) very well, the individual heights have a lot of variability around that line.  For more on these intervals, [read here](#bivariate-prediction)

