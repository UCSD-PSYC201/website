---
  output: 
    html_document
---
  
# "ANOVA"

An "Analysis of Variance" (anova) of a linear model partitions the overall variability in our response variable into the different sources considered by the model.  Here we talk about the special case of analyzing the variance partitioning of a linear model with only **categorical predictors**.


```{r echo=F, message=F, warning=F}
library(dplyr)
library(knitr)
library(kableExtra)
df <- data.frame(y = c(64, 67, 65, 66, 72), x=c(9.4,11.8,3.4,1.0,8.2))
df %>% kable('html') %>% kable_styling(bootstrap_options = "striped", full_width = F, position='float_right')
```
 
## Review: [anova of linear regression](bivariate-determination.html)


For instance, when we do a simple linear regression, fitting a model of them form $y_i = \beta_0 + \beta_i x_i + \epsilon_i$, we can partition the overall variability in $y$ (called "sum of squares total", $\operatorname{SST}=\sum_i(y_i-\bar y)^2$) into   
(1) the variability which is explained by the linear relationship with $x$ ("sum of squares of the regression"; $\operatorname{SSR}=\sum_i((\hat \beta_0 + \hat \beta_1 x_i) - \bar y)^2$ ) and    
(2) the left over -- residual -- variability ("sum of squares error"; $\operatorname{SSE}=\sum_i(y_i - (\hat \beta_0 + \hat \beta_1 x_i))^2$ ).    
We might then calculate an $F = \frac{\operatorname{SSR}}{\operatorname{SSE}/(n-2)}$ statistic and run an $F$ test to ask if more variance is explained by the regression (bigger SSR) than we would expect by chance if the real `y~x`  slope is 0 (the null hypothesis that $\beta_1=0$).   
All of this is done via the `R` command: `anova(lm(y~x))`.

We can analyze the partioning of variability for any linear model; however, it is most common to use the term "analysis of variance" when talking about a special class of linear models: ones that include only **categorical predictors** (rather than *numerical* predictors -- linear models with only numerical predictors we typically call "regression").


```{r echo=F, message=F, warning=F}
df <- data.frame(y = round(rnorm(10, 67,5)), 
                 sex=sample(c('male', 'female'), 10, replace = T)) %>%
  mutate(male = as.integer(sex=='male'), female = as.integer(sex=='female'))
df %>% kable('html') %>% kable_styling(bootstrap_options = "striped", full_width = F, position='float_right')
```

## Coding categorical predictors.

Let's say we have some response variable $y$ (e.g., height), and a **categorical** predictor variable (e.g., sex: \{male, female\}).  Adopting the linear model assumption that $y$ values are distributed normally around some (conditional) mean, we might adopt a model of:

$$y_i = \mu_{\mbox{sex}_i} + \epsilon_i$$

Where the notation $\mu_{{\mbox{sex}_i}}$ is our way of proposing a vector `sex` with a value of 'male' or 'female' for each observation $i$, and suggesting that $\mu$ is a vector of two means  $\mu_{\mbox{male}}$ and $\mu_{\mbox{female}}$, corresponding to the the mean height of males and females.  This is how we thought about two groups when considering a simple, equal variance, two-sample t-test.

An equivalent way of writing this, which looks more obviously like a standard linear model, is:

$$y_i = \beta_{\mbox{male}} \cdot \mbox{male}_i + \beta_{\mbox{female}} \cdot \mbox{female}_i + \epsilon_i$$
Now we are considering two indicator variables: $\mbox{male}_i$ (which is 1 for males and 0 for females), and $\mbox{female}_i$ (which is 0 for males and 1 for females).  Consequently, our overall model equation yields $y_i = \beta_{\mbox{male}} + \epsilon_i$ for males and $y_i = \beta_{\mbox{female}} + \epsilon_i$ for females.  So, now it is transparent that the $\beta_{\mbox{male}}$ and $\beta_{\mbox{female}}$ coefficients are effectively the means of males and females, just like $\mu_{\mbox{male}}$ and $\mu_{\mbox{female}}$ in our first way of writing out this model.

We can consider yet another, equivalent formulation of the same model:

$$y_i = \beta_{0} + \beta_{\mbox{male}} \cdot \mbox{male}_i + \epsilon_i$$
For females, this expression reduces to $y_i = \beta_0  + \epsilon_i$; for males, it will be $y_i = \beta_{0} + \beta_{\mbox{male}} + \epsilon_i$.  Thus, this model formulation defines the mean of females as $\mu_{\mbox{female}} = \beta_0$ (the intercept), and the mean of males as $\mu_{\mbox{male}} = \beta_0 + \beta_{\mbox{male}}$.  The slope $\beta_{\mbox{male}}$ thus corresponds to the difference between the male and female means.

This last formulation is the default for how `R` codes for categorical variables or "factors": it sets the first factor level to the intercept (by default, alphabetically first -- so here, "female"), and then codes for the difference between the first factor level and every subsequent level:

```{r}
lm(data = df, y~sex)
```

Note the 'sexmale' coefficient: its name reflects that it is coding for the 'male' level of the 'sex' categorical predictor, and the number associated with it is the difference between the mean of males and the mean of females.



```{r echo=F, message=F, warning=F}
df2 <- data.frame(y = round(rnorm(10, 67,5)), 
                 gender=c('Cisgender', 'Genderqueer/gender non-conform[ing]', 'Trans/transgender', 'Man', 'Woman', 'Other')[c(1:6,1:4)])
df2 %>% kable('html') %>% kable_styling(bootstrap_options = "striped", full_width = F, position='float_right')
```

### Many factor levels

If we have more than two factor levels, then `R`'s coding scheme will again set the first level (alphabetically) to the intercept, and then code for the differences between each other level and the first one:

```{r}
lm(data=df2, y~gender)
```

In this case we have 6 factor levels, thus we have 1 intercept, and 5 difference coefficients, with each one coding the difference in mean between the corresponding factor level and the first ('cisgender') level.

### Aside: alternate coding schemes.

There are many different ways to code for different means for different factor levels, which we will largely ignore, and just use the default coding scheme in `R`.  However, you can read descriptions of the managerie of coding schemes [from the UCLA statistical consulting group](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/).

### Aside: matrix formulation.

Why does the calculation of a linear model with different means for all factor levels involve postulating numerical "dummy coding"" variables?  This makes more sense if we realize that inside any statistical computing software, all least squares linear models are generally represented in linear algebra notation:

$$y = \mathbf{X}\beta + \epsilon$$

where $y$ and $\epsilon$ are $n \times 1$ vectors of the y values and residuals, respectively; $\beta$ is a $p \times 1$ vector corresponding to all the coefficients; and $\mathbf{X}$ is an $n \times p$ matrix of all the predictor variables.  

For instance, a simple linear regression (e.g., the `y~x` regrssion from the first table on this page) would be effectively treated as:

$$
\begin{bmatrix}
  y_1 \\
  y_2 \\ 
  y_3 \\ 
  \vdots \\ 
  y_n
\end{bmatrix} = 
\begin{bmatrix}
    1 & x_1\\
    1 & x_2 \\
    1 & x_3 \\
    \vdots & \vdots \\
    1 & x_n
\end{bmatrix}
\begin{bmatrix}
    \beta_0 \\
    \beta_1 
\end{bmatrix} + 
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\ 
  \epsilon_3 \\ 
  \vdots \\ 
  \epsilon_n
\end{bmatrix}
   \quad\quad 
   \quad\quad 
\begin{bmatrix}
  64 \\
  67 \\ 
  65 \\ 
  66 \\ 
  72
\end{bmatrix} = 
\begin{bmatrix}
    1 & 9.4 \\
    1 & 11.8 \\
    1 & 3.4 \\
    1 & 1.0 \\
    1 & 8.2
\end{bmatrix}
\begin{bmatrix}
    \beta_0 \\
    \beta_1 
\end{bmatrix} + 
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\ 
  \epsilon_3 \\ 
  \epsilon_4 \\ 
  \epsilon_5 
\end{bmatrix}
$$

Fitting the $\beta$s is accomplished via the very general expression:

$$\hat \beta = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty$$

This works for whatever prediction variables you stick inside the matrix $\mathbf{X}$ (provided they aren't linearly redundant), so the process of regression with categorical predictors requires coming up with some numerical variables that can code for the differences in means across categories.

You can see the matrix $\mathbf{X}$ constructed by `R` to encode your particular formula (e.g., `y~sex`) for a particular data set via the `model.matrix()` command:


```{r}
model.matrix(lm(data = df, y~sex))
```


```{r echo=F, message=F, warning=F}
ms = c('USA'=69, 'North Korea'=65, 'South Korea'=67, 'Netherlands'=71)
df3 <- data.frame(height = round(rnorm(100, 0,5)), 
                 country=sample(c('USA', 'North Korea', 'Netherlands', 'South Korea'), 100, T)) %>%
  mutate(height = height + ms[country])
df3 %>% head(10) %>% kable('html') %>% kable_styling(bootstrap_options = "striped", full_width = F, position='float_right')
```

## anova of an anova (linear model with categorical predictor)

Let's say we have heights for men from different countries (first 10 rows shown in the table on the right).  

```{r}
library(ggplot2)
df3 %>% ggplot(aes(x=country, y=height, color=country))+
  geom_point(position=position_jitter(0.25))+
  stat_summary(fun.data = mean_se, geom = 'pointrange', size=0.75, width=2, color='black')
```

The linear model of `height~country` would find the coefficients coding for how the mean height varies across countries.  With the four countries, to code for differences in means from the intercept (Netherlands) we need 3 coefficients (North Korea - Netherlands, South Korea - Netherlands, and USA - Netherlands):

```{r}
lm(data=df3, height~country)
```

These coefficients correspond to:   
- Intercept: mean height of the Netherlands.   
- countryNorth Korea: difference of mean(NK)-mean(netherlands)   
- countrySouth Korea: difference of mean(SK)-mean(netherlands)   
- countryUSA: difference of mean(USA)-mean(netherlands)  


```{r}
dfmu = df3 %>% group_by(country) %>% summarize(m=mean(height))
dfmu$comp = c(0, rep(filter(dfmu, country=='Netherlands')$m,3))
library(ggplot2)
df3 %>% ggplot(aes(x=country, y=height, color=country))+
  geom_point(position=position_jitter(0.25))+
  geom_hline(data = filter(dfmu, country=='Netherlands'), aes(yintercept = m), color='gray')+
  geom_segment(data = dfmu, 
               aes(x=as.integer(country)-0.4,
                   xend=as.integer(country)+0.4, 
                   y=m, 
                   yend=m), color='black')+
  geom_segment(data = dfmu, 
               aes(x=as.integer(country)-0.4,
                   xend=as.integer(country)-0.4, 
                   y=comp, 
                   yend=m), color='gray', arrow=arrow(length=unit(0.01, "npc")))+
  theme_minimal()+
  theme(panel.grid = element_blank())
```

Thus, when we look at the `anova` of this linear model, the degrees of freedom for the 'country' factor are not 1 (as was the case for every isolated numerical predictor in standard regression), but 3.  Because we have three coefficients (country North Korea, country South Korea, and country USA) coding for differences among countries.

```{r}
anova(lm(data=df3, height~country))
```

In general, the degrees of freedom for a categorical predictor is $k-1$, where $k$ is the number of categories.

Otherwise, the sum of squares, mean squares, F statistic, and p value have the same meaning as in a regular least squares regression:

```{r, fig.width=10, fig.height=5, echo=FALSE}
cur.theme = theme_minimal()+
  theme(legend.position = 'none',
        axis.text.x = element_text(angle=90, hjust=0.5, vjust=1),
        panel.grid = element_blank())
df3 <- df3 %>% mutate(global.mean = mean(height)) %>%
  group_by(country) %>%
  mutate(country.mean = mean(height),
         country.pos = ((1:n())/n()-0.5)*0.8)
p1 <- df3 %>%  ggplot(aes(x=as.integer(country)+country.pos, y=height, color=country))+
  geom_point()+
  geom_hline(aes(yintercept = mean(df3$height)), color='gray')+
  geom_segment(data = dfmu, 
               aes(x=as.integer(country)-0.4,
                   xend=as.integer(country)+0.4, 
                   y=m, 
                   yend=m), color='black')+
  scale_x_continuous(name='country', breaks=c(1,2,3,4), labels = dfmu$country)+
  geom_segment(aes(x=as.integer(country)+country.pos, y=height, yend=global.mean, xend = as.integer(country)+country.pos), color='gray')+
  ggtitle('Sum of squares total')+cur.theme
p2 <- df3 %>%  ggplot(aes(x=as.integer(country)+country.pos, y=height, color=country))+
  geom_point()+
  geom_hline(aes(yintercept = mean(df3$height)), color='gray')+
  geom_segment(data = dfmu, 
               aes(x=as.integer(country)-0.4,
                   xend=as.integer(country)+0.4, 
                   y=m, 
                   yend=m), color='black')+
  scale_x_continuous(name='country', breaks=c(1,2,3,4), labels = dfmu$country)+
  geom_segment(aes(x=as.integer(country)+country.pos, y=country.mean, yend=global.mean, xend = as.integer(country)+country.pos), color='gray')+
  ggtitle('Sum of squares regression (country)')+cur.theme
p3 <- df3 %>%  ggplot(aes(x=as.integer(country)+country.pos, y=height, color=country))+
  geom_point()+
  geom_hline(aes(yintercept = mean(df3$height)), color='gray')+
  geom_segment(data = dfmu, 
               aes(x=as.integer(country)-0.4,
                   xend=as.integer(country)+0.4, 
                   y=m, 
                   yend=m), color='black')+
  scale_x_continuous(name='country', breaks=c(1,2,3,4), labels = dfmu$country)+
  geom_segment(aes(x=as.integer(country)+country.pos, y=country.mean, yend=height, xend = as.integer(country)+country.pos), color='gray')+
  ggtitle('Sum of squares error/residuals')+cur.theme

gridExtra::grid.arrange(p1,p2,p3, ncol=3)
```

As in all anovas, the "mean squares" for a given term is that term's sum of squares, divided by its degrees of freedom.  Likewise, the F statistic for a given term is the mean squares for that term, divided by the mean squares of the error term.

$$F(\mbox{df}_\mbox{term}, \mbox{df}_\mbox{error}) = \frac{\operatorname{SS}[\mbox{term}]/\mbox{df}_\mbox{term}}{\operatorname{SS}[\mbox{error}]/\mbox{df}_\mbox{error}}$$

And the p-value is the upper tail of the corresponding F distribution:  `1-pf(F,df_1, df_2)`.
