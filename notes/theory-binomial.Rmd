---
output: 
    html_document
---

# From probability to statistics with the Binomial

These notes are (a) very mathy, (b) very optional, and (c) cover a very broad range of material that we will not directly address this term.  I hope that for especially advanced and ambitious folks, this kind of theoretical exposition of what is done, and why, in various statistical methods will be helpful.  For folks who are less comfortable with math, this will be a whole lot of difficult material that is likely to increase, rather than reduce, confusion.


Let's say our data are the outcomes of 10 coin flips obtained by flipping a coin 10 times:  
$X = \{x_1, x_2, ..., x_10\} = \{H,H,H,H,H,H,H,H,T,T\}$.

## Data description / summary

The first thing we might do is summarize the data.  Whenever we summarize data we inevitably make some assumptions about which aspects of our data are important, and which are not.  Often, these assumptions reflect our hypotheses about how the data were generated, and which aspects of the data ought to be relatively stable across samples.   "Descriptive statistics" is usually used as the opposite of "inferential statistics", suggesting that when we choose some way to summarize the data, we are in no way drawing inferences or attempting to generalize from the sample.  This is true only technically, but not practically, since our data descriptions often tend to coincide with estimators of model parameters.  Hopefully this will become more clear as this section continues.

Let's start with our data: 10 outcomes of a coin flip: H,H,H,H,H,H,H,H,T,T.

How should we summarize the data?  Well, we can first tabulate the frequencies of different outcomes, and if we are so inclined, we can plot these frequencies as a histogram.

| outcome | frequency |   
|:------|------:|   
| H       | 8         |   
| T       | 2         |

By summarizing the frequencies of heads and tails, we are throwing out information about the order in which those data points came.  This is a useful summary insofar as we believe that the order doesn't matter.  The order doesn't matter if we believe that those particular H/T outcomes are independent (if the flip of a coin does not depend on the previous outcome of the coin flip, or any other coin flip at all).  

However, if we believed that our data came from a coin flipping process where the coin-flipper tended to flip the coin an even number of times, that means that the second outcome will tend to be like the first outcome, and the third like the second, etc.  Thus, the individual coin flips are not independent, because they were generated by a process that tends to reproduce the same outcome in succession.  If we thought that such a process was underlying out data, then instead of tabulating the raw frequencies of heads and tails we might choose to tabulate how often the outcomes were different in consecutive pairs of flips (note that while we have 10 flips, we only have 9 consecutive pairs).  By choosing to tabulate repetitions/swaps, we throw out information about whether the coin was heads or tails, and which came up more often:

| outcome | frequency |   
|:------|------:|    
| same    | 8         |   
| different | 1       |

Which of these histograms is a more useful summary depends on our beliefs about how the data were generated, and which features of the data are more or less important.  It is important to note that both summaries throw out some information while highlighting some other information.

In addition to tabulating frequencies, often data are summarized with some ["statistics"](../introduction/descriptive.html).  Any function of the data that returns some simple number (not always, but usually just one number), is called a **statistic**.  Some of these statistics are more useful than others because they have useful properties and serve as estimators for parameters of common models.  All of these summarize some aspect of the data, while ignoring other aspects; thus in choosing a particular summary statistic you effectively presume which aspects of your data are important, and which can be disregarded.

For instance, for our data we might consider a number of statistics:  

$\frac{\text{# heads}}{\text{total}} = 0.8$, or   

$\frac{\text{# heads}}{\text{# tails}} = 4.0$, or   

"position of first occurrence of tails" $= 9$, or    

$\frac{\text{# repeats}}{\text{# consecutive pairs}} = 0.\bar8$, etc.  

In short, the choice of which aspects of the data to summarize necessarily reflects some implicit beliefs about the structure that ought to be present in the data.  Similarly, when we decide to present some [graphical summary](../visualization/visualization.html) of the data, we will also need to choose which aspects of the data to emphasize, and which can be obscured. Therefore the choice of which descriptive statistics to report and consider should be done with ample thought and care (like everything else in scientific research)

## Estimation

Descriptive statistics are somewhat motivated by an (implicitly) assumed model of the data, if we make this process explicit, we will postulate a particular [statistical model](../introduction/uses-of-models.html) of the data.  We usually call this the *population* model, which will have some unknown **parameters**.  For instance, we might suppose that our sequence of coin flips reflect independent, identically distributed outcomes from the flip of a bent coin that comes up heads with probability $\theta$ (theta; note we are now switching to using greek letters for population parameters), and tails with probability $1-\theta$. Thus our model of the data is that the number of heads, $k$, out of $n$ flips is a sample from a Binomial: $k \sim \operatorname{Binomial}(k \mid n, \theta)$, and we need to use our sample of coin flips to estimate the population parameter (obtaining the estimate $\hat\theta$).

### Point Estimates

Point estimates are a single value, corresponding to our best guess about the latent parameter; these are contrasted with *interval estimates* which provide a range of plausible values that the parameter might be.  We will consider two approaches to estimating parameters, in our case just $\theta$: **Maximum Likelihood** and **Maximum A Posteriori**. 

There are others methods for estimation, but explaining how they work, and the logic behind them, is rather convoluted.  Moreover, there is rarely any reason to prefer them.  Popular alternative approaches include "Method of Moments" and "Minimum squared error" (a.k.a. "Least squares").  Later in the course we will talk about least squares estimates, but this will be in the context of models where least squares error estimates are equivalent to the maximum likelihood estimates, so while we will use the least squares estimation procedure, we can think of it as a convenient way to obtain a maximum likelihood estimate.  (One notable exception is estimators for the variance.)

#### Classical (Maximum Likelihood) estimate

The Maximum Likelihood estimate (ML estimate) aims to find the parameter value $\theta$ which makes the data most likely, thus maximizing the likelihood function.  

What is the **likelihood function**?  

Consider the conditional probability $P(X|\theta)$, this assigns probabilities to different data outcomes for a given $\theta$. For instance:

$P(3 \text{ heads out of } 8|\theta=0.8) = \operatorname{Binomial}(3 \mid 8, 0.8) = {8 \choose 3} 0.8^3(1-0.8)^7$ = `r dbinom(3, 8, 0.8)`   
$P(7 \text{ heads out of } 8|\theta=0.8) = \operatorname{Binomial}(7 \mid 8, 0.8) = {8 \choose 7} 0.8^7(1-0.8)^1$ = `r dbinom(7, 8, 0.8)`   

If we consider $P(X|\theta,n)$ as a function over possible $X$s, assigning probability to different data outcomes while $\theta$ remains fixed, then $P(X|\theta,n)$ is a conditional probability distribution over possible data sets (so if you sum $P(X|\theta,n)$ over all possible $X$s, you will get 1.)

In contrast, the likelihood function, usually written as $\mathcal{L}(X|\theta)$, is not a probability distribution.  It is a collection of $P(X|\theta)$ values for different values of $\theta$ while $X$ is fixed.

The likelihood function does not sum to 1 when you sum over all $\theta$s; however, it does indicate how likely the data are under each possible $\theta$:  

$\mathcal{L}(3 \text{ heads out of } 8|\theta=0.2) = \operatorname{Binomial}(3 \mid 8, 0.2) = {8 \choose 3} 0.2^3(1-0.2)^7$ = `r dbinom(3, 8, 0.2)`   
$\mathcal{L}(3 \text{ heads out of } 8|\theta=0.4) = \operatorname{Binomial}(3 \mid 8, 0.4) = {8 \choose 3} 0.4^3(1-0.4)^7$ = `r dbinom(3, 8, 0.4)`   
$\mathcal{L}(3 \text{ heads out of } 8|\theta=0.6) = \operatorname{Binomial}(3 \mid 8, 0.6) = {8 \choose 3} 0.6^3(1-0.6)^7$ = `r dbinom(3, 8, 0.6)`   
$\mathcal{L}(3 \text{ heads out of } 8|\theta=0.8) = \operatorname{Binomial}(3 \mid 8, 0.8) = {8 \choose 3} 0.8^3(1-0.8)^7$ = `r dbinom(3, 8, 0.8)`   

The likelihood function effectively describes how well different values of our parameter $\theta$ describe the data $X$.  If we choose the maximum point of this function, we choose the parameter value that maximizes the likelihood of the data.

We can plot the likelihood function for our data ($X= \{H,H,H,H,H,H,H,H,T,T\}$) below:

```{r}
data = c('H','H','H','H','H','H','H','H','T','T')
n.heads = sum(data=='H')
n.total = length(data)
thetas = seq(0,1,by=0.001)
lik.theta = dbinom(n.heads, n.total, thetas)
library(ggplot2)
ggplot(data.frame(theta=thetas, likelihood=lik.theta), aes(theta, likelihood))+geom_line()
```

We can find the value of $\theta$ that maximizes the likelihood with:

```{r}
thetas[which(lik.theta==max(lik.theta))]
```

The $\theta$ for which the likelihood is greatest is 0.8.  Thus, our maximum likelihood estimate is $\hat \theta_{ML} = 0.8$.  The hat operator ($\hat \cdot$) is convention for indicating an estimate of a parameter value, the subscript usually denotes the type of estimator used to obtain the estimate (here "ML" for "maximum likelihood").  Note that for many statistical models, the maximum likelihood estimate need not be calculated numerically, but has an analytical solution; we will go over these when appropriate in the future, but for now, I want to convey what the maximum likelihood estimator means.

#### Bayesian (Maximum a posteriori) estimate

The maximum a posteriori estimate (MAP estimate) uses Bayes' rule to invert the probability, to obtain $P(\theta|X)$ from $P(X|\theta)$.  Of course, to do so one must specify some sort of prior distribution over the parameter $\theta$ ($P(\theta)$; this need to choose a prior is one reason why some are uneasy about Bayesian estimation).  Let's say we believe that a coin is unlikely to be biased too far away from 0.5, so our prior will be centered on 0.5.  We can express such a prior with a [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution).  

$P(\theta)= \operatorname{Beta}(2,2)$  This is the prior on $\theta$.   

So now, we can calculate the posterior as:

$P(\theta|X) = \frac{\mathcal{L}(X|\theta)P(\theta)}{\int\limits_0^1 \mathcal{L}(X|\theta)P(\theta)d\theta}$    
(Note that the denominator comes from the law of total probability.)

In practice we could use an analytical solution for this posterior distribution (the Beta distribution, by virtue of it being a [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior) for the Binomial); however, here we can just do it via a crude numerical estimate.

```{r}
p.theta = dbeta(thetas, 2, 2)
p.theta = p.theta/sum(p.theta)
p.data = sum(lik.theta*p.theta)
p.theta.data = lik.theta*p.theta / p.data
p.theta.data.uni = lik.theta/sum(lik.theta)

ggplot(rbind(data.frame(theta=thetas, probability=p.theta, whichone="beta prior"),
              data.frame(theta=thetas, probability=p.theta.data, whichone="posterior (beta)"),
              data.frame(theta=thetas, probability=p.theta.data.uni, whichone="posterior (uniform)")), 
      aes(theta, probability, color=whichone))+geom_line()

thetas[which(p.theta.data == max(p.theta.data))] # Maximum a posteriori
```

Note that  
(a) the posterior distribution under a uniform prior (which is just the normalized likelihood), has the same shape and peak as just the likelihood function,     
(b) the posterior under a non-uniform prior gives us an estimate somewhere between the likelihood and the prior.  

Here the the maximum a posteriori (MAP) estimate is 0.75; we would write this as $\hat \theta_{MAP} = 0.75$.  (the MAP estimate under a uniform prior gives us the same estimate as the maximum likelihood).

#### Estimators

While the two approaches we have described are general, all-purpose methods to obtain ML and MAP estimates, typically, we will not work through all of this.  Instead we will use special types of statistics (again, statistics meaning "functions of the data") which someone else has proven to yield the appropriate estimates: these are called **estimators**.  For instance, $k/n$ is a maximum likelihood estimator for the Bernoulli/Binomial $\theta$, in the sense that it will always give the $\theta$ value which maximizes $\mathcal{L}(D|\theta)$.  The sample mean (which we will get to later) $\bar x = \sum_{i=1}^n x_i / n$ is the maximum likelihood estimator for the mean ($\mu$) when we assume the data come from a Gaussian, because, again, it will always maximize the Gaussian likelihood.  So, estimators are functions of the data that yield useful estimates.  

Not all estimators map neatly on to the ML estimate, and sometimes this is quite desirable.  For instance, the maximum likelihood estimate of the variance is biased because of the skewed shape of the likelihood function for variance.  In this case, a different estimator is used (which can be thought of as a correction to the ML estimate).

### Interval Estimates

Our point estimates, $\hat \theta_{ML}$ and $\hat \theta_{MAP}$, are not particularly useful on their own, because they are bound to have some error/uncertainty due to sampling variability.  In other words: if we flip the same coin 10 more times, we will get different outcomes, and the number of heads will not be exactly 8; thus, the 8/10 estimate from our data arises from the idiosyncrasies of our specific sample.  Consequently, instead of providing a single best guess (a point) as an estimate, it is usually preferable to provide a range (interval) of guesses around our best estimate, to indicate the expected precision of our estimate.  Usually these are expressed as a percentage (e.g., a 95\% interval).  And for the general case we will often refer to a $100*q\%$ interval (where $q$ is between 0 and 1).

However, when providing such interval estimate, the difference between frequentist and Bayesian approaches to statistics becomes quite important.

#### Bayesian Credible Intervals

I will start by describing Bayesian interval estimates, called "credible intervals", because their definition, interpretation, and procedure for calculation is intuitive -- it is what most people think of when they read "95\% interval estimate of men's shoe size is 7 to 11", or other such statements.  

$[a,b]$ is a $100*q\%$ credible interval for our parameter $\theta$ if $P(a \leq \theta \leq b | X) = q$.  Which should be intuitive: $[a,b]$ is a $100q\%$ credible interval for our parameter $\theta$ if the posterior probability that $\theta$ is between $a$ and $b$ is $q$. 

In the previous section we calculated the posterior distribution of $\theta$: $P(\theta|X)$.  From this distribution we can compute the cumulative distribution function: $F_{\theta}(\theta') = P(\theta \leq \theta' | X)$.  This cdf has an intuitive interpretation: what is the probability that the parameter value $\theta$ is less than or equal to $\theta'$?  

Thus, if we aim to obtain a $100q\%$ interval, we need to find values $a$ and $b$ such that $F_{\theta}(b) - F_{\theta}(a) = q$.  There are many pairs of values for which this will be true, but some of them are more useful than another.  It is generally most convenient to take an interval that leaves equal amounts of probability on either side (thus guaranteeing to include the median, so that $F_{\theta}(a) = 0.5 - q/2$ and $F_{\theta}(b) = 0.5 + q/2$. (Note that this procedure does not guarantee that the MAP estimate -- the mode of the posterior distribution-- will be included in the interval, but it is easiest to define an interval around the median; moreover, when dealing with wide intervals [when $q$ is quite large], it doesn't much matter.)

For instance, if we use our numerical estimates from before, we can calculate an approximation of the cdf, and estimate quantiles from it numerically.

```{r}
cum.probability = cumsum(p.theta.data)
ggplot(data.frame(theta=thetas, cumulative.prob=cum.probability), aes(theta, cumulative.prob))+geom_line()
range(thetas[cum.probability>=0.025 & cum.probability<=0.975])
```

Obviously, the validity of our estimates here is contingent on the granularity of our numerical approximation: if we consider only a small number of thetas, we cannot estimate an interval estimate very precisely.  

These credible intervals are constructed from the posterior distribution of the parameter given the data; thus we can make statements like "given the data, (and our prior), the probability that $\theta$ is between 0.46 and 0.91 is 95%". 

#### Frequentist Confidence Intervals

Frequentists are not as lucky as Bayesians when it comes to constructing interval estimates.  According to the philosophy of frequentist statistics, there exists a true parameter value in the world.  This value is fixed, and it is meaningless to assign probabilities to different possible parameter values (because probabilities are relative frequencies, and the parameter value will always be the same -- it is fixed).  Consequently, a given interval estimate either includes, or does not include, the true parameter value, and again, we cannot assign a probability to the statement that "the true parameter value is within this interval" (that statement is either true or false, and it will always be either true or false, so we can't assign it a relative frequency).  However, while we cannot make frequentist probability statements about a particular estimate, we can make probability statements about our *procedure*, so we might say, "the procedure that I used for constructing this 95% confidence interval, if repeated many many times in different experiments, will contain the respective true parameter values 95% of the time".

The Wikipedia confidence interval page provides the rather unwieldy formal definition of a confidence interval which I will simplify for our case.  We can define two functions of the data (statistics) $L(X)$ and $U(X)$.  If the interval $[L(X), U(X)]$ is a $100q\%$ confidence interval then if we generate many data sets from a Bernoulli distribution with a parameter $\theta$, and we construct a lower and upper bound using the functions $L(\cdot)$ and $U(\cdot)$ for each data set,  then the parameter $\theta$ will be within those bounds $q$ proportion of the time.  

Thus, the $100q\%$ associated with a confidence interval statement is not a statement about the parameter value, but a statement about the functions $L()$ and $U()$: those functions have the property that 95% of the intervals constructed using these functions will contain the true parameter value for that experiment.  This is why frequentist confidence intervals are confusing.

That's all well and good, but how do we actually obtain a confidence interval in our case of 8 heads and 2 tails?  [A number of approaches](http://en.wikipedia.org/wiki/Binomial\_proportion\_confidence\_interval) to constructing intervals are available that have (approximately) the required property described above.   The most common relies on a normal approximation (which works reasonably provided you have enough data and $\theta$ is not too close to 0 or 1).  This confidence interval with $q=1-\alpha$, is defined as:   
$\hat\theta \pm Z_{\alpha/2} \sqrt{\frac{\hat\theta(1-\hat\theta)}{n}}$,   
where $Z_{\alpha/2}$ is the z-score for $(\alpha/2)$ -- the $(\alpha/2)$ quantile of the standard normal distribution (we will discuss this later).  Since we haven't yet talked about the normal distribution, we will not discus this approximate confidence interval further until we are really interested in categorical data; plus, that interval estimate will be inappropriate because of our small sample size).

#### Confidence intervals from null hypothesis tests

Instead of using a confidence interval from the normal approximation, we will consider the "exact" frequentist confidence interval, purely for pedagogical (not practical) purposes.  Under this approach, we construct a confidence interval via significance testing.  This yields the second interpretation of a frequentist confidence interval:  "A $100q\%$ confidence interval for a parameter $\theta$ is the range of values that $\theta_0$ (the null hypothesis value of $\theta$) could take on without you being able to reject the null hypothesis with your data at significance level $\alpha = 1-q$".  We will now construct a confidence interval for our Bernoulli $\theta$ in this manner (Although normally this could done analytically, here we will go through the exercise of calculating it numerically).

We will use the `binom.test` function to get a p.value for the null hypothesis binomial test, and obtain this p-value for every theta we can consider, then keep the thetas we could not reject, and define our interval based on the range of the theta values we could not reject.

```{r}
p.val = sapply(thetas, function(theta){min(binom.test(n.heads, n.total, theta, alternative = "greater")$p.value,
                                           binom.test(n.heads, n.total, theta, alternative = "less")$p.value)})
not.rejected.thetas = thetas[p.val>(1-0.95)/2]
range(not.rejected.thetas)
```

So the interval we get out is [0.45 0.96].

## Null Hypothesis Significance testing (NHST)

Hypothesis testing typically follows these steps:

1) First, you define a null hypothesis, usually written as $\mathcal{H}_0$.  This null hypothesis should capture some belief about the population (or data distribution), which is the default/standard, or the dull "no discovery here" current belief.  For instance, if comparing salaries between men and women, you might want the null hypothesis to be "they have the same average".  Or if you are Kepler measuring planetary orbits, you might pose the null hypothesis that orbits are circular (rather than elliptical).  Or if you are measuring coin flips, as we are, your null hypothesis could sensibly be "they are independent, identically distributed samples from a fair coin ($X\sim \text{Bernoulli}(\theta=0.5)$)".  Note that our null hypothesis here is "parametric" in the sense that it makes a claim about the distribution of the data by specifying a parametric distribution of the data.

2) Second, you define a "test statistic".  This is like like any other statistic (a function of the data which returns a value), but it will serve as the basis for your significance test.  Let's call the value you obtain for your test statistic $v_x$ (for "value (from the data $x$)").  For us, a sensible choice of test statistic would be "the number of heads" observed: $v_x = 8$.

3) Third, you obtain (look up somewhere, compute numerically, or calculate analytically) the "sampling distribution" for this test statistic under the null hypothesis.  In other words, if the null hypothesis were true, what values of the test statistic would you expect to see, with what frequency.  We can refer to this distribution as $P_0(V)$.  This is the "sampling distribution of V" where V is the test statistic of interest under the null hypothesis.  This is often simply called the "null hypothesis distribution" (beware that this creates some ambiguity, since the null hypothesis distribution also refers to the distribution of possible data under the null hypothesis.)  Fortunately, we have already calculated the sampling distribution for our "number of heads" test statistic: the Binomial distribution.

4) Fourth, you compare where your observed test statistic ($v_x$) falls with respect to the null-hypothesis sampling distribution of the test statistic ($P_0(V)$).  Your goal is to ascertain the probability under the null hypothesis of observing a test-statistic value at least as "extreme" as the one you calculated of your data.  $P_0(V \geq v_x) = \text{p-value}$.  (We are doing a one-tailed test here -- not because it is the right thing to do, but because it is easier to describe as a start.) This is the "p value" -- the probability that a test statistic at least as extreme as the one you observed would arise from the null hypothesis distribution.  You will then reject the null hypothesis if this probability is small enough, and if the probability is not sufficiently small, you will "fail to reject" the null hypothesis. (You will never "accept" the null hypothesis, since NHST follows Karl Popper's "falsificationist" approach to the philosophy of science: meaning that science can only falsify theories, but never validate them.)

There are a number of variations on hypothesis testing, but the essence remains the same.  

The traditional procedure is to choose a "significance level" some time before step 4.  A significance level, usually denoted as $\alpha$ ("alpha"), indicates the rate at which you are willing to falsely reject the null hypothesis.  The traditional procedure suggests that you use $\alpha$ to compute a critical value $v_{crit}$, defined (for our one tailed test) as $P_0(V \geq v_{crit})=\alpha$.  The null hypothesis is rejected if your test statistic is more extreme than the critical value ($v_x \geq v_{crit}$).  Note that if $v_x \geq v_{crit}$, then, necessarily $\text{p-value} \leq \alpha$, these are exactly the same condition for rejecting the null hypothesis.  In the past the typical approach was to choose one of a few standard $\alpha$ values, then find the critical statistic; this was the case because one could not calculate a p-values due to lack of computers, and folks relied on pre-calculated tables to find specific critical values.  Nowadays, it is much more common to simply report the calculated p-value, because people can.

Choosing $\alpha$ is quite arbitrary, but according to tradition in psychology and social science, $\alpha=0.05$. This tradition arose because Ronald Fisher said (paraphrasing), "Being wrong 1 out of 20 times seems fine".  Note that the standards in other fields are considerably higher, for instance, in physics, the standard for a discovery is "six-sigma" -- an observation that is 6 standard deviations away from the average of the noise -- which amounts to $\alpha=10^{-9}=0.000000001$.  There is hardly ever any reason to prefer an $\alpha$ value that is higher than 0.05 (that is, to decide that you are content with a higher than 1 in 20 chance of false positives), but there is often reason to choose a lower alpha value.  When p-values are between 0.1 and 0.05, they are often called "marginally significant" and a number of other [awkward phrases](https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/).  

There are other variations (involving the directionality of the test, but we will cover those later).

Ok, so let's say we want to test the null hypothesis $\mathcal{H}_0: D \sim \text{Bernoulli}(\theta_0 = 0.5)$, via the test statistic "number of heads" ($v$).  We have previously introduced the Binomial distribution, which, conveniently, gives us the sampling distribution of our test statistic under our null hypothesis.  Recall that the $\text{Binomial}(k|n,\theta)$ distribution distributes probability over the number of successes $k$ out of $n$ attempts, with a probability of success $\theta$. 

So in our case:  
$P_0(V=v) = P(K=k|n=10,\theta_0=0.5)=\text{Binomial}(k|n=10,\theta_0=0.5)$.

```{r}
n.total = length(data)
n.heads = sum(data=='H')
ks = seq(0,n.total)
ggplot(data.frame(n.heads = ks, p.H0=dbinom(ks, n.total, 0.5), geq=ks>=n.heads), 
       aes(x=n.heads, y=p.H0, fill=geq))+geom_bar(stat="identity")
```

Let's work through this graph.   The x axis shows $k$: the number of heads one might get from $n$ flips. The y axis shows the the probability of obtaining $k$ heads out of $n=10$ flips where each flip is heads with probability $\theta_0=0.5$; in other words, this is our null hypothesis distribution for the test statistic $k$.  The colors of the bars indicate whether or not this value of $k$ is at least as large as the number we saw in our sample.  The total probability of the bars where $k$ is at least as large as n.heads is the p-value for the one (upper) tailed null hypothesis test.  

```{r}
p.value = sum(dbinom(ks[ks>=n.heads],n.total,0.5))
p.value
```

We can then compare the p.value to $\alpha=0.05$ to see if it is significant.

```{r}
p.value <= 0.05
```

Note that we did a one-tailed test, meaning we only looked for extreme outcomes on one side of the distribution (8 or more).  A two-tailed test would also consider equally extreme outcomes on the other side (2 or fewer heads out of 10).  A two tail test distributes the probability $\alpha$ to both tails (but this is somewhat tricky to do with potentially asymmetric binomial distributions, so let's skip it for now).

The statistical test we just described is often called the [Binomial test](https://en.wikipedia.org/wiki/Binomial_test) (obviously because it is based on the binomial distribution), which might be run with `binomial.test()` in R.  While it is useful for testing null hypotheses about the probability underlying a bunch of data that comes from Bernoulli trials, it is used more often to test null hypotheses about the [median or sign](https://en.wikipedia.org/wiki/Sign_test).

## Model selection

Thus far every individual analysis we have done has assumed a particular model for our data: for instance, we assumed that the coin flips are independent, but perhaps biased; or the converse: the coin flips are not independent, but unbiased with respect to heads/tails). How do we decide between these two models?  This is the domain of model selection, and all model selection approaches have a similar structure: they find a tradeoff between how well a given model can fit the data and how complicated the model is.

At the start of the estimation section we proposed two different models of our data ($\{H,H,H,H,H,H,H,H,T,T\}$); we now add a third:

$M_0$: The data are independent samples from a Bernoulli distribution with some unknown parameter $\theta$  (this is the intuitive model of coin-flips, so each one is independent of the others)

$M_1$: The data are sequentially dependent samples, the first one is H or T with probability 0.5, and $x_i$ is the same as $x_{i-1}$ with probability $\theta$, and different with probability $(1-\theta)$.  (This model makes sense if you suppose that the person flipping the coin is able to flip the coin an even number of times more often than an odd number of times -- or vice versa).

$M_2$: The data are sequentially dependent samples, the first one is H or with probability $\theta_1$, and $x_i$ is the same as $x_{i-1}$ with probability $\theta_2$, and different with probability $(1-\theta_2)$.

Which model is a better description of our data?  

### Penalized (maximum) likelihood selection criteria

First, let's consider the [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) to choose a model.  AIC is defined as: 

$\operatorname{AIC} = 2m - 2 \log\left({\mathcal{L}(D \mid \hat \theta_{ML})}\right)$

Where $m$ is the number of parameters (1 in $M_0$ and $M_1$, and 2 in $M_2$), $\hat \theta_{ML}$ is the maximum likelihood estimate of those parameters; therefore, $\mathcal{L}(D \mid \hat \theta_{ML})$ is the maximized likelihood of the data $D$ under the given model.  The better model is the one with the lowest AIC value.  

This model selection "criterion" has all the usual properties of model selection methods: it trades off model fit to the data ($- 2 \log\left({\mathcal{L}(D \mid \hat \theta_{ML})}\right)$) while penalizing models with more parameters ($2m$).  

We will write out the likelihood functions for the three models.  

For $M_0$:

$\mathcal{L}_0(X \mid \theta) = \prod\limits_{i=1}^n P(x_i \mid \theta) = \theta^k(1-\theta)^{n-k}$, where $k$ is the number of heads, and $n$ is the length of the sequence.

For $M_1$:

$\mathcal{L}_1(X \mid \theta) = 0.5\prod\limits_{i=2}^n P(x_i \mid \theta,x_{i-1}) = 0.5\theta^k(1-\theta)^{n-k-1}$, where $k$ is the number of repetitions, and $n$ is the length of the sequence.

For $M_2$:

$\mathcal{L}_2(X \mid \theta_1,\theta_2) = P(x_1|\theta_1)\prod\limits_{i=2}^n P(x_i \mid \theta_2,x_{i-1}) = \theta_1^a(1-\theta_1)^{1-a}\theta_2^k(1-\theta_2)^{n-k-1}$, where $a=1$ is the first coin of the sequence is heads, and 0 otherwise; $k$ is the number of repetitions; and $n$ is the length of the sequence.

In the interest of space, I will not write out the exercise of finding the maximum likelihood parameters for these models, but they are:    

$M_0: \hat \theta_{ML} = 0.8$;    
$M_1: \hat \theta_{ML} = 8/9 = 0.\bar8$;    
$M_2: \hat \theta_{ML} = [1.0, 0.\bar8]$

We can compute the AIC values for each model:\\
$\operatorname{AIC}_0 = 2(1)-2\log \mathcal{L}_0(D \mid \hat \theta_{ML}) = 2-2 \log (0.00671) = 12$   
$\operatorname{AIC}_1 = 2(1)-2\log \mathcal{L}_1(D \mid \hat \theta_{ML}) = 2-2 \log (0.02165)  = 9.7$   
$\operatorname{AIC}_2 = 2(2)-2\log \mathcal{L}_2(D \mid \hat \theta_{ML}) = 4-2 \log (0.0433)  = 10.3$

So of the three models proposed, $M_1$ wins, in the sense that it has the lowest AIC value.  So, our data are best described as sequentially dependent; however, the increase in maximized likelihood obtained from the more complicated $M_2$ does not sufficiently offset the extra parameter.  This is generally how penalized maximum likelihood model selection works (e.g., [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) differs only in that it has a slightly different magnitude of penalty for each additional parameter).

### Bayesian model selection (and Bayes Factors)

Bayesian model selection works a bit differently than penalized maximum likelihood criteria.  Instead of adopting an ad-hoc penalty for extra parameters, we will calculate the *marginal likelihood* of the model, by marginalizing over parameters.  This requires specifying a *prior* about the parameters for each model.

Specifically, we wish to calculate:

$P(M_i \mid D) = \frac{P(D \mid M_i)P(M_i)}{P(D)}$

To do so, we need to calculate the *marginal likelihood* of the data, marginalizing over parameters:

$P(D \mid M_i) = \int \mathcal{L}_i(D \mid \theta,M_i) P(\theta \mid M_i) d\theta$

And to calculate this, we need to specify the prior distribution over model parameters:

$P(\theta \mid M_i)$

So let's specify some very vague priors (uniform on all $\theta$s):

$P(\theta \mid M_i) = \operatorname{Uniform}(0,1)$

While we can calculate the marginal likelihood analytically, let's do it numerically instead:

```{r}
p.theta = function(theta){dunif(theta,0,1)}
d.theta = 0.001
theta = seq(0,1,by=d.theta)

p.M = c("M0"=1/3, "M1"=1/3, "M2"=1/3)

p.theta.M0 = p.theta(theta)
likelihood.M0 = function(theta){theta^8*(1-theta)^2}
p.data.theta_M0 = vapply(theta, likelihood.M0, c("lik" = 0))
p.data.M0 = sum(p.theta.M0*p.data.theta_M0*d.theta)

p.theta.M1 = p.theta(theta)
likelihood.M1 = function(theta){0.5*theta^8*(1-theta)^1}
p.data.theta_M1 = vapply(theta, likelihood.M1, c("lik" = 0))
p.data.M1 = sum(p.theta.M1*p.data.theta_M1*d.theta)

p.theta_1.M2 = p.theta(theta)
p.theta_2.M2 = p.theta(theta)
p.theta.M2 = outer(p.theta_1.M2, p.theta_2.M2, function(a,b){a*b})
likelihood.M2 = function(theta_1, theta_2){theta_1^1*theta_2^8*(1-theta_2)^1}
p.data.theta_M2 = outer(theta, theta, likelihood.M2)
p.data.M2 = sum(p.theta.M2*p.data.theta_M2*d.theta*d.theta)

p.data.M = c("M0"=p.data.M0, "M1"=p.data.M1, "M2"=p.data.M2)

p.data = sum(p.M*p.data.M)

(p.M.data = p.M*p.data.M/p.data)
```

This kinds of Bayesian model selection is hard because calculating $P(D|M)$ -- the likelihood of the data while marginalizing over possible parameter values under that model -- is hard.  

#### Bayes Factors

Often Bayesian model selection is used to compare just two hypotheses: some null $H_0$, which has an assumption of an effect of 0, and some alternative $H_1$ in which the effect size is a parameter.

For instance, we might say that the two models are:

$H_0$: The data are independent samples from a Bernoulli distribution with $\theta=0.5$  (this is the null hypothesis postulating that the coin has zero bias).

$H_1$: The data are independent samples from a Bernoulli distribution with $\theta$ free to vary (this is the alternate hypothesis in which the coin *can* have some bias).

The logic of Bayes factors is that if we consider the ratio of the posterior probability of $H_1$ and $H_0$, the calculation can be broken up into the ratio of the priors, and the ratio of the (marginal) likelihoods:

$$\frac{P(H_1 \mid D)}{P(H_0 \mid D)} = \frac{P(D  \mid H_1)}{P(D \mid H_0)} \frac{P(H_1)}{P(H_0)}$$

Since we want to know what kind of evidence in favor of $H_1$ (or $H_0$) our data offer, we are only interested in the ratio of the marginal likelihoods.  This ratio is called the Bayes factor:

$$\operatorname{BF} = \frac{P(D  \mid H_1)}{P(D \mid H_0)} = \frac{\int \mathcal{L}(D \mid \theta,H_1) P(\theta \mid H_1)d\theta}{\int \mathcal{L}(D \mid \theta,H_0) P(\theta \mid H_0)d\theta}$$

Again, this is generally hard to calculate, because it is hard to calculate the marginal likelihood.  However, here we can do so numerically:

```{r}
d.theta = 0.001
theta = seq(0,1,by=d.theta)

p.data.H0 = 0.5^8*0.5^2

p.theta.H1 = dunif(theta,0,1)
likelihood.H1 = function(theta){theta^8*(1-theta)^2}
p.data.theta_H1 = vapply(theta, likelihood.H1, c("lik" = 0))
p.data.H1 = sum(p.theta.H1*p.data.theta_H1*d.theta)

(bayesfactor = p.data.H1/p.data.H0)
```

This bayes factor (about 2), is generally not considered to be sufficiently strong evidence in favor of the alternate hypothesis (the prescribed cutoffs can be found [here](https://en.wikipedia.org/wiki/Bayes_factor))
